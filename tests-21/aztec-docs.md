# Table of Contents

- [Aztec Documentation](#aztec-documentation)
    - [Glossary](#glossary)
**Overview**
  - [Aztec Overview](#aztec-overview)
  - [Getting Started on Local Network](#getting-started-on-local-network)
  - [Setting up for Devnet](#setting-up-for-devnet)
**Docs**
  - [Tutorials](#tutorials)
    - [Contract Tutorials](#contract-tutorials)
      - [Private Token Contract](#private-token-contract)
    - [Full-Stack Tutorials](#full-stack-tutorials)
      - [Bridge Your NFT to Aztec](#bridge-your-nft-to-aztec)
  - [Aztec.nr](#aztec-nr)
    - [Noir VSCode Extension](#noir-vscode-extension)
    - [Compiling Contracts](#compiling-contracts)
    - [Testing Contracts](#testing-contracts)
    - [Debugging Aztec Code](#debugging-aztec-code)
    - [Framework Description](#framework-description)
      **Defining Functions**
        - [How to Define Functions](#how-to-define-functions)
        - [Understanding Function Context](#understanding-function-context)
        - [Visibility](#visibility)
        - [Inner Workings of Functions](#inner-workings-of-functions)
        - [Function Attributes and Macros](#function-attributes-and-macros)
        - [Private <> Public Communication](#private-public-communication)
      - [Contract Structure](#contract-structure)
      - [Importing Aztec.nr](#importing-aztec-nr)
      - [Declaring Contract Storage](#declaring-contract-storage)
      - [Calling Other Contracts](#calling-other-contracts)
      - [Implementing custom notes](#implementing-custom-notes)
      - [Emitting Events](#emitting-events)
      - [Aztec macros](#aztec-macros)
      - [Global Variables](#global-variables)
      - [Aztec<>Ethereum Messaging](#aztec-ethereum-messaging)
        - [Data Structures](#data-structures)
        - [Inbox](#inbox)
        - [Outbox](#outbox)
        - [Registry](#registry)
      - [Enabling Authentication Witnesses](#enabling-authentication-witnesses)
      - [Communicating Cross-Chain](#communicating-cross-chain)
      - [Contract Artifacts](#contract-artifacts)
      - [Contract Upgrades](#contract-upgrades)
      - [Advanced Topics](#advanced-topics)
        - [Partial Notes](#partial-notes)
        - [Profiling and Optimizing Contracts](#profiling-and-optimizing-contracts)
        - [Writing Efficient Contracts](#writing-efficient-contracts)
        - [Proving Historic State](#proving-historic-state)
        - [Using Capsules](#using-capsules)
        - [Oracle Functions](#oracle-functions)
    - [Aztec.nr API Reference](#aztec-nr-api-reference)
  - [Aztec.js](#aztec-js)
    - [Getting Started](#getting-started)
    - [Creating Accounts](#creating-accounts)
    - [Deploying Contracts](#deploying-contracts)
    - [Sending Transactions](#sending-transactions)
    - [Simulating Functions](#simulating-functions)
    - [Using Authentication Witnesses](#using-authentication-witnesses)
    - [Paying Fees](#paying-fees)
    - [Testing Aztec.nr contracts with TypeScript](#testing-aztec-nr-contracts-with-typescript)
    - [Reference](#reference)
  - [Aztec CLI](#aztec-cli)
  - [Wallet CLI](#wallet-cli)
    - [Reference](#reference)
  - [Resources](#resources)
    **Considerations and Limitations**
      - [Limitations](#limitations)
    - [Migration notes](#migration-notes)
- [Aztec Connect Sunset](#aztec-connect-sunset)
  - [Testnet (v2.1.4)](#testnet-v2-1-4)
**Reference**
  - [Ignition (v2.1.9)](#ignition-v2-1-9)
**Getting Started**
  - [Prerequisites](#prerequisites)
**Setup**
  - [Running a Full Node](#running-a-full-node)
  - [Running a Sequencer](#running-a-sequencer)
    - [Registering a Sequencer](#registering-a-sequencer)
    - [Become a Staking Provider](#become-a-staking-provider)
    - [High Availability Sequencers](#high-availability-sequencers)
  - [Running a Prover](#running-a-prover)
  - [Building Node Software from Source](#building-node-software-from-source)
  - [Using and running a bootnode](#using-and-running-a-bootnode)
  - [Using and uploading snapshots](#using-and-uploading-snapshots)
**Operation**
  - [Monitoring](#monitoring)
    - [Setup Guides](#setup-guides)
      - [Prometheus Setup](#prometheus-setup)
      - [Grafana Setup](#grafana-setup)
    - [Key Metrics Reference](#key-metrics-reference)
    - [Complete Example and Troubleshooting](#complete-example-and-troubleshooting)
  - [Keystore Usage](#keystore-usage)
    - [Creating Sequencer Keystores](#creating-sequencer-keystores)
    - [Key storage methods](#key-storage-methods)
    - [Sample configuration patterns](#sample-configuration-patterns)
    - [Troubleshooting and Best Practices](#troubleshooting-and-best-practices)
  - [Sequencer Management](#sequencer-management)
    - [Governance and Proposal Process](#governance-and-proposal-process)
    - [Slashing and Offenses](#slashing-and-offenses)
    - [Claiming Rewards](#claiming-rewards)
    - [Useful Commands](#useful-commands)
  - [FAQs & Common Issues](#faqs-common-issues)
  - [Changelog](#changelog)
    - [v2.0.2 (from v1.2.1)](#v2-0-2-from-v1-2-1)
  - [Cli Reference](#cli-reference)
  - [Node JSON RPC API reference](#node-json-rpc-api-reference)
  - [Ethereum RPC call reference](#ethereum-rpc-call-reference)
  - [Glossary](#glossary)

---


# Aztec Documentation

Source: https://docs.aztec.network/

# Aztec Documentation

Build private smart contracts on Ethereum's leading privacy-first L2 zkRollup.


## What is Aztec?Aztec is a **privacy-first Layer 2** zkRollup on Ethereum. While Ethereum makes everything publicly visible, Aztec brings privacy to smart contracts, enabling confidential transactions and private state. Aztec is *not EVM compatible* and extends the Ethereum ecosystem with a new
privacy-preserving virtual machine. To learn more about how Aztec achieves
privacy, check out the [foundational topics](/developers/docs/foundational-topics).

## Private Functions

Execute and prove functions on a user's device, keeping logic and data confidential.

## Public Functions

Execute transparent operations in the Aztec Virtual Machine when visibility is needed.

## Private State

Store data as encrypted UTXOs that only the owner can decrypt and access.

## Public State

Maintain transparent state in a public merkle tree when required by your application.

## Composability

Seamlessly compose private and public execution with private and public state.

## L1 ↔ L2 Messaging

Send public and private messages between Ethereum and Aztec.

## Quick Links

[QuickstartDeploy your first contract in minutes](/developers/getting_started_on_devnet)[TutorialsStep-by-step guides to build on Aztec](/developers/docs/tutorials/contract_tutorials/counter_contract)[Write Smart ContractsAztec.nr framework documentation](/developers/docs/aztec-nr)[aztec.js SDKJavaScript/TypeScript library for Aztec](/developers/docs/aztec-js)
## Resources

[Aztec Starter RepoTemplate project to kickstart development](https://github.com/AztecProtocol/aztec-starter)[Noir Language DocsLearn the Noir programming language](https://noir-lang.org/docs)[Join the CommunityConnect with other builders](https://airtable.com/appMhZd7lsZS3v27R/pagxWYAHYYrnrrXmm/form)[GlossaryKey terms and definitions](/developers/docs/resources/glossary)

---


# Glossary

Source: https://docs.aztec.network/developers/docs/resources/glossary

Version: Devnet (v3.0.0-devnet.20251212)

On this page

## ACIR (Abstract Circuit Intermediate Representation)")

ACIR bytecode is the compilation target of private functions. ACIR expresses arithmetic circuits and has no control flow: any control flow in functions is either unrolled (for loops) or flattened (by inlining and adding predicates). ACIR contains different types of opcodes including arithmetic operations, BlackBoxFuncCall (for efficient operations like hashing), Brillig opcodes (for unconstrained hints), and MemoryOp (for dynamic array access). Private functions compiled to ACIR are executed by the ACVM (Abstract Circuit Virtual Machine) and proved using Barretenberg.

## AVM (Aztec Virtual Machine)")

The Aztec Virtual Machine (AVM) executes the public section of a transaction. It is conceptually similar to the Ethereum Virtual Machine (EVM) but designed specifically for Aztec's needs. Public functions are compiled to AVM bytecode and executed by sequencers in the AVM. The AVM uses a flat memory model with tagged memory indexes to track maximum potential values and bit sizes. It supports control flow (if/else) and includes specific opcodes for blockchain operations like timestamp and address access, but doesn't allow arbitrary oracles for security reasons.

## AztecAztec is a privacy-first Layer 2 rollup on Ethereum. It supports smart contracts with both private & public state and private & public execution.

`aztec` is a CLI tool (with an extensive set of parameters) that enables users to perform a wide range of tasks. It can: compile and test contracts, run a node, run a local network, execute tests, generate contract interfaces for javascript and more.

Full reference [here](/developers/docs/aztec-cli/cli_reference).

## Aztec WalletThe Aztec Wallet is a CLI wallet, `aztec-wallet`, that allows a user to manage accounts and interact with an Aztec network. It includes a PXE.

Full reference [here](/developers/docs/wallet-cli/cli_wallet_reference).

## `aztec-up``aztec-up` updates the local aztec executables to the latest version (default behavior) or to a specified version.

## Aztec.jsA [Node package](https://www.npmjs.com/package/@aztec/aztec.js) to help make Aztec dApps.

Read more and review the source code [here](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/yarn-project/aztec.js).

## Aztec.nr[Aztec.nr](https://github.com/AztecProtocol/aztec-packages/tree/next/noir-projects/aztec-nr) is a framework for writing Aztec smart contracts with Noir that abstracts away state management. It handles things like note generation, state trees etc. It's essentially a giant Noir library which abstracts the complexities of interacting with Aztec.

Read more and review the source code [here](https://aztec.nr).

## BarretenbergAztec's cryptography back-end. Refer to the graphic at the top of [this page](https://medium.com/aztec-protocol/explaining-the-network-in-aztec-network-166862b3ef7d) to see how it fits in the Aztec architecture.

Barretenberg's source code can be found [here](https://github.com/AztecProtocol/barretenberg).

## bb / bb.js`bb` (CLI) and its corresponding `bb.js` (node module) are tools that prove and verify circuits. It also has helpful functions such as: writing solidity verifier contracts, checking a witness, and viewing a circuit's gate count.

## CommitmentA cryptographic commitment is a hash of some data (plus randomness) that hides the original value but allows you to later prove you committed to that specific value, by proving knowledge of a valid preimage, without being able to change it.

In Aztec, a commitment refers to a cryptographic hash of a note. Rather than storing entire notes in a data tree, note commitments (hashes of the notes) are stored in a merkle tree called the note hash tree. Users prove that they have the note pre-image information when they update private state in a contract. This allows the network to verify the existence of private data without revealing its contents.

## Merkle TreeA Merkle tree is a binary tree data structure where adjacent nodes are hashed together recursively to produce a single node called the root hash.

Merkle trees in Aztec are used to store cryptographic commitments. They are used across five Aztec Merkle trees: the note hash tree (stores commitments to private notes), the nullifier tree (stores nullifiers for spent notes), the public data tree (stores public state), the contract tree and the archive tree. All trees use domain-separated Poseidon2 hashing with specific tree identifiers and layer separation to ensure security and prevent cross-tree attacks.

## `nargo`With `nargo`, you can start new projects, compile, execute, and test your Noir programs.

You can find more information in the nargo installation docs [here](https://noir-lang.org/docs/getting_started/installation/) and the nargo command reference [here](https://noir-lang.org/docs/reference/nargo_commands).

## NoirNoir is a Domain Specific Language (DSL) for SNARK proving systems. It is used for writing smart contracts in Aztec because private functions on Aztec are implemented as SNARKs to support privacy-preserving operations.

## Noir Language ServerThe Noir Language Server can be used in vscode to facilitate writing programs in Noir by providing syntax highlighting, circuit introspection and an execution interface. The Noir LSP addon allows the dev to choose their tool, nargo or `aztec`, when writing a pure Noir program or an Aztec smart contract.

You can find more info about the LSP [in the Noir docs](https://noir-lang.org/docs/tooling/language_server).

## NodeA node is a computer running Aztec software that participates in the Aztec network. A specific type of node is a sequencer. Nodes run the public execution environment (AVM), validate proofs, and maintain the 5 state Merkle trees (note hash, nullifier, public state, contract and archive trees).

The Aztec testnet rolls up to Ethereum Sepolia.

To run your own node see [here](/network).

## NoteIn Aztec, a Note is like an envelope containing private data. A commitment (hash) of this note is stored in an append-only Merkle tree and stored by all the nodes in the network. Notes can be encrypted to be shared with other users. Data in a note may represent some variable's state at a point in time.

## Note DiscoveryNote discovery refers to the process of a user identifying and decrypting the encrypted notes that belong to them. Aztec uses a note tagging system where senders tag encrypted onchain logs containing notes in a way that only the sender and recipient can identify. The tag is derived from a shared secret and an index (a shared counter that increments each time the sender creates a note for the recipient). This allows users to efficiently find their notes without brute force decryption or relying on offchain communication.

## NullifierA nullifier is a unique value that, once posted publicly, proves something has been used or consumed without revealing what that thing was.

In the context of Aztec, a nullifier is derived from a note and signifies the note has been "spent" or consumed without revealing which specific note was spent. When a note is updated or spent in Aztec, the protocol creates a nullifier from the note data using the note owner's nullifier key. This nullifier is inserted into the nullifier Merkle tree. The nullifier mechanism prevents double-spending while maintaining privacy by not requiring deletion of the original note commitment, which would leak information.

## Partial NotesPartial notes are a concept that allows users to commit to an encrypted value, and allows a counterparty to update that value without knowing the specific details of the encrypted value. They are notes that are created in a private function with values that are not yet considered finalized (e.g., `amount` in a `UintNote`). The partial note commitment is computed using multi scalar multiplication on an elliptic curve, then passed to a public function where another party can add value to the note without knowing its private contents. This enables use cases like private fee payments, DEX swaps, and lending protocols.

## Programmable PrivacyAztec achieves programmable privacy through its hybrid architecture that supports both private and public smart contract execution. Private functions run client-side with zero-knowledge proofs, while public functions run onchain. This allows developers to program custom privacy logic, choosing what data remains private and what becomes public, with composability between private and public state and execution contexts.

## ProversThe Prover in a ZK system is the entity proving they have knowledge of a valid witness that satisfies a statement. In the context of Aztec, this is the entity that creates the proof that some computation was executed correctly. Here, the statement would be "I know the inputs and outputs that satisfy the requirements for the computation, and I did the computation correctly."

Aztec will be launched with a fully permissionless proving network (pieces of code that produce the proofs for valid rollup state transitions) that anyone can participate in.

How this works will be discussed via a future RFP process on Discourse, similarly to the Sequencer RFP.

## Proving KeyA key that is used to generate a proof. In the case of Aztec, these are compiled from Noir smart contracts.

## Private Execution Environment (PXE)")

The private execution environment is where private computation occurs. This is local such as your device or browser.

Read more [here](/developers/docs/foundational-topics/pxe).

## Local NetworkThe local network is a development Aztec network that runs on your machine and interacts with a development Ethereum node. It allows you to develop and deploy Noir smart contracts but without having to interact with testnet or mainnet (when the time comes).

Included in the local network:

* Local Ethereum network (Anvil)
* Deployed Aztec protocol contracts (for L1 and L2)
* A set of test accounts with some test tokens to pay fees
* Development tools to compile contracts and interact with the network (aztec command and aztec-wallet)
* All of this comes packaged in a Docker container to make it easy to install and run.

## SequencerA sequencer is a specialized node that is generally responsible for:

* Selecting pending transactions from the mempool
* Ordering transactions into a block
* Verifying all private transaction proofs and execute all public transactions to check their validity
* Computing the ROLLUP\_BLOCK\_REQUEST\_DATA
* Computing state updates for messages between L2 & L1
* Broadcasting the ROLLUP\_BLOCK\_REQUEST\_DATA to the prover network via the proof pool for parallelizable computation.
* Building a rollup proof from completed proofs in the proof pool
* Tagging the pending block with an upgrade signal to facilitate forks
* Publishing completed block with proofs to Ethereum as an ETH transaction

Aztec will be launched with a fully permissionless sequencer network that anyone can participate in.

How this works is being discussed actively in the [Discourse forum](https://discourse.aztec.network/t/request-for-proposals-decentralized-sequencer-selection/350/). Once this discussion process is completed, we will update the glossary and documentation with specifications and instructions for how to run.

Previously in [Aztec Connect](https://medium.com/aztec-protocol/sunsetting-aztec-connect-a786edce5cae) there was a single sequencer, and you can find the Typescript reference implementation called Falafel [here](https://github.com/AztecProtocol/aztec-connect/tree/master/yarn-project/falafel).

## Smart ContractsPrograms that run on the Aztec network are called smart contracts, similar to [programs](https://ethereum.org/en/developers/docs/smart-contracts/) that run on Ethereum.

However, these will be written in the [Noir](https://noir-lang.org/) programming language, and may optionally include private state and private functions.

## StatementA statement in Aztec's zero-knowledge context refers to the public assertion being proved about a private computation. For example, a statement might be "I know the inputs and outputs that satisfy the requirements for this computation, and I executed the computation correctly." The statement defines what is being proven without revealing the private details (the witness) that prove it. In Aztec, statements typically involve proving correct execution of private functions, valid note ownership, or proper state transitions.

## VerifierThe entity responsible for verifying the validity of a ZK proof. In the context of Aztec, this is:

* **The sequencers**: verify that private functions were executed correctly.
* **The Ethereum L1 smart contract**: verifies batches of transactions were executed correctly.

## Verification KeyA key that is used to verify the validity of a proof generated from a proving key from the same smart contract.

## WitnessIn the context of Aztec's zero-knowledge proofs, a witness refers to the private inputs and intermediate values that satisfy the constraints of a circuit. When executing a private function, the ACVM generates the witness of the execution - the complete set of values that prove the computation was performed correctly. The witness includes both the secret inputs provided by the user and all intermediate computational steps, but is never revealed publicly. Only a cryptographic proof of the witness's validity is shared.

## Zero-knowledge (ZK) proofproof")

Zero-knowledge proofs in Aztec are cryptographic proofs that allow someone to prove they know certain information or have performed a computation correctly without revealing the underlying data. Aztec uses various ZK-SNARK protocols including UltraPlonk and Honk. These proofs enable private execution where users can prove they executed a private function correctly and that they own certain notes, without revealing the function inputs, note contents, or internal computation details. The proofs are verified onchain to ensure the integrity of private state transitions.

---


# Aztec Overview

Source: https://docs.aztec.network/developers/overview

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This page outlines Aztec's fundamental technical concepts. It is recommended to read this before diving into building on Aztec.

## What is Aztec?Aztec is a privacy-first Layer 2 on Ethereum. It supports smart contracts with both private & public state and private & public execution.

![](/assets/ideal-img/Aztec_overview.27d90d6.640.png)

## High level view![](/assets/ideal-img/aztec-high-level.2417c8f.640.png)

1. A user interacts with Aztec through Aztec.js (like web3js or ethersjs)
2. Private functions are executed in the PXE, which is client-side
3. Proofs and tree updates are sent to the Public VM (running on an Aztec node)
4. Public functions are executed in the Public VM
5. The Public VM rolls up the transactions that include private and public state updates into blocks
6. The block data and proof of a correct state transition are submitted to Ethereum for verification

## Private and public executionPrivate functions are executed client side, on user devices to maintain maximum privacy. Public functions are executed by a remote network of nodes, similar to other blockchains. These distinct execution environments create a directional execution flow for a single transaction--a transaction begins in the private context on the user's device then moves to the public network. This means that private functions executed by a transaction can enqueue public functions to be executed later in the transaction life cycle, but public functions cannot call private functions.

## Private Execution Environment (PXE)")

Private functions are executed on the user's device in the Private Execution Environment (PXE, pronounced 'pixie'), then it generates proofs for onchain verification. It is a client-side library for execution and proof-generation of private operations. It holds keys, notes, and generates proofs. It is included in aztec.js, a TypeScript library, and can be run within Node or the browser.

Note: It is easy for private functions to be written in a detrimentally unoptimized way, because many intuitions of regular program execution do not apply to proving. For more about writing performant private functions in Noir, see [this page](https://noir-lang.org/docs/explainers/explainer-writing-noir) of the Noir documentation.

## Aztec Virtual Machine (AVM)")

Public functions are executed by the Aztec Virtual Machine (AVM), which is conceptually similar to the Ethereum Virtual Machine (EVM). As such, writing efficient public functions follow the same intuition as gas-efficient solidity contracts.

The PXE is unaware of the Public VM. And the Public VM is unaware of the PXE. They are completely separate execution environments. This means:

* The PXE and the Public VM cannot directly communicate with each other
* Private transactions in the PXE are executed first, followed by public transactions

## Private and public statePrivate state works with UTXOs, which are chunks of data that we call notes. To keep things private, notes are stored in an [append-only UTXO tree](/developers/docs/foundational-topics/advanced/storage/indexed_merkle_tree), and a nullifier is created when notes are invalidated (aka deleted). Nullifiers are stored in their own [nullifier tree](/developers/docs/foundational-topics/advanced/storage/indexed_merkle_tree).

Public state works similarly to other chains like Ethereum, behaving like a public ledger. Public data is stored in a public data tree.

![Public vs private state](/assets/images/public-and-private-state-diagram-ff88262b40b259d4fe4c8b7d667924aa.png)

Aztec [smart contract](/developers/docs/aztec-nr/framework-description/contract_structure) developers should keep in mind that different data types are used when manipulating private or public state. Working with private state is creating commitments and nullifiers to state, whereas working with public state is directly updating state.

## Accounts and keys## Account abstractionEvery account in Aztec is a smart contract (account abstraction). This allows implementing different schemes for authorizing transactions, nonce management, and fee payments.

Developers can write their own account contract to define the rules by which user transactions are authorized and paid for, as well as how user keys are managed.

Learn more about account contracts [here](/developers/docs/foundational-topics/accounts).

## Key pairsEach account in Aztec is backed by 3 key pairs:

* A **nullifier key pair** used for note nullifier computation
* A **incoming viewing key pair** used to encrypt a note for the recipient
* A **outgoing viewing key pair** used to encrypt a note for the sender

As Aztec has native account abstraction, accounts do not automatically have a signing key pair to authenticate transactions. This is up to the account contract developer to implement.

## NoirNoir is a zero-knowledge domain specific language used for writing smart contracts for the Aztec network. It is also possible to write circuits with Noir that can be verified on or offchain. For more in-depth docs into the features of Noir, go to the [Noir documentation](https://noir-lang.org/).

---


# Getting Started on Local Network

Source: https://docs.aztec.network/developers/getting_started_on_local_network

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Get started on your local environment using a local network. If you'd rather jump into devnet, read the [getting started on devnet guide](/developers/getting_started_on_devnet).

The local network is a local development Aztec network running fully on your machine, and interacting with a development Ethereum node. You can develop and deploy on it just like on a testnet or mainnet (when the time comes). The local network makes it faster and easier to develop and test your Aztec applications.

What's included in the local network:

* Local Ethereum network (Anvil)
* Deployed Aztec protocol contracts (for L1 and L2)
* A set of test accounts with some test tokens to pay fees
* Development tools to compile contracts and interact with the network (`aztec` and `aztec-wallet`)

All of this comes packaged in a Docker container to make it easy to install and run.

This guide will teach you how to install the Aztec local network, run it using the Aztec CLI, and interact with contracts using the wallet CLI. To jump right into the testnet instead, click the `Testnet` tab.

## PrerequisitesYou need two global dependencies on your machine:

* Aztec libraries use Node.js version v22.15.x (lts/jod), and backwards compatible from version 20. You can use [nvm](https://github.com/nvm-sh/nvm) to help manage node versions.
* Docker (visit [this page of the Docker docs](https://docs.docker.com/get-docker/) on how to install it)

## Install and run the local network## Start DockerDocker needs to be running in order to install the local network. Find instructions on the [Docker website](https://docs.docker.com/get-started/).

## Install the local networkRun:

```
bash -i <(curl -s https://install.aztec.network)
```

Once the installation is complete, install the specific version:

```
aztec-up 3.0.0-devnet.20251212
```

This will install the following tools:

* **aztec** - compiles and tests aztec contracts and launches various infrastructure subsystems (full local network, sequencer, prover, pxe, etc) and provides utility commands to interact with the network
* **aztec-up** - a tool to upgrade the aztec toolchain to the latest, or specific versions.
* **aztec-wallet** - a tool for interacting with the aztec network

## Start the local networkOnce these have been installed, to start the local network, run:

```
aztec start --local-network
```

**Congratulations, you have just installed and run the Aztec local network!**

```
     /\        | |  
    /  \    ___| |_ ___  ___  
   / /\ \  |_  / __/ _ \/ __|  
  / ____ \  / /| ||  __/ (__  
 /_/___ \_\/___|\__\___|\___|
```

In the terminal, you will see some logs:

1. Local network version
2. Contract addresses of rollup contracts
3. PXE (private execution environment) setup logs
4. Initial accounts that are shipped with the local network and can be used in tests

You'll know the local network is ready to go when you see something like this:

```
[INFO] Aztec Server listening on port 8080
```

## Using the local network test accountsFor convenience, the local network comes with 3 initial accounts that are prefunded, helping bootstrap payment of any transaction. To use them, you will need to add them to your pxe/wallet.

To add the test accounts in the wallet, run this in another terminal:

```
aztec-wallet import-test-accounts
```

We'll use the first test account, `test0`, throughout to pay for transactions.

## Creating an account in the local network```
aztec-wallet create-account -a my-wallet -f test0
```

info

`aztec-wallet` will generate transaction proofs by default. This is not required when sending transactions on the local network, but it is required when sending transactions on the devnet or mainnet.

You can turn off proof generation by adding the `--prover none` flag to the command or setting `PXE_PROVER=none`.

This will create a new wallet with an account and give it the alias `my-wallet`. Accounts can be referenced with `accounts:<alias>`. You will see logs telling you the address, public key, secret key, and more.

On successful deployment of the account, you should see something like this:

```
New account:  
  
Address:         0x066108a2398e3e2ff53ec4b502e4c2e778c6de91bb889de103d5b4567530d99c  
Public key:      0x007343da506ea513e6c05ba4d5e92e3c682333d97447d45db357d05a28df0656181e47a6257e644c3277c0b11223b28f2b36c94f9b0a954523de61ac967b42662b60e402f55e3b7384ba61261335040fe4cd52cb0383f559a36eeea304daf67d1645b06c38ee6098f90858b21b90129e7e1fdc4666dd58d13ef8fab845b2211906656d11b257feee0e91a42cb28f46b80aabdc70baad50eaa6bb2c5a7acff4e30b5036e1eb8bdf96fad3c81e63836b8aa39759d11e1637bd71e3fc76e3119e500fbcc1a22e61df8f060004104c5a75b52a1b939d0f315ac29013e2f908ca6bc50529a5c4a2604c754d52c9e7e3dee158be21b7e8008e950991174e2765740f58  
Secret key:     0x1c94f8b19e91d23fd3ab6e15f7891fde7ba7cae01d3fa94e4c6afb4006ec0cfb  
Partial address: 0x2fd6b540a6bb129dd2c05ff91a9c981fb5aa2ac8beb4268f10b3aa5fb4a0fcd1  
Salt:            0x0000000000000000000000000000000000000000000000000000000000000000  
Init hash:       0x28df95b579a365e232e1c63316375c45a16f6a6191af86c5606c31a940262db2  
Deployer:        0x0000000000000000000000000000000000000000000000000000000000000000  
  
Waiting for account contract deployment...  
Deploy tx hash:  0a632ded6269bda38ad6b54cd49bef033078218b4484b902e326c30ce9dc6a36  
Deploy tx fee:   200013616  
Account stored in database with aliases last & my-wallet
```

You may need to scroll up as there are some other logs printed after it.

You can double check by running `aztec-wallet get-alias accounts:my-wallet`.

For simplicity we'll keep using the test account, let's deploy our own test token!

## Deploying a contractThe local network comes with some contracts that you can deploy and play with. One of these is an example token contract.

Deploy it with this:

```
aztec-wallet deploy TokenContractArtifact --from accounts:test0 --args accounts:test0 TestToken TST 18 -a testtoken
```

This takes

* the contract artifact as the argument, which is `TokenContractArtifact`
* the deployer account, which we used `test0`
* the args that the contract constructor takes, which is the `admin` (`accounts:test0`), `name` (`TestToken`), `symbol` (`TST`), and `decimals` (`18`).
* an alias `testtoken` (`-a`) so we can easily reference it later with `contracts:testtoken`

On successful deployment, you should see something like this:

```
aztec:wallet [INFO] Using wallet with address 0x066108a2398e3e2ff53ec4b502e4c2e778c6de91bb889de103d5b4567530d99c +0ms  
Contract deployed at 0x15ce68d4be65819fe9c335132f10643b725a9ebc7d86fb22871f6eb8bdbc3abd  
Contract partial address 0x25a91e546590d77108d7b184cb81b0a0999e8c0816da1a83a2fa6903480ea138  
Contract init hash 0x0abbaf0570bf684da355bd9a9a4b175548be6999625b9c8e0e9775d140c78506  
Deployment tx hash: 0a8ccd1f4e28092a8fa4d1cb85ef877f8533935c4e94b352a38af73eee17944f  
Deployment salt: 0x266295eb5da322aba96fbb24f9de10b2ba01575dde846b806f884f749d416707  
Deployment fee: 200943060  
Contract stored in database with aliases last & testtoken
```

In the next step, let's mint some tokens!

## Minting public tokensCall the public mint function like this:

```
aztec-wallet send mint_to_public --from accounts:test0 --contract-address contracts:testtoken --args accounts:test0 100
```

This takes

* the function name as the argument, which is `mint_to_public`
* the `from` account (caller) which is `accounts:test0`
* the contract address, which is aliased as `contracts:testtoken` (or simply `testtoken`)
* the args that the function takes, which is the account to mint the tokens into (`test0`), and `amount` (`100`).

This only works because we are using the secret key of the admin who has permissions to mint.

A successful call should print something like this:

```
aztec:wallet [INFO] Using wallet with address 0x066108a2398e3e2ff53ec4b502e4c2e778c6de91bb889de103d5b4567530d99c +0ms  
Maximum total tx fee:   1161660  
Estimated total tx fee: 116166  
Estimated gas usage:    da=1127,l2=115039,teardownDA=0,teardownL2=0  
  
Transaction hash: 2ac383e8e2b68216cda154b52e940207a905c1c38dadba7a103c81caacec403d  
Transaction has been mined  
 Tx fee: 200106180  
 Status: success  
 Block number: 17  
 Block hash: 1e27d200600bc45ab94d467c230490808d1e7d64f5ee6cee5e94a08ee9580809  
Transaction hash stored in database with aliases last & mint_to_public-9044
```

You can double-check by calling the function that checks your public account balance:

```
aztec-wallet simulate balance_of_public --from test0 --contract-address testtoken --args accounts:test0
```

This should print

```
Simulation result:  100n
```

## Playing with hybrid state and private functionsIn the following steps, we'll move some tokens from public to private state and check our private and public balance.

```
aztec-wallet send transfer_to_private --from accounts:test0 --contract-address testtoken --args accounts:test0 25
```

The arguments for `transfer_to_private` function are:

* the account address to transfer to
* the amount of tokens to send to private

A successful call should print something similar to what you've seen before.

Now when you call `balance_of_public` again you will see 75!

```
aztec-wallet simulate balance_of_public --from test0 --contract-address testtoken --args accounts:test0
```

This should print

```
Simulation result:  75n
```

And then call `balance_of_private` to check that you have your tokens!

```
aztec-wallet simulate balance_of_private --from test0 --contract-address testtoken --args accounts:test0
```

This should print

```
Simulation result:  25n
```

**Congratulations, you now know the fundamentals of working with the Aztec local network!** You are ready to move onto the more fun stuff.

## What's next?Want to build something cool on Aztec?

* Check out the [Token Contract Tutorial](/developers/docs/tutorials/contract_tutorials/token_contract) for a beginner tutorial, or jump into more advanced ones
* Start on your own thing and check out the How To Guides to help you!

---


# Setting up for Devnet

Source: https://docs.aztec.network/developers/getting_started_on_devnet

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide explains the differences between the local network and devnet, how to migrate from the local network to devnet, and how to start developing directly on devnet.

## Local Network vs Devnet: Key DifferencesBefore diving into the setup, it's important to understand the differences between the local network and devnet:

## Local Network (Local Development)")

* Runs locally on your machine
* No proving by default (faster development)
* No fees
* Instant block times
* Test accounts automatically deployed
* Ideal for rapid development and testing

## Devnet (Remote Network)")

* Remote environment with network of sequencers
* Always has fees enabled (need to pay or sponsor fees)
* ~36 second block times, longer L1 settlement
* No automatic test accounts

info

If you're new to Aztec and want to understand local development first, check out the [local network guide](/developers/docs/tutorials/local_network).

## PrerequisitesBefore working with devnet, ensure you have:

1. [Docker](https://docs.docker.com/get-started/get-docker/) installed
2. Aztec CLI installed:

```
bash -i <(curl -s https://install.aztec.network)
```

3. The devnet version installed:

```
aztec-up 3.0.0-devnet.20251212
```

warning

The devnet is version dependent. It is currently running version `3.0.0-devnet.20251212`. Maintain version consistency when interacting with the devnet to reduce errors.

## Getting Started on Devnet## Step 1: Set up your environmentSet the required environment variables:

```
export VERSION=3.0.0-devnet.20251212  
export NODE_URL=https://next.devnet.aztec-labs.com/  
export SPONSORED_FPC_ADDRESS=0x1586f476995be97f07ebd415340a14be48dc28c6c661cc6bdddb80ae790caa4e
```

## Step 2: Create and deploy an accountUnlike the local network, devnet has no pre-deployed accounts. You need to create your own, but first you need to register the sponsored FPC to pay transaction fees:

```
aztec-wallet register-contract \  
    --node-url $NODE_URL \  
    --alias sponsoredfpc \  
    $SPONSORED_FPC_ADDRESS SponsoredFPC \  
    --salt 0
```

Then create your account and deploy it:

```
aztec-wallet create-account \  
    --node-url $NODE_URL \  
    --alias my-wallet \  
    --payment method=fpc-sponsored,fpc=$SPONSORED_FPC_ADDRESS
```

note

The first transaction will take longer as it downloads proving keys. If you see `Timeout awaiting isMined`, the transaction is still processing - this is normal on testnet.

## Step 3: Deploy and interact with contractsDeploy a token contract as an example:

```
aztec-wallet deploy \  
    --node-url $NODE_URL \  
    --from accounts:my-wallet \  
    --payment method=fpc-sponsored,fpc=$SPONSORED_FPC_ADDRESS \  
    --alias token \  
    TokenContract \  
    --args accounts:my-wallet Token TOK 18 --no-wait
```

You can check the transaction status on [aztecscan](https://devnet.aztecscan.xyz).

Interact with your deployed contract:

```
aztec-wallet send mint_to_private \  
    --node-url $NODE_URL \  
    --from accounts:my-wallet \  
    --payment method=fpc-sponsored,fpc=$SPONSORED_FPC_ADDRESS \  
    --contract-address token \  
    --args accounts:my-wallet 10
```

## Migrating from the Local Network to TestnetIf you have an existing app running on your local network, here's how to migrate it to testnet:

## 1. Connect to Testnet NodeInstead of running a local network, connect to the testnet node:

```
export NODE_URL=https://aztec-testnet-fullnode.zkv.xyz
```

When running `aztec-wallet` commands, include the node URL:

```
aztec-wallet create-account -a main --node-url $NODE_URL
```

## 2. Initialize a TestWallet for DevnetYou can connect to testnet directly from your app using AztecJS:

In the browser:

```
import { TestWallet } from "@aztec/test-wallet/client/lazy";
```

In Node.js:

```
import { TestWallet } from "@aztec/test-wallet/server";
```

Then initialize with devnet configuration:

```
import { createAztecNodeClient } from "@aztec/aztec.js/node";  
import { TestWallet } from "@aztec/test-wallet/server";  
  
const NODE_URL = "https://next.devnet.aztec-labs.com";  
const node = createAztecNodeClient(NODE_URL);  
const wallet = await TestWallet.create(node);
```

## 3. Handle Fees on DevnetUnlike the local network, devnet requires fee payment. You have three options:

1. **User pays their own fees** - Send them tokens or direct them to the faucet
2. **Your contract sponsors fees** - Deploy a fee-paying contract
3. **Use the canonical sponsored FPC** - Recommended for getting started

info

See the [aztec-starter](https://github.com/AztecProtocol/aztec-starter/blob/154758c866fe34174f2e22b59e70e277fe8ecc73/src/utils/deploy_account.ts#L39) for an example of how to deploy a contract with the sponsored FPC.

## 4. Important Migration Considerations* **Register all contracts**: Including account contracts and the sponsored FPC in the wallet
* **No test accounts**: You'll need to deploy accounts manually
* **Longer transaction times**: Handle timeouts gracefully - transactions may still succeed
* **L1-L2 messaging delays**:
  + L1→L2: Wait ~1.5-2 minutes (vs 2 blocks on the local network)
  + L2→L1: Wait ~30 minutes for finalization (vs immediate on the local network)

## Key Considerations When Using Devnet## Handling Transaction TimeoutsDevnet transactions take longer than on the local network. Handle timeouts gracefully:

```
try {  
  const receipt = await tx.wait();  
} catch (error) {  
  if (error.message.includes("Timeout awaiting isMined")) {  
    console.log("Transaction sent but still being mined");  
    // Check block explorer for status  
  }  
}
```

## Environment DetectionDetect which environment your code is running against:

```
const isDevnet = process.env.NODE_URL?.includes("devnet");  
const nodeUrl = process.env.NODE_URL || "http://localhost:8080";
```

## Devnet information## RPC<https://next.devnet.aztec-labs.com>

## Packages Versions / Github Tag3.0.0-devnet.20251212

## Network Configuration* **l1ChainId**: 11155111
* **rollupVersion**: 1647720761

* **enr**: enr:-N24QOknmfktIHJYBqePfUaRLt1NQttmjTJdHE22yEerFtjONWD3\_AAcf9vuj0YbMYbnZVKt1AW4a-DvepWbsUbmAu0HhWF6dGVjsTAwLTExMTU1MTExLTVkODRiNjRiLTE2NDc3MjA3NjEtMjM0OGIxMmUtMTk1ODg2ZWSCaWSCdjSCaXCEI-Y9PYlzZWNwMjU2azGhAnE-dCLZNTZIKiFJsu\_mi7QYlgndx1lvKCnVu-75zEoig3RjcIKd0IN1ZHCCndCDdmVylTMuMC4wLWRldm5ldC4yMDI1MTIxMg

## Migration Notes[Migration Notes](/developers/docs/resources/migration_notes)

## L1 Contract Addresses* **registryAddress**: `0x548ed380440c3eef42f222ceda1d6770b8999f8c`
* **rollupAddress**: `0x5d84b64b0b2f468df065d8cf01fff88a73238a13`
* **inboxAddress**: `0x8ea98d35d7712ca236ac7a2b2f47d9fb5c9154e8`
* **outboxAddress**: `0x6628f5648dcee4ee4c3262ed35a995039cadb669`
* **feeJuiceAddress**: `0x543a5f9ae03f0551ee236edf51987133fb3da3e2`
* **stakingAssetAddress**: `0x3dae418ad4dbd49e00215d24079a10ac3bc9ef4f`
* **feeJuicePortalAddress**: `0x5eee7cb811f638b70fe1a04d2318530c55d7bd87`
* **coinIssuerAddress**: `0xe4805eda5e880355ff4ded78dcf38ae6077b5dba`
* **rewardDistributorAddress**: `0x9417a0ee4fc66079a32aa7103b2a3d2dc2606dbd`
* **governanceProposerAddress**: `0x7c5f4cec86ef9a920a8fd03d5a01059e32fccb9a`
* **governanceAddress**: `0x26af139c092172e5a4ab9a9d7ddeed41c1d68bc7`
* **gseAddress**: `0xc5cb82799169bb08a20ede20e5b57f337c735ac4`

## Protocol Contract Addresses* **classRegistry**: `0x0000000000000000000000000000000000000000000000000000000000000003`
* **feeJuice**: `0x0000000000000000000000000000000000000000000000000000000000000005`
* **instanceRegistry**: `0x0000000000000000000000000000000000000000000000000000000000000002`
* **multiCallEntrypoint**: `0x0000000000000000000000000000000000000000000000000000000000000004`

## Next Steps* **New to Aztec?** Start with the [local network tutorial](/developers/docs/tutorials/local_network) for faster development
* **Ready for production testing?** Continue using devnet
* **Learn more:** Check out our [tutorials](/developers/docs/tutorials/contract_tutorials/counter_contract)
* **Explore:** Visit [Aztec Playground](https://play.aztec.network/)

## Additional Resources* [Fee payment guide](/developers/docs/aztec-js/how_to_pay_fees)
* [Running a node](/network)
* [Block explorers](https://devnet.aztecscan.xyz)

---


# Tutorials

Source: https://docs.aztec.network/developers/docs/tutorials/local_network

Version: Devnet (v3.0.0-devnet.20251212)

On this page

* Current version: `v3.0.0-devnet.20251212`
* Update with `aztec-up 3.0.0-devnet.20251212`

On this page you will find

* [Versions](#versions)
  + [Dependency versions](#dependency-versions)
  + [Example contract versions](#example-contract-versions)
  + [Language server version](#language-server-version)
* [Updating](#updating)
  + [Steps to keep up to date](#steps-to-keep-up-to-date)
* [Updating Aztec.nr packages](#updating-aztecnr-packages)
  + [Automatic update](#automatic-update)
  + [Manual update](#manual-update)
* [Updating Aztec.js packages](#updating-aztecjs-packages)
* [Local Network PXE Proving](#local-network-pxe-proving)
  + [Local Network in Proving Mode](#local-network-in-proving-mode)
    - [Usage](#usage)
  + [Proving with `aztec-wallet`](#proving-with-aztec-wallet)

## VersionsAztec tools (local network, nargo), dependencies (Aztec.nr), and sample contracts are constantly being improved.
When developing and referring to example .nr files/snippets, it is helpful to verify the versions of different components (below), and if required keep them in lock-step by [updating](#updating).

## Dependency versionsDependency versions in a contract's `Nargo.toml` file correspond to the `aztec-packages` repository tag `aztec-packages` (filter tags by `aztec`...)

If you get an error like: `Cannot read file ~/nargo/github.com/AztecProtocol/aztec-packages/...`
Check the `git=` github url, tag, and directory.

## Example contract versionsExample contracts serve as a helpful reference between versions of the Aztec.nr framework since they are strictly maintained with each release.

Code referenced in the documentation is sourced from contracts within [this directory (GitHub link)](https://github.com/AztecProtocol/aztec-packages/tree/v3.0.0-devnet.20251212/noir-projects/noir-contracts/contracts).

As in the previous section, the location of the noir contracts moved at version `0.24.0`, from `yarn-project/noir-contracts` before, to `noir-projects/noir-contracts`.

tip

Notice the difference between the sample Counter contract from `0.23.0` to `0.24.0` shows the `note_type_id` was added.

```
diff ~/nargo/github.com/AztecProtocol/v0.23.0/yarn-project/noir-contracts/contracts/test/counter_contract/src/main.nr ~/nargo/github.com/AztecProtocol/v0.24.0/noir-projects/noir-contracts/contracts/test/counter_contract/src/main.nr
```

```
57a58  
>         note_type_id: Field,
```

## Language server versionThe [Noir LSP](/developers/docs/aztec-nr/installation) uses your local version of `aztec`, and thus also `aztec compile`.
The path of the former (once installed) can be seen by hovering over "Nargo" in the bottom status bar of VS Code, and the latter via the `which aztec` command.

caution

For Aztec contract files, this should be `aztec` and for noir-only files this should be `nargo`. Mismatching tools and file types will generate misleading syntax and compiler errors.

This can present confusion when opening older contracts (and dependencies) written in older version of noir, such as:

* Logs filled with errors from the dependencies
* Or the LSP fails (re-runs automatically then stops)
  The second point requires a restart of the extension, which you can trigger with the command palette (Ctrl + Shift + P) and typing "Reload Window".

## Updating## Steps to keep up to date1. Update the Aztec local network to the latest version (includes `aztec` command, pxe, etc):

```
aztec-up
```

To update to a specific version, pass the version number after the `aztec-up` command, or set `VERSION` for a particular git tag, eg for [v**0.77.0**](https://github.com/AztecProtocol/aztec-packages/tree/v0.77.0)

```
aztec-up 0.77.0  
# or  
VERSION=0.77.0 aztec-up
```

2. Update Aztec.nr and individual @aztec dependencies:

Inside your project run:

```
cd your/aztec/project  
aztec update . --contract src/contract1 --contract src/contract2
```

The local network must be running for the update command to work. Make sure it is [installed and running](/developers/getting_started_on_local_network).

Follow [updating Aztec.nr packages](#updating-aztecnr-packages) and [updating JavaScript packages](#updating-aztecjs-packages) guides.

3. Refer to [Migration Notes](/developers/docs/resources/migration_notes) on any breaking changes that might affect your dapp

---

There are four components whose versions need to be kept compatible:

1. Aztec local network (includes the `aztec` command)
2. `Aztec.nr`, the Noir framework for writing Aztec contracts

First three are packaged together in docker and are kept compatible by running `aztec-up`.
But you need to update your Aztec.nr version manually or using `aztec update`.

## Updating Aztec.nr packages## Automatic updateYou can update your Aztec.nr packages to the appropriate version with the `aztec update` command. Run this command from the root of your project and pass the paths to the folders containing the Nargo.toml files for your projects like so:

```
aztec update . --contract src/contract1 --contract src/contract2
```

## Manual updateTo update the aztec.nr packages manually, update the tags of the `aztec.nr` dependencies in the `Nargo.toml` file.

```
[dependencies]  
-aztec = { git="https://github.com/AztecProtocol/aztec-packages", tag="v0.7.5", directory="noir-projects/aztec-nr/aztec" }  
+aztec = { git="https://github.com/AztecProtocol/aztec-packages", tag="v3.0.0-devnet.20251212", directory="noir-projects/aztec-nr/aztec" }  
-value_note = { git="https://github.com/AztecProtocol/aztec-packages", tag="v0.7.5", directory="noir-projects/aztec-nr/value-note" }  
+value_note = { git="https://github.com/AztecProtocol/aztec-packages", tag="v3.0.0-devnet.20251212", directory="noir-projects/aztec-nr/value-note" }
```

Go to the contract directory and try compiling it to verify that the update was successful:

```
cd /your/contract/directory  
aztec compile        # compiles the contract
```

If the dependencies fail to resolve ensure that the tag matches a tag in the [aztec-packages repository (GitHub link)](https://github.com/AztecProtocol/aztec-packages/tags).

## Updating Aztec.js packagesTo update Aztec.js packages, go to your `package.json` and replace the versions in the dependencies.

```
[dependencies]  
-"@aztec/accounts": "0.7.5",  
+"@aztec/accounts": "v3.0.0-devnet.20251212",  
-"@aztec/noir-contracts.js": "0.35.1",  
+"@aztec/accounts": "v3.0.0-devnet.20251212",
```

## Local Network PXE ProvingThe local network does not have client-side proving in the PXE enabled by default. This reduces testing times and increases development speed by allowing for rapid iteration.

You may want to enable client-side proving in the local network to better understand how long it takes to execute Aztec transactions. There are 2 ways of doing this:

1. Run the local network in proving mode (every transaction wil be proved) or
2. Use `aztec-wallet` cli to prove a one-off transaction

note

Proving is much slower and should only be used sparingly to analyze real proving times of executing private functions of a contract.

## Local Network in Proving ModeHere every transaction, contract deployment will be proved. If you want to just prove a single transaction, follow [proving with aztec-wallet cli](#proving-with-aztec-wallet).

## UsageTo enable client-side proving:

```
PXE_PROVER_ENABLED=1 aztec start --local-network
```

The local network will take much longer to start. The first time it starts, it will need to download a large crs file, which can take several minutes even on a fast internet connection. This is a one-time operation, you will not need to download it again until you update to a new Aztec version.

The local network will also deploy 3 Schnorr account contracts on startup. The local network will need to generate transaction proofs for deployment, which will take additional time.

Once everything has been set up, you will see that the PXE is listening on `localhost:8080` as you would see with the local network running in the default mode. At this point you can use the local network as you would without client-side proving enabled.

## Proving with `aztec-wallet`You can enable proving on a per-transaction basis using the `aztec-wallet` CLI by setting the `PXE_PROVER_ENABLED` environment variable to `1`. This will use your local `bb` binary to prove the transaction.

```
PXE_PROVER_ENABLED=1 aztec-wallet create-account -a test
```

Check the [Quickstart](/developers/getting_started_on_local_network) for a refresher on how to send transactions using `aztec-wallet` or check the [reference here](/developers/docs/wallet-cli/cli_wallet_reference)

Note that you do not need to restart the local network in order to start sending proven transactions. You can optionally set this for one-off transactions.

If this is the first time you are sending transactions with proving enabled, it will take a while to download a CRS file (which is several MBs) that is required for proving.

note

You can also profile your transactions to get gate count, if you don't want to prove your transactions but check how many constraints it is. Follow the [guide here](/developers/docs/aztec-nr/framework-description/advanced/how_to_profile_transactions)

You can learn more about custom commands in the [CLI reference](/developers/docs/aztec-cli/cli_reference).

---


# Contract Tutorials

Source: https://docs.aztec.network/developers/docs/tutorials/contract_tutorials/counter_contract

Version: Devnet (v3.0.0-devnet.20251212)

On this page

In this guide, we will create our first Aztec.nr smart contract. We will build a simple private counter, where you can keep your own private counter - so no one knows what ID you are at or when you increment! This contract will get you started with the basic setup and syntax of Aztec.nr, but doesn't showcase all of the awesome stuff Aztec is capable of.

This tutorial is compatible with the Aztec version `v3.0.0-devnet.20251212`. Install the correct version with `aztec-up -v 3.0.0-devnet.20251212`. Or if you'd like to use a different version, you can find the relevant tutorial by clicking the version dropdown at the top of the page.

## Prerequisites* You have followed the [quickstart](/developers/getting_started_on_local_network)
* Running Aztec local network
* Installed [Noir LSP](/developers/docs/aztec-nr/installation) (optional)

## Set up a projectRun this to create a new contract project:

```
aztec new --contract counter
```

Your structure should look like this:

```
.  
|-counter  
| |-src  
| | |-main.nr  
| |-Nargo.toml
```

The file `main.nr` will soon turn into our smart contract!

Add the following dependencies to `Nargo.toml` under the autogenerated content:

```
[dependencies]  
aztec = { git="https://github.com/AztecProtocol/aztec-nr/", tag="v3.0.0-devnet.20251212", directory="aztec" }  
balance_set = { git="https://github.com/AztecProtocol/aztec-nr/", tag="v3.0.0-devnet.20251212", directory="balance-set" }
```

## Define the functionsGo to `main.nr`, and replace the boilerplate code with this contract initialization:

```
use dep::aztec::macros::aztec;  
  
#[aztec]  
pub contract Counter {  
}
```

This defines a contract called `Counter`.

## ImportsWe need to define some imports.

Write this inside your contract, ie inside these brackets:

```
pub contract Counter {  
    // imports go here!  
}
```

imports

```
use aztec::{  
    macros::{functions::{external, initializer}, storage::storage},  
    messages::message_delivery::MessageDelivery,  
    oracle::debug_log::debug_log_format,  
    protocol_types::{address::AztecAddress, traits::ToField},  
    state_vars::Owned,  
};  
use balance_set::BalanceSet;
```

> [Source code: docs/examples/contracts/counter\_contract/src/main.nr#L7-L16](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/counter_contract/src/main.nr#L7-L16)

* `macros::{functions::{external, initializer}, storage::storage}`
  Imports the macros needed to define function types (`external`, `initializer`) and the `storage` macro for declaring contract storage structures.
* `messages::message_delivery::MessageDelivery`
  Imports `MessageDelivery` for specifying how note delivery should be handled (e.g., constrained onchain delivery).
* `oracle::debug_log::debug_log_format`
  Imports a debug logging utility for printing formatted messages during contract execution.
* `protocol_types::{address::AztecAddress, traits::ToField}`
  Brings in `AztecAddress` (used to identify accounts/contracts) and traits for converting values to field elements, necessary for serialization and formatting inside Aztec.
* `state_vars::Owned`
  Brings in `Owned`, a wrapper for state variables that have a single owner.
* `use balance_set::BalanceSet`
  Imports `BalanceSet` from the `balance_set` dependency, which provides functionality for managing private balances (used for our counter).

## Declare storageAdd this below the imports. It declares the storage variables for our contract. We use an `Owned` state variable wrapping a `BalanceSet` to manage private balances for each owner.

storage\_struct

```
#[storage]  
struct Storage<Context> {  
    counters: Owned<BalanceSet<Context>, Context>,  
}
```

> [Source code: docs/examples/contracts/counter\_contract/src/main.nr#L18-L23](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/counter_contract/src/main.nr#L18-L23)

## Keep the counter privateNow we’ve got a mechanism for storing our private state, we can start using it to ensure the privacy of balances.

Let’s create a constructor method to run on deployment that assigns an initial count to a specified owner. This function is called `initialize`, but behaves like a constructor. It is the `#[initializer]` decorator that specifies that this function behaves like a constructor. Write this:

constructor

```
#[initializer]  
#[external("private")]  
// We can name our initializer anything we want as long as it's marked as aztec(initializer)  
fn initialize(headstart: u64, owner: AztecAddress) {  
    self.storage.counters.at(owner).add(headstart as u128).deliver(  
        MessageDelivery.CONSTRAINED_ONCHAIN,  
    );  
}
```

> [Source code: docs/examples/contracts/counter\_contract/src/main.nr#L25-L34](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/counter_contract/src/main.nr#L25-L34)

This function accesses the counters from storage. It adds the `headstart` value to the `owner`'s counter using `at().add()`, then calls `.deliver(MessageDelivery.CONSTRAINED_ONCHAIN)` to ensure the note is delivered onchain.

We have annotated this and other functions with `#[external("private")]` which are ABI macros so the compiler understands it will handle private inputs.

## Incrementing our counterNow let's implement an `increment` function to increase the counter.

increment

```
#[external("private")]  
fn increment(owner: AztecAddress) {  
    debug_log_format("Incrementing counter for owner {0}", [owner.to_field()]);  
    self.storage.counters.at(owner).add(1).deliver(MessageDelivery.CONSTRAINED_ONCHAIN);  
}
```

> [Source code: docs/examples/contracts/counter\_contract/src/main.nr#L36-L42](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/counter_contract/src/main.nr#L36-L42)

The `increment` function works similarly to the `initialize` function. It logs a debug message, then adds 1 to the owner's counter and delivers the note onchain.

## Getting a counterThe last thing we need to implement is a function to retrieve a counter value.

get\_counter

```
#[external("utility")]  
unconstrained fn get_counter(owner: AztecAddress) -> pub u128 {  
    self.storage.counters.at(owner).balance_of()  
}
```

> [Source code: docs/examples/contracts/counter\_contract/src/main.nr#L44-L49](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/counter_contract/src/main.nr#L44-L49)

This is a `utility` function used to obtain the counter value outside of a transaction. We access the `owner`'s balance from the `counters` storage variable using `at(owner)`, then call `balance_of()` to retrieve the current count. This yields a private counter that only the owner can decrypt.

## CompileNow we've written a simple Aztec.nr smart contract, we can compile it.

## Compile the smart contractIn the `./counter/` directory, run:

```
aztec compile
```

This command compiles your Noir contract and creates a `target` folder with a `.json` artifact inside.

After compiling, you can generate a TypeScript class using the `aztec codegen` command.

In the same directory, run this:

```
aztec codegen -o src/artifacts target
```

You can now use the artifact and/or the TS class in your Aztec.js!

## Next Steps## Optional: Learn more about concepts mentioned here* [Functions and annotations like `#[external("private")]`](/developers/docs/aztec-nr/framework-description/functions/function_transforms#private-functions)

---


# Private Token Contract

Source: https://docs.aztec.network/developers/docs/tutorials/contract_tutorials/token_contract

Version: Devnet (v3.0.0-devnet.20251212)

On this page

## The Privacy Challenge: Mental Health Benefits at GiggleGiggle (a fictional tech company) wants to support their employees' mental health by providing BOB tokens that can be spent at Bob's Psychology Clinic. However, employees have a crucial requirement: **complete privacy**. They don't want Giggle to know:

* How many BOB tokens they've actually used
* When they're using mental health services
* Their therapy patterns or frequency

In this tutorial, we'll build a token contract that allows Giggle to mint BOB tokens for employees while ensuring complete privacy in how those tokens are spent.

## PrerequisitesThis is an intermediate tutorial that assumes you have:

* Completed the [Counter Contract tutorial](/developers/docs/tutorials/contract_tutorials/counter_contract)
* A Running Aztec local network (see the Counter tutorial for setup)
* Basic understanding of Aztec.nr syntax and structure
* Aztec toolchain installed (`aztec-up -v 3.0.0-devnet.20251212`)

If you haven't completed the Counter Contract tutorial, please do so first as we'll skip the basic setup steps covered there.

## What We're BuildingWe'll create BOB tokens with:

* **Public and Private minting**: Giggle can mint tokens in private or public
* **Public and Private transfers**: Employees can spend tokens at Bob's clinic with full privacy

## Project SetupLet's create a simple yarn + aztec.nr project:

```
mkdir bob_token_contract  
cd bob_token_contract  
yarn init  
# This is to ensure yarn uses node_modules instead of pnp for dependency installation  
yarn config set nodeLinker node-modules  
yarn add @aztec/aztec.js@v3.0.0-devnet.20251212 @aztec/accounts@v3.0.0-devnet.20251212 @aztec/test-wallet@v3.0.0-devnet.20251212 @aztec/kv-store@v3.0.0-devnet.20251212  
aztec init
```

## Contract structureWe have a messy, but working structure. In `src/main.nr` we even have a proto-contract. Let's replace it with a simple starting point:

```
use aztec::macros::aztec;  
  
#[aztec]  
pub contract BobToken {  
    // We'll build the mental health token here  
}
```

The `#[aztec]` macro transforms our contract code to work with Aztec's privacy protocol. We'll rename it from `StarterToken` to `BobToken` to reflect our use case.

Let's import the Aztec.nr library by adding it to our dependencies in `Nargo.toml`:

```
[package]  
name = "bob_token_contract"  
type = "contract"  
  
[dependencies]  
aztec = { git = "https://github.com/AztecProtocol/aztec-nr/", tag = "v3.0.0-devnet.20251212", directory = "aztec" }
```

Since we're here, let's import more specific stuff from this library:

```
#[aztec]  
pub contract BobToken {  
    use aztec::{  
        macros::{functions::{external, initializer, only_self}, storage::storage},  
        messages::message_delivery::MessageDelivery,  
        protocol_types::address::AztecAddress,  
        state_vars::{Map, Owned},  
        state_vars::public_mutable::PublicMutable,  
    };  
}
```

These are the different macros we need to define the visibility of functions, and some handy types and functions.

## Building the Mental Health Token System## The Privacy ArchitectureBefore we start coding, let's understand how privacy works in our mental health token system:

1. **Public Layer**: Giggle mints tokens publicly - transparent and auditable
2. **Private Layer**: Employees transfer and spend tokens privately - completely confidential
3. **Cross-layer Transfer**: Employees can move tokens between public and private domains as needed

This architecture ensures that while the initial allocation is transparent (important for corporate governance), the actual usage remains completely private.

Privacy Note

In Aztec, private state uses a UTXO model with "notes" - think of them as encrypted receipts that only the owner can decrypt and spend. When an employee receives BOB tokens privately, they get encrypted notes that only they can see and use.

Let's start building! Remember to import types as needed - your IDE's Noir extension can help with auto-imports.

## Part 1: Public Minting for TransparencyLet's start with the public components that Giggle will use to mint and track initial token allocations.

## Setting Up StorageFirst, define the storage for our BOB tokens:

```
#[storage]  
struct Storage<Context> {  
    // Giggle's admin address  
    owner: PublicMutable<AztecAddress, Context>,  
    // Public balances - visible for transparency  
    public_balances: Map<AztecAddress, PublicMutable<u64, Context>, Context>,  
}
```

This storage structure allows:

* `owner`: Stores Giggle's admin address (who can mint tokens)
* `public_balances`: Tracks public token balances (employees can verify their allocations)

Why Public Balances?

While employees want privacy when spending, having public balances during minting allows:

1. Employees to verify they received their mental health benefits
2. Auditors to confirm fair distribution
3. Transparency in the allocation process

## Initializing Giggle as OwnerWhen deploying the contract, we need to set Giggle as the owner:

setup

```
#[initializer]  
#[external("public")]  
fn setup() {  
    // Giggle becomes the owner who can mint mental health tokens  
    self.storage.owner.write(self.msg_sender().unwrap());  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L33-L40](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L33-L40)

The `#[initializer]` decorator ensures this runs once during deployment. Only Giggle's address will have the power to mint new BOB tokens for employees.

## Minting BOB Tokens for EmployeesGiggle needs a way to allocate mental health tokens to employees:

mint\_public

```
#[external("public")]  
fn mint_public(employee: AztecAddress, amount: u64) {  
    // Only Giggle can mint tokens  
    assert_eq(self.msg_sender().unwrap(), self.storage.owner.read(), "Only Giggle can mint BOB tokens");  
  
    // Add tokens to employee's public balance  
    let current_balance = self.storage.public_balances.at(employee).read();  
    self.storage.public_balances.at(employee).write(current_balance + amount);  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L42-L52](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L42-L52)

This public minting function:

1. Verifies that only Giggle (the owner) is calling
2. Transparently adds tokens to the employee's public balance
3. Creates an auditable record of the allocation

Real-World Scenario

Imagine Giggle allocating 100 BOB tokens to each employee at the start of the year. This public minting ensures employees can verify they received their benefits, while their actual usage remains private.

## Public Transfers (Optional Transparency)")

While most transfers will be private, we'll add public transfers for cases where transparency is desired:

transfer\_public

```
#[external("public")]  
fn transfer_public(to: AztecAddress, amount: u64) {  
    let sender = self.msg_sender().unwrap();  
    let sender_balance = self.storage.public_balances.at(sender).read();  
    assert(sender_balance >= amount, "Insufficient BOB tokens");  
  
    // Deduct from sender  
    self.storage.public_balances.at(sender).write(sender_balance - amount);  
  
    // Add to recipient  
    let recipient_balance = self.storage.public_balances.at(to).read();  
    self.storage.public_balances.at(to).write(recipient_balance + amount);  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L54-L68](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L54-L68)

This might be used when:

* An employee transfers tokens to a colleague who's comfortable with transparency
* Bob's clinic makes a public refund
* Any scenario where privacy isn't required

## Admin Transfer (Future-Proofing)")

In case Giggle's mental health program administration changes:

transfer\_ownership

```
#[external("public")]  
fn transfer_ownership(new_owner: AztecAddress) {  
    assert_eq(self.msg_sender().unwrap(), self.storage.owner.read(), "Only current admin can transfer ownership");  
    self.storage.owner.write(new_owner);  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L70-L76](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L70-L76)

## Your First Deployment - Let's See It Work## Compile Your ContractYou've written enough code to have a working token! Let's compile and test it:

```
aztec compile
```

## Generate TypeScript Interface```
aztec codegen target --outdir artifacts
```

You should now have a nice typescript interface in a new `artifacts` folder. Pretty useful!

## Deploy and TestCreate `index.ts`. We will connect to our running local network and its wallet, then deploy the test accounts and get three wallets out of it. Ensure that your local network is running:

```
aztec start --local-network
```

Then we will use the `giggleWallet` to deploy our contract, mint 100 BOB to Alice, then transfer 10 of those to Bob's Clinic publicly... for now. Let's go:

```
import { BobTokenContract } from './artifacts/BobToken.js';  
import { AztecAddress } from '@aztec/aztec.js/addresses';  
import { createAztecNodeClient } from '@aztec/aztec.js/node';  
import { getInitialTestAccountsData } from '@aztec/accounts/testing';  
import { TestWallet } from '@aztec/test-wallet/server';  
import { openTmpStore } from '@aztec/kv-store/lmdb';  
  
async function main() {  
    // Connect to local network  
    const node = createAztecNodeClient('http://localhost:8080');  
  
    const store = await openTmpStore();  
  
    const wallet = await TestWallet.create(node);  
  
    const [giggleWalletData, aliceWalletData, bobClinicWalletData] = await getInitialTestAccountsData();  
    const giggleAccount = await wallet.createSchnorrAccount(giggleWalletData.secret, giggleWalletData.salt);  
    const aliceAccount = await wallet.createSchnorrAccount(aliceWalletData.secret, aliceWalletData.salt);  
    const bobClinicAccount = await wallet.createSchnorrAccount(bobClinicWalletData.secret, bobClinicWalletData.salt);  
  
    const giggleAddress = (await giggleAccount.getAccount()).getAddress();  
    const aliceAddress = (await aliceAccount.getAccount()).getAddress();  
    const bobClinicAddress = (await bobClinicAccount.getAccount()).getAddress();  
  
    const bobToken = await BobTokenContract  
        .deploy(  
            wallet,  
        )  
        .send({ from: giggleAddress })  
        .deployed();  
  
    await bobToken.methods  
        .mint_public(aliceAddress, 100n)  
        .send({ from: giggleAddress })  
        .wait();  
  
    await bobToken.methods  
        .transfer_public(bobClinicAddress, 10n)  
        .send({ from: aliceAddress })  
        .wait();  
}  
  
main().catch(console.error);
```

Run your test:

```
npx tsx index.ts
```

tip

What's this `tsx` dark magic? Well, it just compiles and runs typescript using reasonable defaults. Pretty cool for small snippets like this!

## 🎉 CelebrateCongratulations! You've just deployed a working token contract on Aztec! You can:

* ✅ Mint BOB tokens as Giggle
* ✅ Transfer tokens between employees
* ✅ Track balances publicly

But there's a problem... **Giggle can see everything!** They know:

* Who's transferring tokens
* How much is being spent
* When mental health services are being used

This defeats the whole purpose of our mental health privacy initiative. Let's fix this by adding private functionality!

## Part 2: Adding Privacy - The Real Magic BeginsNow let's add the privacy features that make our mental health benefits truly confidential.

## Understanding Private NotesHere's where Aztec's privacy magic happens. Unlike public balances (a single number), private balances are collections of encrypted "notes". Think of it this way:

* **Public balance**: "Alice has 100 BOB tokens" (visible to everyone)
* **Private balance**: Alice has encrypted notes [Note1: 30 BOB, Note2: 50 BOB, Note3: 20 BOB] that only she can decrypt

When Alice spends 40 BOB tokens at Bob's clinic:

1. She consumes Note1 (30 BOB) and Note2 (50 BOB) = 80 BOB total
2. She creates a new note for Bob's clinic (40 BOB)
3. She creates a "change" note for herself (40 BOB)
4. The consumed notes are nullified (marked as spent)

In this case, all that the network sees (including Giggle) is just "something happening to some state in some contract". How cool is that?

## Updating Storage for PrivacyFor something like balances, you can use a simple library called `balance_set` which abstracts away a custom private Note. A Note is at the core of how private state works in Aztec and you can read about it [here](/developers/docs/foundational-topics/state_management). For now, let's just import the library in `Nargo.toml`:

```
[dependencies]  
aztec = { git="https://github.com/AztecProtocol/aztec-nr", tag="v3.0.0-devnet.20251212", directory="aztec" }  
balance_set = { git = "https://github.com/AztecProtocol/aztec-nr/", tag = "v3.0.0-devnet.20251212", directory = "balance-set" }
```

Then import `BalanceSet` in our contract:

```
use aztec::macros::aztec;  
  
#[aztec]  
pub contract BobToken {  
    // ... other imports  
    use balance_set::BalanceSet;  
    // ...  
}
```

We need to update the contract storage to have private balances as well:

storage

```
#[storage]  
struct Storage<Context> {  
    // Giggle's admin address  
    owner: PublicMutable<AztecAddress, Context>,  
    // Public balances - visible for transparency  
    public_balances: Map<AztecAddress, PublicMutable<u64, Context>, Context>,  
    // Private balances - only the owner can see these  
    private_balances: Owned<BalanceSet<Context>, Context>,  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L20-L31](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L20-L31)

The `private_balances` use `BalanceSet` which manages encrypted notes automatically.

## Moving Tokens to PrivatelandGreat, now our contract knows about private balances. Let's implement a method to allow users to move their publicly minted tokens there:

public\_to\_private

```
#[external("private")]  
fn public_to_private(amount: u64) {  
    let sender = self.msg_sender().unwrap();  
    // This will enqueue a public function to deduct from public balance  
    self.enqueue_self._deduct_public_balance(sender, amount);  
    // Add to private balance  
    self.storage.private_balances.at(sender).add(amount as u128).deliver(  
        MessageDelivery.CONSTRAINED_ONCHAIN,  
    );  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L78-L89](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L78-L89)

And the helper function:

\_deduct\_public\_balance

```
#[external("public")]  
#[only_self]  
fn _deduct_public_balance(owner: AztecAddress, amount: u64) {  
    let balance = self.storage.public_balances.at(owner).read();  
    assert(balance >= amount, "Insufficient public BOB tokens");  
    self.storage.public_balances.at(owner).write(balance - amount);  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L91-L99](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L91-L99)

By calling `public_to_private` we're telling the network "deduct this amount from my balance" while simultaneously creating a Note with that balance in privateland.

## Private TransfersNow for the crucial privacy feature - transferring BOB tokens in privacy. This is actually pretty simple:

transfer\_private

```
#[external("private")]  
fn transfer_private(to: AztecAddress, amount: u64) {  
    let sender = self.msg_sender().unwrap();  
    // Spend sender's notes (consumes existing notes)  
    self.storage.private_balances.at(sender).sub(amount as u128).deliver(  
        MessageDelivery.CONSTRAINED_ONCHAIN,  
    );  
    // Create new notes for recipient  
    self.storage.private_balances.at(to).add(amount as u128).deliver(  
        MessageDelivery.CONSTRAINED_ONCHAIN,  
    );  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L102-L115](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L102-L115)

This function simply nullifies the sender's notes, while adding them to the recipient.

Real-World Impact

When an employee uses 50 BOB tokens at Bob's clinic, this private transfer ensures Giggle has no visibility into:

* The fact that the employee is seeking mental health services
* The frequency of visits
* The amount spent on treatment

## Checking BalancesEmployees can check their BOB token balances without hitting the network by using utility unconstrained functions:

check\_balances

```
#[external("utility")]  
unconstrained fn private_balance_of(owner: AztecAddress) -> pub u128 {  
    self.storage.private_balances.at(owner).balance_of()  
}  
  
#[external("utility")]  
unconstrained fn public_balance_of(owner: AztecAddress) -> pub u64 {  
    self.storage.public_balances.at(owner).read()  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L117-L127](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L117-L127)

## Part 3: Securing Private MintingLet's make this a little bit harder, and more interesting. Let's say Giggle doesn't want to mint the tokens in public. Can we have private minting on Aztec?

Sure we can. Let's see.

## Understanding Execution DomainsOur BOB token system operates in two domains:

1. **Public Domain**: Where Giggle mints tokens transparently
2. **Private Domain**: Where employees spend tokens confidentially

The key challenge: How do we ensure only Giggle can mint tokens when the minting happens in a private function?

Privacy Trade-off

Private functions can't directly read current public state (like who the owner is). They can only read historical public state or enqueue public function calls for validation.

## The Access Control ChallengeWe want Giggle to mint BOB tokens directly to employees' private balances (for maximum privacy), but we need to ensure only Giggle can do this. The challenge: ownership is stored publicly, but private functions can't read current public state.

Let's use a clever pattern where private functions enqueue public validation checks. First we make a little helper function in public. Remember, public functions always run *after* private functions, since private functions run client-side.

\_assert\_is\_owner

```
#[external("public")]  
#[only_self]  
fn _assert_is_owner(address: AztecAddress) {  
    assert_eq(address, self.storage.owner.read(), "Only Giggle can mint BOB tokens");  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L129-L135](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L129-L135)

Now we can add a secure private minting function. It looks pretty easy, and it is, since the whole thing will revert if the public function fails:

mint\_private

```
#[external("private")]  
fn mint_private(employee: AztecAddress, amount: u64) {  
    // Enqueue ownership check (will revert if not Giggle)  
    self.enqueue_self._assert_is_owner(self.msg_sender().unwrap());  
  
    // If check passes, mint tokens privately  
    self.storage.private_balances.at(employee).add(amount as u128).deliver(  
        MessageDelivery.CONSTRAINED_ONCHAIN,  
    );  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L137-L148](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L137-L148)

This pattern ensures:

1. The private minting executes first (creating the proof)
2. The public ownership check executes after
3. If the check fails, the entire transaction (including the private part) reverts
4. Only Giggle can successfully mint BOB tokens

## Part 4: Converting Back to PublicFor the sake of completeness, let's also have a function that brings the tokens back to publicland:

private\_to\_public

```
#[external("private")]  
fn private_to_public(amount: u64) {  
    let sender = self.msg_sender().unwrap();  
    // Remove from private balance  
    self.storage.private_balances.at(sender).sub(amount as u128).deliver(  
        MessageDelivery.CONSTRAINED_ONCHAIN,  
    );  
    // Enqueue public credit  
    self.enqueue_self._credit_public_balance(sender, amount);  
}  
  
#[external("public")]  
#[only_self]  
fn _credit_public_balance(owner: AztecAddress, amount: u64) {  
    let balance = self.storage.public_balances.at(owner).read();  
    self.storage.public_balances.at(owner).write(balance + amount);  
}
```

> [Source code: docs/examples/contracts/bob\_token\_contract/src/main.nr#L151-L169](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/bob_token_contract/src/main.nr#L151-L169)

Now you've made changes to your contract, you need to recompile your contract.

Here are the steps from above, for reference:

```
aztec compile  
aztec codegen target --outdir artifacts
```

## Testing the Complete Privacy SystemNow that you've implemented all the privacy features, let's update our test script to showcase the full privacy flow:

## Update Your Test ScriptLet's stop being lazy and add a nice little "log" function that just spits out everyone's balances to the console, for example:

```
// at the top of your file  
async function getBalances(contract: BobTokenContract, aliceAddress: AztecAddress, bobAddress: AztecAddress) {  
    Promise.all([  
        contract.methods  
            .public_balance_of(aliceAddress)  
            .simulate({ from: aliceAddress }),  
        contract.methods  
            .private_balance_of(aliceAddress)  
            .simulate({ from: aliceAddress }),  
        contract.methods  
            .public_balance_of(bobAddress)  
            .simulate({ from: bobAddress }),  
        contract.methods  
            .private_balance_of(bobAddress)  
            .simulate({ from: bobAddress })  
    ]).then(([alicePublicBalance, alicePrivateBalance, bobPublicBalance, bobPrivateBalance]) => {  
        console.log(`📊 Alice has ${alicePublicBalance} public BOB tokens and ${alicePrivateBalance} private BOB tokens`);  
        console.log(`📊 Bob's Clinic has ${bobPublicBalance} public BOB tokens and ${bobPrivateBalance} private BOB tokens`);  
    });  
}
```

Looks ugly but it does what it says: prints Alice's and Bob's balances. This will make it easier to see our contract working.

Now let's add some more stuff to our `index.ts`:

```
async function main() {  
    // ...etc  
    await bobToken.methods  
        .mint_public(aliceAddress, 100n)  
        .send({ from: giggleAddress })  
        .wait();  
    await getBalances(bobToken, aliceAddress, bobClinicAddress);  
  
    await bobToken.methods  
        .transfer_public(bobClinicAddress, 10n)  
        .send({ from: aliceAddress })  
        .wait();  
    await getBalances(bobToken, aliceAddress, bobClinicAddress);  
  
    await bobToken.methods  
        .public_to_private(90n)  
        .send({ from: aliceAddress })  
        .wait();  
    await getBalances(bobToken, aliceAddress, bobClinicAddress);  
  
    await bobToken.methods  
        .transfer_private(bobClinicAddress, 50n)  
        .send({ from: aliceAddress })  
        .wait();  
    await getBalances(bobToken, aliceAddress, bobClinicAddress);  
  
    await bobToken.methods  
        .private_to_public(10n)  
        .send({ from: aliceAddress })  
        .wait();  
    await getBalances(bobToken, aliceAddress, bobClinicAddress);  
  
    await bobToken.methods  
        .mint_private(aliceAddress, 100n)  
        .send({ from: giggleAddress })  
        .wait();  
    await getBalances(bobToken, aliceAddress, bobClinicAddress);  
}  
  
main().catch(console.error);
```

The flow is something like:

* Giggle mints Alice 100 BOB in public
* Alice transfers 10 BOB to Bob in public
* Alice makes the remaining 90 BOB private
* Alice transfers 50 of those to Bob, in private
* Of the remaining 40 BOB, she makes 10 public again
* Giggle mints 100 BOB tokens for Alice, in private

Let's give it a try:

```
npx tsx index.ts
```

You should see the complete privacy journey from transparent allocation to confidential usage!

## SummaryYou've built a privacy-preserving token system that solves a real-world problem: enabling corporate mental health benefits while protecting employee privacy. This demonstrates Aztec's unique ability to provide both transparency and privacy where each is most needed.

The BOB token shows how blockchain can enable new models of corporate benefits that weren't possible before - where verification and privacy coexist, empowering employees to seek help without fear of judgment or career impact.

## What You Learned* How to create tokens with both public and private states
* How to bridge between public and private domains
* How to implement access control across execution contexts
* How to build real-world privacy solutions on Aztec

## Continue Your Journey* Explore [cross-chain communication](/developers/docs/aztec-nr/framework-description/ethereum-aztec-messaging) to integrate with existing health systems
* Learn about [account abstraction](/developers/docs/foundational-topics/accounts) for recovery mechanisms

---


# Full-Stack Tutorials

Source: https://docs.aztec.network/developers/docs/tutorials/js_tutorials/aztecjs-getting-started

Version: Devnet (v3.0.0-devnet.20251212)

On this page

In this guide, we will retrieving the local network and deploy a pre-written token contract to it using Aztec.js. [Check out the source code](https://github.com/AztecProtocol/aztec-packages/blob/master/noir-projects/noir-contracts/contracts/app/token_contract/src/main.nr). We will then use Aztec.js to interact with this contract and transfer tokens.

Before starting, make sure to be running Aztec local network at version 3.0.0-devnet.20251212. Check out [the guide](/developers/docs/tutorials/local_network) for info about that.

## Set up the projectFirst, create a new directory for your project and initialize it with yarn:

```
mkdir token-tutorial  
cd token-tutorial  
yarn init -y
```

Next, add the TypeScript dependencies:

```
yarn add typescript @types/node tsx
```

tip

Never heard of `tsx`? Well, it will just run `typescript` with reasonable defaults. Pretty cool for a small example like this one. You may want to tune in your own project's `tsconfig.json` later!

Let's also import the Aztec dependencies for this tutorial:

```
yarn add @aztec/aztec.js@3.0.0-devnet.20251212 @aztec/accounts@3.0.0-devnet.20251212 @aztec/noir-contracts.js@3.0.0-devnet.20251212 @aztec/test-wallet@3.0.0-devnet.20251212
```

Aztec.js assumes your project is using ESM, so make sure you add `"type": "module"` to `package.json`. You probably also want at least a `start` script. For example:

```
{  
  "type": "module",  
  "scripts": {  
    "start": "tsx index.ts"  
  }  
}
```

## Connecting to the local networkNow let's connect to the Aztec local network and set up test accounts.

**Step 1: Start the Aztec Local Network**

In a separate terminal, run:

```
aztec start --local-network
```

Keep this terminal running throughout the tutorial.

**Step 2: Create the index.ts file**

Create an `index.ts` file in the root of your project with the following code. This connects to the local network and imports test accounts (Alice and Bob):

```
import { createAztecNodeClient } from "@aztec/aztec.js/node";  
import { TestWallet } from "@aztec/test-wallet/server";  
import { getInitialTestAccountsData } from "@aztec/accounts/testing";  
  
const nodeUrl = "http://localhost:8080";  
const node = createAztecNodeClient(nodeUrl);  
const wallet = await TestWallet.create(node);  
  
const [alice, bob] = await getInitialTestAccountsData();  
await wallet.createSchnorrAccount(alice.secret, alice.salt);  
await wallet.createSchnorrAccount(bob.secret, bob.salt);
```

**Step 3: Verify the script runs**

Run the script to make sure everything is set up correctly:

```
yarn start
```

If there are no errors, you're ready to continue. For more details on connecting to the local network, see [this guide](/developers/docs/aztec-js/how_to_connect_to_local_network).

## Deploy the token contractNow that we have our accounts loaded, let's deploy a pre-compiled token contract from the Aztec library. You can find the full code for the contract [here (GitHub link)](https://github.com/AztecProtocol/aztec-packages/tree/v3.0.0-devnet.20251212/noir-projects/noir-contracts/contracts/app/token_contract/src).

Add the following to `index.ts` to import the contract and deploy it with Alice as the admin:

```
import { TokenContract } from "@aztec/noir-contracts.js/Token";  
  
const token = await TokenContract.deploy(  
  wallet,  
  alice.address,  
  "TokenName",  
  "TKN",  
  18  
)  
  .send({ from: alice.address })  
  .deployed();
```

## Mint and transferLet's go ahead and have Alice mint herself some tokens, in private:

```
await token.methods  
  .mint_to_private(alice.address, 100)  
  .send({ from: alice.address })  
  .wait();
```

Let's check both Alice's and Bob's balances now:

```
let aliceBalance = await token.methods  
  .balance_of_private(alice.address)  
  .simulate({ from: alice.address });  
console.log(`Alice's balance: ${aliceBalance}`);  
let bobBalance = await token.methods  
  .balance_of_private(bob.address)  
  .simulate({ from: bob.address });  
console.log(`Bob's balance: ${bobBalance}`);
```

Alice should have 100 tokens, while Bob has none yet.

Great! Let's have Alice transfer some tokens to Bob, also in private:

```
await token.methods  
  .transfer(bob.address, 10)  
  .send({ from: alice.address })  
  .wait();  
bobBalance = await token.methods  
  .balance_of_private(bob.address)  
  .simulate({ from: bob.address });  
console.log(`Bob's balance: ${bobBalance}`);
```

Bob should now see 10 tokens in his balance.

## Other cool thingsSay that Alice is nice and wants to set Bob as a minter. Even though it's a public function, it can be called in a similar way:

```
await token.methods  
  .set_minter(bob.address, true)  
  .send({ from: alice.address })  
  .wait();
```

Bob is now the minter, so he can mint some tokens to himself:

```
await token.methods  
  .mint_to_private(bob.address, 100)  
  .send({ from: bob.address })  
  .wait();  
bobBalance = await token.methods  
  .balance_of_private(bob.address)  
  .simulate({ from: bob.address });  
console.log(`Bob's balance: ${bobBalance}`);
```

info

Have a look at the [contract source](https://github.com/AztecProtocol/aztec-packages/blob/master/noir-projects/noir-contracts/contracts/app/token_contract/src/main.nr). Notice is that the `mint_to_private` function we used above actually starts a partial note. This allows the total balance to increase while keeping the recipient private! How cool is that?

---


# Bridge Your NFT to Aztec

Source: https://docs.aztec.network/developers/docs/tutorials/js_tutorials/token_bridge

Version: Devnet (v3.0.0-devnet.20251212)

On this page

## Why Bridge an NFT?Imagine you own a CryptoPunk NFT on Ethereum. You want to use it in games, social apps, or DeFi protocols, but gas fees on Ethereum make every interaction expensive. What if you could move your Punk to Aztec (L2), use it **privately** in dozens of applications, and then bring it back to Ethereum when you're ready to sell?

In this tutorial, you'll build a **private NFT bridge**. By the end, you'll understand how **portals** work and how **cross-chain messages** flow between L1 and L2.

Before starting, make sure you have the Aztec local network running at version v3.0.0-devnet.20251212. Check out [the local network guide](/developers/getting_started_on_local_network) for setup instructions.

## What You'll BuildYou'll create two contracts with **privacy at the core**:

* **NFTPunk (L2)** - An NFT contract with encrypted ownership using `PrivateSet`
* **NFTBridge (L2)** - A bridge that mints NFTs privately when claiming L1 messages

This tutorial focuses on the L2 side to keep things manageable. You'll learn the essential privacy patterns that apply to any asset bridge on Aztec.

## Project SetupLet's start simple. Since this is an Ethereum project, it's easier to just start with Hardhat:

```
git clone https://github.com/critesjosh/hardhat-aztec-example
```

You're cloning a repo here to make it easier for Aztec's `l1-contracts` to be mapped correctly. You should now have a `hardhat-aztec-example` folder with Hardhat's default starter, with a few changes in `package.json`.

We want to add a few more dependencies now before we start:

```
cd hardhat-aztec-example  
yarn add @aztec/aztec.js@3.0.0-devnet.20251212 @aztec/accounts@3.0.0-devnet.20251212 @aztec/stdlib@3.0.0-devnet.20251212 @aztec/test-wallet@3.0.0-devnet.20251212 tsx
```

Now start the local network in another terminal:

```
aztec start --local-network
```

This should start two important services on ports 8080 and 8545, respectively: Aztec and Anvil (an Ethereum development node).

## Part 1: Building the NFT ContractLet's start with a basic NFT contract on Aztec. That's the representation of the NFT locked on the L2 side:

Let's create that crate in the `contracts` folder so it looks tidy:

```
aztec new contracts/aztec/nft  
cd contracts/aztec/nft
```

Noir Language Server

If you're using VS Code, install the [Noir Language Support extension](https://marketplace.visualstudio.com/items?itemName=noir-lang.vscode-noir) for syntax highlighting, error checking, and code completion while writing Noir contracts.

Open `Nargo.toml` and make sure `aztec` is a dependency:

```
[dependencies]  
aztec = { git = "https://github.com/AztecProtocol/aztec-nr", tag = "v3.0.0-devnet.20251212", directory = "aztec" }
```

## Create the NFT NoteFirst, let's create a custom note type for private NFT ownership. In the `src/` directory, create a new file called `nft.nr`:

```
touch src/nft.nr
```

In this file, you're going to create a **private note** that represents NFT ownership. This is a struct with macros that indicate it is a note that can be compared and packed:

nft\_note\_struct

```
use dep::aztec::{macros::notes::note, protocol_types::traits::Packable};  
  
#[derive(Eq, Packable)]  
#[note]  
pub struct NFTNote {  
    pub token_id: Field,  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft/src/nft.nr#L1-L16](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft/src/nft.nr#L1-L16)

You now have a note that represents the owner of a particular NFT. Next, move on to the contract itself.

Custom Notes

Notes are powerful concepts. Learn more about how to use them in the [state management guide](/developers/docs/foundational-topics/state_management).

## Define StorageBack in `main.nr`, you can now build the contract storage. You need:

* **admin**: Who controls the contract (set once, never changes)
* **minter**: The bridge address (set once by admin)
* **nfts**: Track which NFTs exist (public, needed for bridging)
* **owners**: Private ownership using the NFTNote

One interesting aspect of this storage configuration is the use of `DelayedPublicMutable`, which allows private functions to read and use public state. You're using it to publicly track which NFTs are already minted while keeping their owners private. Read more about `DelayedPublicMutable` in [the storage guide](/developers/docs/aztec-nr/framework-description/how_to_define_storage).

Write the storage struct and a simple [initializer](/developers/docs/foundational-topics/contract_creation#initialization) to set the admin in the `main.nr` file:

contract\_setup

```
use aztec::macros::aztec;  
pub mod nft;  
  
#[aztec]  
pub contract NFTPunk {  
    use crate::nft::NFTNote;  
    use dep::aztec::{  
        macros::{functions::{external, initializer, only_self}, storage::storage},  
        protocol_types::address::AztecAddress,  
        state_vars::{  
            delayed_public_mutable::DelayedPublicMutable, Map, Owned, PrivateSet, PublicImmutable,  
        },  
    };  
    use dep::aztec::messages::message_delivery::MessageDelivery;  
    use aztec::note::{  
        note_getter_options::NoteGetterOptions, note_interface::NoteProperties,  
        note_viewer_options::NoteViewerOptions,  
    };  
    use aztec::utils::comparison::Comparator;  
  
    #[storage]  
    struct Storage<Context> {  
        admin: PublicImmutable<AztecAddress, Context>,  
        minter: PublicImmutable<AztecAddress, Context>,  
        nfts: Map<Field, DelayedPublicMutable<bool, 2, Context>, Context>,  
        owners: Owned<PrivateSet<NFTNote, Context>, Context>,  
    }  
    #[external("public")]  
    #[initializer]  
    fn constructor(admin: AztecAddress) {  
        self.storage.admin.initialize(admin);  
    }
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft/src/main.nr#L1-L29](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft/src/main.nr#L1-L29)

## Utility FunctionsAdd an internal function to handle the `DelayedPublicMutable` value change. Mark the function as public and `#[only_self]` so only the contract can call it:

mark\_nft\_exists

```
#[external("public")]  
#[only_self]  
fn _mark_nft_exists(token_id: Field, exists: bool) {  
    self.storage.nfts.at(token_id).schedule_value_change(exists);  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft/src/main.nr#L39-L45](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft/src/main.nr#L39-L45)

This function is marked with `#[only_self]`, meaning only the contract itself can call it. It uses `schedule_value_change` to update the `nfts` storage, preventing the same NFT from being minted twice or burned when it doesn't exist. You'll call this public function from a private function later using `enqueue_self`.

Another useful function checks how many notes a caller has. You can use this later to verify the claim and exit from L2:

notes\_of

```
#[external("utility")]  
unconstrained fn notes_of(from: AztecAddress) -> Field {  
    let notes = self.storage.owners.at(from).view_notes(NoteViewerOptions::new());  
    notes.len() as Field  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft/src/main.nr#L61-L67](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft/src/main.nr#L61-L67)

## Add Minting and BurningBefore anything else, you need to set the minter. This will be the bridge contract, so only the bridge contract can mint NFTs. This value doesn't need to change after initialization. Here's how to initialize the `PublicImmutable`:

set\_minter

```
#[external("public")]  
fn set_minter(minter: AztecAddress) {  
    assert(self.storage.admin.read().eq(self.msg_sender().unwrap()), "caller is not admin");  
    self.storage.minter.initialize(minter);  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft/src/main.nr#L31-L37](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft/src/main.nr#L31-L37)

Now for the magic - minting NFTs **privately**. The bridge will call this to mint to a user, deliver the note using [constrained message delivery](/developers/docs/aztec-nr/framework-description/how_to_emit_event) (best practice when "sending someone a
note") and then [enqueue a public call](/developers/docs/aztec-nr/framework-description/how_to_call_contracts) to the `_mark_nft_exists` function:

mint

```
#[external("private")]  
fn mint(to: AztecAddress, token_id: Field) {  
    assert(  
        self.storage.minter.read().eq(self.msg_sender().unwrap()),  
        "caller is not the authorized minter",  
    );  
  
    // we create an NFT note and insert it to the PrivateSet - a collection of notes meant to be read in private  
    let new_nft = NFTNote { token_id };  
    self.storage.owners.at(to).insert(new_nft).deliver(MessageDelivery.CONSTRAINED_ONCHAIN);  
  
    // calling the internal public function above to indicate that the NFT is taken  
    self.enqueue_self._mark_nft_exists(token_id, true);  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft/src/main.nr#L47-L59](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft/src/main.nr#L47-L59)

The bridge will also need to burn NFTs when users withdraw back to L1:

burn

```
#[external("private")]  
fn burn(from: AztecAddress, token_id: Field) {  
    assert(  
        self.storage.minter.read().eq(self.msg_sender().unwrap()),  
        "caller is not the authorized minter",  
    );  
  
    // from the NFTNote properties, selects token_id and compares it against the token_id to be burned  
    let options = NoteGetterOptions::new()  
        .select(NFTNote::properties().token_id, Comparator.EQ, token_id)  
        .set_limit(1);  
    let notes = self.storage.owners.at(from).pop_notes(options);  
    assert(notes.len() == 1, "NFT not found");  
  
    self.enqueue_self._mark_nft_exists(token_id, false);  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft/src/main.nr#L69-L81](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft/src/main.nr#L69-L81)

## Compiling!Let's verify it compiles:

```
aztec compile
```

🎉 You should see "Compiled successfully!" This means our private NFT contract is ready. Now let's build the bridge.

## Part 2: Building the BridgeWe have built the L2 NFT contract. This is the L2 representation of an NFT that is locked on the L1 bridge.

The L2 bridge is the contract that talks to the L1 bridge through cross-chain messaging. You can read more about this protocol [here](/developers/docs/aztec-nr/framework-description/ethereum-aztec-messaging).

Let's create a new contract in the same tidy `contracts/aztec` folder:

```
cd ..  
aztec new nft_bridge  
cd nft_bridge
```

And again, add the `aztec-nr` dependency to `Nargo.toml`. We also need to add the `NFTPunk` contract we just wrote above:

```
[dependencies]  
aztec = { git="https://github.com/AztecProtocol/aztec-nr", tag = "v3.0.0-devnet.20251212", directory = "aztec" }  
NFTPunk = { path = "../nft" }
```

## Understanding BridgesA bridge has two jobs:

1. **Claim**: When someone deposits an NFT on L1, mint it on L2
2. **Exit**: When someone wants to withdraw, burn on L2 and unlock on L1

This means having knowledge about the L2 NFT contract, and the bridge on the L1 side. That's what goes into our bridge's storage.

## Bridge StorageClean up `main.nr` which is just a placeholder, and let's write the storage struct and the constructor. We'll use `PublicImmutable` since these values never change:

bridge\_setup

```
use aztec::macros::aztec;  
  
#[aztec]  
pub contract NFTBridge {  
    use dep::aztec::{  
        macros::{functions::{external, initializer}, storage::storage},  
        protocol_types::{address::{AztecAddress, EthAddress}, hash::sha256_to_field},  
        state_vars::PublicImmutable,  
    };  
    use dep::NFTPunk::NFTPunk;  
  
    #[storage]  
    struct Storage<Context> {  
        nft: PublicImmutable<AztecAddress, Context>,  
        portal: PublicImmutable<EthAddress, Context>,  
    }  
  
    #[external("public")]  
    #[initializer]  
    fn constructor(nft: AztecAddress) {  
        self.storage.nft.initialize(nft);  
    }  
  
    #[external("public")]  
    fn set_portal(portal: EthAddress) {  
        self.storage.portal.initialize(portal);  
    }
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft\_bridge/src/main.nr#L1-L29](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft_bridge/src/main.nr#L1-L29)

You can't initialize the `portal` value in the constructor because the L1 portal hasn't been deployed yet. You'll need another function to set it up after the L1 portal is deployed.

## Adding the Bridge FunctionsThe Aztec network provides a way to consume messages from L1 to L2 called `consume_l1_to_l2_message`.

You need to define how to encode messages. Here's a simple approach: when an NFT is being bridged, the L1 portal sends a hash of its `token_id` through the bridge, signaling which `token_id` was locked and can be minted on L2. This approach is simple but sufficient for this tutorial.

Build the `claim` function, which consumes the message and mints the NFT on the L2 side:

claim

```
#[external("private")]  
fn claim(to: AztecAddress, token_id: Field, secret: Field, message_leaf_index: Field) {  
    // Compute the message hash that was sent from L1  
    let token_id_bytes: [u8; 32] = (token_id as Field).to_be_bytes();  
    let content_hash = sha256_to_field(token_id_bytes);  
  
    // Consume the L1 -> L2 message  
    self.context.consume_l1_to_l2_message(  
        content_hash,  
        secret,  
        self.storage.portal.read(),  
        message_leaf_index,  
    );  
  
    // Mint the NFT on L2  
    let nft: AztecAddress = self.storage.nft.read();  
    self.call(NFTPunk::at(nft).mint(to, token_id));  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft\_bridge/src/main.nr#L31-L50](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft_bridge/src/main.nr#L31-L50)

Secret

The secret prevents front-running. Certainly you don't want anyone to claim your NFT on the L2 side by just being faster. Adding a secret acts like a "password": you can only claim it if you know it.

Similarly, exiting to L1 means burning the NFT on the L2 side and pushing a message through the protocol. To ensure only the L1 recipient can claim it, hash the `token_id` together with the `recipient`:

exit

```
#[external("private")]  
fn exit(token_id: Field, recipient: EthAddress) {  
    // Create L2->L1 message to unlock NFT on L1  
    let token_id_bytes: [u8; 32] = token_id.to_be_bytes();  
    let recipient_bytes: [u8; 20] = recipient.to_be_bytes();  
    let content = sha256_to_field(token_id_bytes.concat(recipient_bytes));  
    self.context.message_portal(self.storage.portal.read(), content);  
  
    // Burn the NFT on L2  
    let nft: AztecAddress = self.storage.nft.read();  
    self.call(NFTPunk::at(nft).burn(self.context.msg_sender().unwrap(), token_id));  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/aztec/nft\_bridge/src/main.nr#L52-L68](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/aztec/nft_bridge/src/main.nr#L52-L68)

Cross-chain messaging on Aztec is powerful because it doesn't conform to any specific format—you can structure messages however you want.

Private Functions

Both `claim` and `exit` are `#[external("private")]`, which means the bridging process is private—nobody can see who's bridging which NFT by watching the chain.

## Compile the Bridge```
aztec compile
```

Bridge compiled successfully! Now process both contracts and generate TypeScript bindings:

```
cd ../nft  
aztec codegen target --outdir ../artifacts  
  
cd ../nft_bridge  
aztec codegen target --outdir ../artifacts
```

An `artifacts` folder should appear with TypeScript bindings for each contract. You'll use these when deploying the contracts.

## Part 3: The Ethereum SideNow build the L1 contracts. You need:

* A simple ERC721 NFT contract (the "CryptoPunk")
* A portal contract that locks/unlocks NFTs and communicates with Aztec

## Install DependenciesAztec's contracts are already in your `package.json`. You just need to add the OpenZeppelin contracts that provide the default ERC721 implementation:

```
cd ../../..  
yarn add @openzeppelin/contracts
```

## Create a Simple NFTDelete the "Counter" contracts that show up by default in `contracts` and create `contracts/SimpleNFT.sol`:

```
touch contracts/SimpleNFT.sol
```

Create a minimal NFT contract sufficient for demonstrating bridging:

simple\_nft

```
pragma solidity >=0.8.27;  
  
import {ERC721} from "@openzeppelin/contracts/token/ERC721/ERC721.sol";  
  
contract SimpleNFT is ERC721 {  
    uint256 private _currentTokenId;  
  
    constructor() ERC721("SimplePunk", "SPUNK") {}  
  
    function mint(address to) external returns (uint256) {  
        uint256 tokenId = _currentTokenId++;  
        _mint(to, tokenId);  
        return tokenId;  
    }  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/SimpleNFT.sol#L2-L18](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/SimpleNFT.sol#L2-L18)

## Create the NFT PortalThe NFT Portal has more code, so build it step-by-step. Create `contracts/NFTPortal.sol`:

```
touch contracts/NFTPortal.sol
```

Initialize it with Aztec's registry, which holds the canonical contracts for Aztec-related contracts, including the Inbox and Outbox. These are the message-passing contracts—Aztec sequencers read any messages on these contracts.

portal\_setup

```
import {IERC721} from "@openzeppelin/contracts/token/ERC721/IERC721.sol";  
import {IRegistry} from "@aztec/l1-contracts/src/governance/interfaces/IRegistry.sol";  
import {IInbox} from "@aztec/l1-contracts/src/core/interfaces/messagebridge/IInbox.sol";  
import {IOutbox} from "@aztec/l1-contracts/src/core/interfaces/messagebridge/IOutbox.sol";  
import {IRollup} from "@aztec/l1-contracts/src/core/interfaces/IRollup.sol";  
import {DataStructures} from "@aztec/l1-contracts/src/core/libraries/DataStructures.sol";  
import {Hash} from "@aztec/l1-contracts/src/core/libraries/crypto/Hash.sol";  
  
contract NFTPortal {  
    IRegistry public registry;  
    IERC721 public nftContract;  
    bytes32 public l2Bridge;  
  
    IRollup public rollup;  
    IOutbox public outbox;  
    IInbox public inbox;  
    uint256 public rollupVersion;  
  
    function initialize(address _registry, address _nftContract, bytes32 _l2Bridge) external {  
        registry = IRegistry(_registry);  
        nftContract = IERC721(_nftContract);  
        l2Bridge = _l2Bridge;  
  
        rollup = IRollup(address(registry.getCanonicalRollup()));  
        outbox = rollup.getOutbox();  
        inbox = rollup.getInbox();  
        rollupVersion = rollup.getVersion();  
    }
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/NFTPortal.sol#L4-L33](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/NFTPortal.sol#L4-L33)

The core logic is similar to the L2 logic. `depositToAztec` calls the `Inbox` canonical contract to send a message to Aztec, and `withdraw` calls the `Outbox` contract.

Add these two functions with explanatory comments:

portal\_deposit\_and\_withdraw

```
    // Lock NFT and send message to L2  
    function depositToAztec(uint256 tokenId, bytes32 secretHash) external returns (bytes32, uint256) {  
        // Lock the NFT  
        nftContract.transferFrom(msg.sender, address(this), tokenId);  
  
        // Prepare L2 message - just a naive hash of our tokenId  
        DataStructures.L2Actor memory actor = DataStructures.L2Actor(l2Bridge, rollupVersion);  
        bytes32 contentHash = Hash.sha256ToField(abi.encode(tokenId));  
  
        // Send message to Aztec  
        (bytes32 key, uint256 index) = inbox.sendL2Message(actor, contentHash, secretHash);  
        return (key, index);  
    }  
  
    // Unlock NFT after L2 burn  
    function withdraw(  
        uint256 tokenId,  
        uint256 l2BlockNumber,  
        uint256 leafIndex,  
        bytes32[] calldata path  
    ) external {  
        // Verify message from L2  
        DataStructures.L2ToL1Msg memory message = DataStructures.L2ToL1Msg({  
            sender: DataStructures.L2Actor(l2Bridge, rollupVersion),  
            recipient: DataStructures.L1Actor(address(this), block.chainid),  
            content: Hash.sha256ToField(abi.encodePacked(tokenId, msg.sender))  
        });  
  
        outbox.consume(message, l2BlockNumber, leafIndex, path);  
  
        // Unlock NFT  
        nftContract.transferFrom(address(this), msg.sender, tokenId);  
    }  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/contracts/NFTPortal.sol#L35-L70](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/contracts/NFTPortal.sol#L35-L70)

The portal handles two flows:

* **depositToAztec**: Locks NFT on L1, sends message to L2
* **withdraw**: Verifies L2 message, unlocks NFT on L1

## CompileLet's make sure everything compiles:

```
npx hardhat compile
```

You should see successful compilation of both contracts!

## Part 4: Compiling, Deploying, and TestingNow deploy everything and test the full flow. This will help you understand how everything fits together.

Delete the placeholders in `scripts` and create `index.ts`:

```
touch scripts/index.ts
```

This script will implement the user flow.

Testnet

This section assumes you're working locally using the local network. For the testnet, you need to account for some things:

* Your clients need to point to some Sepolia Node and to the public Aztec Full Node
* You need to [deploy your own Aztec accounts](/developers/docs/aztec-js/how_to_create_account)
* You need to pay fees in some other way. Learn how in the [fees guide](/developers/docs/aztec-js/how_to_pay_fees)

## Deploying and InitializingFirst, initialize the clients: `aztec.js` for Aztec and `viem` for Ethereum:

setup

```
import { privateKeyToAccount } from "viem/accounts";  
import {  
  createPublicClient,  
  createWalletClient,  
  http,  
  pad,  
  getAbiItem,  
  toEventHash,  
} from "viem";  
import { foundry } from "viem/chains";  
import { EthAddress } from "@aztec/aztec.js/addresses";  
import { Fr } from "@aztec/aztec.js/fields";  
import { createAztecNodeClient } from "@aztec/aztec.js/node";  
import { computeSecretHash } from "@aztec/stdlib/hash";  
import { computeL2ToL1MembershipWitness } from "@aztec/stdlib/messaging";  
import { sha256ToField } from "@aztec/foundation/crypto/sha256";  
import { computeL2ToL1MessageHash } from "@aztec/stdlib/hash";  
import { TestWallet } from "@aztec/test-wallet/server";  
import { getInitialTestAccountsData } from "@aztec/accounts/testing";  
import SimpleNFT from "../artifacts/contracts/SimpleNFT.sol/SimpleNFT.json";  
import NFTPortal from "../artifacts/contracts/NFTPortal.sol/NFTPortal.json";  
import { NFTPunkContract } from "../contracts/aztec/artifacts/NFTPunk.js";  
import { NFTBridgeContract } from "../contracts/aztec/artifacts/NFTBridge.js";  
  
// Setup L1 clients using anvil's 1st account which should have a ton of ETH already  
const l1Account = privateKeyToAccount(  
  "0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80"  
);  
const publicClient = createPublicClient({  
  chain: foundry,  
  transport: http("http://localhost:8545"),  
});  
const ethWallet = createWalletClient({  
  account: l1Account,  
  chain: foundry,  
  transport: http("http://localhost:8545"),  
});  
  
// Setup L2 using Aztec's local network and one of its initial accounts  
console.log("🔮 Setting up L2...\n");  
const node = createAztecNodeClient("http://localhost:8080");  
const aztecWallet = await TestWallet.create(node);  
const [accData] = await getInitialTestAccountsData();  
const account = await aztecWallet.createSchnorrAccount(  
  accData.secret,  
  accData.salt  
);  
console.log(`✅ Account: ${account.address.toString()}\n`);  
  
// Get node info  
const nodeInfo = await node.getNodeInfo();  
const registryAddress = nodeInfo.l1ContractAddresses.registryAddress.toString();  
const inboxAddress = nodeInfo.l1ContractAddresses.inboxAddress.toString();
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L3-L57](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L3-L57)

You now have wallets for both chains, correctly connected to their respective chains. Next, deploy the L1 contracts:

deploy\_l1\_contracts

```
console.log("📦 Deploying L1 contracts...\n");  
  
const nftDeploymentHash = await ethWallet.deployContract({  
  abi: SimpleNFT.abi,  
  bytecode: SimpleNFT.bytecode as `0x${string}`,  
});  
const nftReceipt = await publicClient.waitForTransactionReceipt({  
  hash: nftDeploymentHash,  
});  
const nftAddress = nftReceipt.contractAddress!;  
  
const portalDeploymentHash = await ethWallet.deployContract({  
  abi: NFTPortal.abi,  
  bytecode: NFTPortal.bytecode as `0x${string}`,  
});  
const portalReceipt = await publicClient.waitForTransactionReceipt({  
  hash: portalDeploymentHash,  
});  
const portalAddress = portalReceipt.contractAddress!;  
  
console.log(`✅ SimpleNFT: ${nftAddress}`);  
console.log(`✅ NFTPortal: ${portalAddress}\n`);
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L59-L82](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L59-L82)

Now deploy the L2 contracts. Thanks to the TypeScript bindings generated with `aztec codegen`, deployment is straightforward:

deploy\_l2\_contracts

```
console.log("📦 Deploying L2 contracts...\n");  
  
const l2Nft = await NFTPunkContract.deploy(aztecWallet, account.address)  
  .send({ from: account.address })  
  .deployed();  
  
const l2Bridge = await NFTBridgeContract.deploy(aztecWallet, l2Nft.address)  
  .send({ from: account.address })  
  .deployed();  
  
console.log(`✅ L2 NFT: ${l2Nft.address.toString()}`);  
console.log(`✅ L2 Bridge: ${l2Bridge.address.toString()}\n`);
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L84-L97](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L84-L97)

Now that you have the L2 bridge's contract address, initialize the L1 bridge:

initialize\_portal

```
console.log("🔧 Initializing portal...");  
  
const hash = await ethWallet.writeContract({  
  address: portalAddress as `0x${string}`,  
  abi: NFTPortal.abi,  
  functionName: "initialize",  
  args: [  
    registryAddress as `0x${string}`,  
    nftAddress as `0x${string}`,  
    l2Bridge.address.toString() as `0x${string}`,  
  ],  
});  
await publicClient.waitForTransactionReceipt({ hash });  
  
console.log("✅ Portal initialized\n");
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L99-L115](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L99-L115)

The L2 contracts were already initialized when you deployed them, but you still need to:

* Tell the L2 bridge about Ethereum's portal address (by calling `set_portal` on the bridge)
* Tell the L2 NFT contract who the minter is (by calling `set_minter` on the L2 NFT contract)

Complete these initialization steps:

initialize\_l2\_bridge

```
console.log("🔧 Setting up L2 bridge...");  
  
await l2Bridge.methods  
  .set_portal(EthAddress.fromString(portalAddress))  
  .send({ from: account.address })  
  .wait();  
  
await l2Nft.methods  
  .set_minter(l2Bridge.address)  
  .send({ from: account.address })  
  .wait();  
  
console.log("✅ Bridge configured\n");
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L117-L131](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L117-L131)

This completes the setup. It's a lot of configuration, but you're dealing with four contracts across two chains.

## L1 → L2 FlowNow for the main flow. Mint a CryptoPunk on L1, deposit it to Aztec, and claim it on Aztec. Put everything in the same script. To mint, call the L1 contract with `mint`, which will mint `tokenId = 0`:

mint\_nft\_l1

```
console.log("🎨 Minting NFT on L1...");  
  
const mintHash = await ethWallet.writeContract({  
  address: nftAddress as `0x${string}`,  
  abi: SimpleNFT.abi,  
  functionName: "mint",  
  args: [l1Account.address],  
});  
await publicClient.waitForTransactionReceipt({ hash: mintHash });  
  
// no need to parse logs, this will be tokenId 0 since it's a fresh contract  
const tokenId = 0n;  
  
console.log(`✅ Minted tokenId: ${tokenId}\n`);
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L133-L148](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L133-L148)

To bridge, first approve the portal address to transfer the NFT, then transfer it by calling `depositToAztec`:

deposit\_to\_aztec

```
console.log("🌉 Depositing NFT to Aztec...");  
  
const secret = Fr.random();  
const secretHash = await computeSecretHash(secret);  
  
const approveHash = await ethWallet.writeContract({  
  address: nftAddress as `0x${string}`,  
  abi: SimpleNFT.abi,  
  functionName: "approve",  
  args: [portalAddress as `0x${string}`, tokenId],  
});  
await publicClient.waitForTransactionReceipt({ hash: approveHash });  
  
const depositHash = await ethWallet.writeContract({  
  address: portalAddress as `0x${string}`,  
  abi: NFTPortal.abi,  
  functionName: "depositToAztec",  
  args: [  
    tokenId,  
    pad(secretHash.toString() as `0x${string}`, { dir: "left", size: 32 }),  
  ],  
});  
const depositReceipt = await publicClient.waitForTransactionReceipt({  
  hash: depositHash,  
});
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L150-L176](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L150-L176)

The `Inbox` contract will emit an important log: `MessageSent(inProgress, index, leaf, updatedRollingHash);`. This log provides the **leaf index** of the message in the [L1-L2 Message Tree](/developers/docs/aztec-nr/framework-description/ethereum-aztec-messaging)—the location of the message in the tree that will appear on L2. You need this index, plus the secret, to correctly claim and decrypt the message.

Use viem to extract this information:

get\_message\_leaf\_index

```
const INBOX_ABI = [  
  {  
    type: "event",  
    name: "MessageSent",  
    inputs: [  
      { name: "l2BlockNumber", type: "uint256", indexed: true },  
      { name: "index", type: "uint256", indexed: false },  
      { name: "hash", type: "bytes32", indexed: true },  
      { name: "rollingHash", type: "bytes16", indexed: false },  
    ],  
  },  
] as const;  
const messageSentTopic = toEventHash(INBOX_ABI[0]);  
const messageSentLog = depositReceipt.logs!.find(  
  (log: any) =>  
    log.address.toLowerCase() === inboxAddress.toLowerCase() &&  
    log.topics[0] === messageSentTopic  
);  
  
const indexHex = messageSentLog!.data!.slice(0, 66);  
const messageLeafIndex = new Fr(BigInt(indexHex));
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L178-L200](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L178-L200)

This extracts the logs from the deposit and retrieves the leaf index. You can now claim it on L2. However, for security reasons, at least 2 blocks must pass before a message can be claimed on L2. If you called `claim` on the L2 contract immediately, it would return "no message available".

Add a utility function to mine two blocks (it deploys a contract with a random salt):

mine\_blocks

```
async function mine2Blocks(aztecWallet: TestWallet, accountAddress: any) {  
  await NFTPunkContract.deploy(aztecWallet, accountAddress)  
    .send({ from: accountAddress, contractAddressSalt: Fr.random() })  
    .deployed();  
  await NFTPunkContract.deploy(aztecWallet, accountAddress)  
    .send({ from: accountAddress, contractAddressSalt: Fr.random() })  
    .deployed();  
}
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L202-L211](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L202-L211)

Now claim the message on L2:

claim\_on\_l2

```
// Mine blocks  
await mine2Blocks(aztecWallet, account.address);  
  
// Check notes before claiming (should be 0)  
console.log("📝 Checking notes before claim...");  
const notesBefore = await l2Nft.methods  
  .notes_of(account.address)  
  .simulate({ from: account.address });  
console.log(`   Notes count: ${notesBefore}`);  
  
console.log("🎯 Claiming NFT on L2...");  
await l2Bridge.methods  
  .claim(account.address, new Fr(Number(tokenId)), secret, messageLeafIndex)  
  .send({ from: account.address })  
  .wait();  
console.log("✅ NFT claimed on L2\n");  
  
// Check notes after claiming (should be 1)  
console.log("📝 Checking notes after claim...");  
const notesAfterClaim = await l2Nft.methods  
  .notes_of(account.address)  
  .simulate({ from: account.address });  
console.log(`   Notes count: ${notesAfterClaim}\n`);
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L213-L237](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L213-L237)

## L2 → L1 FlowGreat! You can expand the L2 contract to add features like NFT transfers. For now, exit the NFT on L2 and redeem it on L1. Mine two blocks because of `DelayedMutable`:

exit\_from\_l2

```
// L2 → L1 flow  
console.log("🚪 Exiting NFT from L2...");  
// Mine blocks, not necessary on devnet, but must wait for 2 blocks  
await mine2Blocks(aztecWallet, account.address);  
  
const recipientEthAddress = EthAddress.fromString(l1Account.address);  
  
const exitReceipt = await l2Bridge.methods  
  .exit(new Fr(Number(tokenId)), recipientEthAddress)  
  .send({ from: account.address })  
  .wait();  
  
console.log(`✅ Exit message sent (block: ${exitReceipt.blockNumber})\n`);  
  
// Check notes after burning (should be 0 again)  
console.log("📝 Checking notes after burn...");  
const notesAfterBurn = await l2Nft.methods  
  .notes_of(account.address)  
  .simulate({ from: account.address });  
console.log(`   Notes count: ${notesAfterBurn}\n`);
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L239-L260](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L239-L260)

Just like in the L1 → L2 flow, you need to know what to claim on L1. Where in the message tree is the message you want to claim? Use the utility `computeL2ToL1MembershipWitness`, which provides the leaf and the sibling path of the message:

get\_withdrawal\_witness

```
// Compute the message hash directly from known parameters  
// This matches what the portal contract expects: Hash.sha256ToField(abi.encodePacked(tokenId, recipient))  
const tokenIdBuffer = new Fr(Number(tokenId)).toBuffer();  
const recipientBuffer = Buffer.from(  
  recipientEthAddress.toString().slice(2),  
  "hex"  
);  
const content = sha256ToField([  
  Buffer.concat([tokenIdBuffer, recipientBuffer]),  
]);  
  
// Get rollup version from the portal contract (it stores it during initialize)  
const version = (await publicClient.readContract({  
  address: portalAddress as `0x${string}`,  
  abi: NFTPortal.abi,  
  functionName: "rollupVersion",  
})) as number;  
  
// Compute the L2→L1 message hash  
const msgLeaf = computeL2ToL1MessageHash({  
  l2Sender: l2Bridge.address,  
  l1Recipient: EthAddress.fromString(portalAddress),  
  content,  
  rollupVersion: new Fr(version),  
  chainId: new Fr(foundry.id),  
});  
  
// Wait for the block to be proven before withdrawing  
// Waiting for the block to be proven is not necessary on the local network, but it is necessary on devnet  
console.log("⏳ Waiting for block to be proven...");  
console.log(`   Exit block number: ${exitReceipt.blockNumber}`);  
  
let provenBlockNumber = await node.getProvenBlockNumber();  
console.log(`   Current proven block: ${provenBlockNumber}`);  
  
while (provenBlockNumber < exitReceipt.blockNumber!) {  
  console.log(  
    `   Waiting... (proven: ${provenBlockNumber}, needed: ${exitReceipt.blockNumber})`  
  );  
  await new Promise((resolve) => setTimeout(resolve, 10000)); // Wait 10 seconds  
  provenBlockNumber = await node.getProvenBlockNumber();  
}  
  
console.log("✅ Block proven!\n");  
  
// Compute the membership witness using the message hash  
const witness = await computeL2ToL1MembershipWitness(  
  node,  
  exitReceipt.blockNumber!,  
  msgLeaf  
);  
const siblingPathHex = witness!.siblingPath  
  .toBufferArray()  
  .map((buf: Buffer) => `0x${buf.toString("hex")}` as `0x${string}`);
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L262-L317](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L262-L317)

With this information, call the L1 contract and use the index and the sibling path to claim the L1 NFT:

withdraw\_on\_l1

```
console.log("💰 Withdrawing NFT on L1...");  
const withdrawHash = await ethWallet.writeContract({  
  address: portalAddress as `0x${string}`,  
  abi: NFTPortal.abi,  
  functionName: "withdraw",  
  args: [  
    tokenId,  
    BigInt(exitReceipt.blockNumber!),  
    BigInt(witness!.leafIndex),  
    siblingPathHex,  
  ],  
});  
await publicClient.waitForTransactionReceipt({ hash: withdrawHash });  
console.log("✅ NFT withdrawn to L1\n");
```

> [Source code: docs/examples/tutorials/token\_bridge\_contract/scripts/index.ts#L319-L334](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/tutorials/token_bridge_contract/scripts/index.ts#L319-L334)

You can now try the whole flow with:

```
npx hardhat run scripts/index.ts --network localhost
```

## What You BuiltA complete private NFT bridge with:

1. **L1 Contracts** (Solidity)

   * `SimpleNFT`: Basic ERC721 for testing
   * `NFTPortal`: Locks/unlocks NFTs and handles L1↔L2 messaging
2. **L2 Contracts** (Noir)

   * `NFTPunk`: Private NFT with encrypted ownership using `PrivateSet`
   * `NFTBridge`: Claims L1 messages and mints NFTs privately
3. **Full Flow**

   * Mint NFT on L1
   * Deploy portal and bridge
   * Lock NFT on L1 → message sent to L2
   * Claim on L2 → private NFT minted
   * Later: Burn on L2 → message to L1 → unlock

## Next Steps* Add a web frontend for easy bridging
* Implement batch bridging for multiple NFTs
* Add metadata bridging
* Write comprehensive tests
* Add proper access controls

Learn More

* [State management page](/developers/docs/foundational-topics/state_management)
* [Cross-chain messaging](/developers/docs/aztec-nr/framework-description/ethereum-aztec-messaging)

---


# Aztec.nr

Source: https://docs.aztec.network/developers/docs/aztec-nr

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Aztec.nr is the smart contract development framework for Aztec. It is a set of utilities that
help you write Noir programs to deploy on the Aztec network.

## Contract Development## Prerequisites* Install [Aztec Local Network and Tooling](/developers/getting_started_on_local_network)
* Install the [Noir LSP](/developers/docs/aztec-nr/installation) for your editor.

## Flow1. Write your contract and specify your contract dependencies. Every contract written for Aztec will have
   aztec-nr as a dependency. Add it to your `Nargo.toml` with

```
# Nargo.toml  
[dependencies]  
aztec = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/smart-contracts/aztec" }
```

Update your `main.nr` contract file to use the Aztec.nr macros for writing contracts.

setup

```
use dep::aztec::macros::aztec;  
  
#[aztec]  
pub contract Counter {
```

> [Source code: docs/examples/contracts/counter\_contract/src/main.nr#L1-L6](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/counter_contract/src/main.nr#L1-L6)

and import dependencies from the Aztec.nr library.

imports

```
use aztec::{  
    macros::{functions::{external, initializer}, storage::storage},  
    messages::message_delivery::MessageDelivery,  
    oracle::debug_log::debug_log_format,  
    protocol_types::{address::AztecAddress, traits::ToField},  
    state_vars::Owned,  
};  
use balance_set::BalanceSet;
```

> [Source code: docs/examples/contracts/counter\_contract/src/main.nr#L7-L16](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/counter_contract/src/main.nr#L7-L16)

info

You can see a complete example of a simple counter contract written with Aztec.nr [here](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/docs/examples/contracts/counter_contract/src/main.nr).

2. [Profile](/developers/docs/aztec-nr/framework-description/advanced/how_to_profile_transactions) the private functions in your contract to get
   a sense of how long generating client side proofs will take
3. Write unit tests [directly in Noir](/developers/docs/aztec-nr/how_to_test_contracts) and end-to-end
   tests [with TypeScript](/developers/docs/aztec-js/how_to_test)
4. [Compile](/developers/docs/aztec-nr/how_to_compile_contract) your contract
5. [Deploy](/developers/docs/aztec-js/how_to_deploy_contract) your contract with Aztec.js

## Section Contents[## 📄️ Noir VSCode Extension

Learn how to install and configure the Noir Language Server for a better development experience.](/developers/docs/aztec-nr/installation)

[## 📄️ Compiling Contracts

Compile your Aztec smart contracts into deployable artifacts using aztec command.](/developers/docs/aztec-nr/how_to_compile_contract)

[## 📄️ Testing Contracts

Write and run tests for your Aztec smart contracts using Noir's TestEnvironment.](/developers/docs/aztec-nr/how_to_test_contracts)

[## 📄️ Debugging Aztec Code

This guide shows you how to debug issues in your Aztec contracts.](/developers/docs/aztec-nr/debugging)

[## 🗃️ Framework Description

15 items](/developers/docs/aztec-nr/framework-description/functions)

[## 📄️ Aztec.nr API Reference

Auto-generated API reference documentation for the Aztec.nr smart contract framework.](/developers/docs/aztec-nr/api)

---


# Noir VSCode Extension

Source: https://docs.aztec.network/developers/docs/aztec-nr/installation

Version: Devnet (v3.0.0-devnet.20251212)

Install the [Noir Language Support extension](https://marketplace.visualstudio.com/items?itemName=noir-lang.vscode-noir) to get syntax highlighting, syntax error detection and go-to definitions for your Aztec contracts.

Once the extension is installed, check your nargo binary by hovering over Nargo in the status bar on the bottom right of the application window. Click to choose the path to `aztec` (or regular nargo, if you have that installed).

You can print the path of your `aztec` executable by running:

```
which aztec
```

To specify a custom nargo executable, go to the VSCode settings and search for "noir", or click extension settings on the `noir-lang` LSP plugin. Update the `Noir: Nargo Path` field to point to your desired `aztec` executable.

---


# Compiling Contracts

Source: https://docs.aztec.network/developers/docs/aztec-nr/how_to_compile_contract

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to compile your Aztec contracts into artifacts ready for deployment and interaction.

## Prerequisites* An Aztec contract written in Aztec.nr
* `aztec` installed
* Contract project with proper `Nargo.toml` configuration

## Compile your contractCompile your Noir contracts to generate JSON artifacts:

```
aztec compile
```

This outputs contract artifacts to the `target` folder.

## Use generated interfacesThe compiler automatically generates type-safe interfaces for contract interaction.

## Import and use contract interfacesUse generated interfaces instead of manual function calls:

```
contract FPC {  
    use dep::token::Token;  
  
    #[external("private")]  
    fn fee_entrypoint_private(amount: Field, asset: AztecAddress, secret_hash: Field, nonce: Field) {  
        assert(asset == storage.other_asset.read());  
        Token::at(asset).transfer_to_public(context.msg_sender(), context.this_address(), amount, nonce).call(&mut context);  
        FPC::at(context.this_address()).pay_fee_with_shielded_rebate(amount, asset, secret_hash).enqueue(&mut context);  
    }  
}
```

warning

Do not import generated interfaces from the same project as the source contract to avoid circular references.

## Next stepsAfter compilation, use the generated artifacts to:

* Deploy contracts with the `Contract` class from `aztec.js`
* Interact with deployed contracts using type-safe interfaces
* Import contracts in other Aztec.nr projects

---


# Testing Contracts

Source: https://docs.aztec.network/developers/docs/aztec-nr/how_to_test_contracts

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to test your Aztec smart contracts using Noir's `TestEnvironment` for fast, lightweight testing.

## Prerequisites* An Aztec contract project with functions to test
* Basic understanding of Noir syntax

tip

For complex cross-chain or integration testing, see the [TypeScript testing guide](/developers/docs/aztec-js/how_to_test).

## Write Aztec contract testsUse `TestEnvironment` from `aztec-nr` for contract unit testing:

* **Fast**: Lightweight environment with mocked components
* **Convenient**: Similar to Foundry for simple contract tests
* **Limited**: No rollup circuits or cross-chain messaging

For complex end-to-end tests, use [TypeScript testing](/developers/docs/aztec-js/how_to_test) with `aztec.js`.

## Run your testsExecute Aztec Noir tests using:

```
aztec test
```

## Test execution process1. Compile contracts
2. Run `aztec test`

warning

Always use `aztec test` instead of `nargo test`. The `TestEnvironment` requires the TXE (Test eXecution Environment) oracle resolver.

## Basic test structure```
use crate::MyContract;  
use aztec::{  
    protocol_types::address::AztecAddress,  
    test::helpers::test_environment::TestEnvironment,  
};  
  
#[test]  
unconstrained fn test_basic_flow() {  
    // 1. Create test environment  
    let mut env = TestEnvironment::new();  
  
    // 2. Create accounts  
    let owner = env.create_light_account();  
}
```

Test execution notes

* Tests run in parallel by default
* Use `unconstrained` functions for faster execution
* See all `TestEnvironment` methods [here](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/test/helpers/test_environment.nr)

Organizing test files

You can organize tests in separate files:

* Create `src/test.nr` with `mod utils;` to import helper functions
* Split tests into modules like `src/test/transfer_tests.nr`, `src/test/auth_tests.nr`
* Import the test module in `src/main.nr` with `mod test;`
* Share setup functions in `src/test/utils.nr`

## Deploying contractsIn order to test you'll most likely want to deploy a contract in your testing environment. First, instantiate a deployer:

```
let deployer = env.deploy("ContractName");  
  
// If on a different crate:  
let deployer = env.deploy("../other_contract");
```

warning

It is always necessary to deploy a contract in order to test it. **It is important to compile before testing**, as `aztec test` does not recompile them on changes. Think of it as regenerating the bytecode and ABI so it becomes accessible externally.

You can then choose whatever you need to initialize by interfacing with your initializer and calling it:

```
let initializer = MyContract::interface().constructor(param1, param2);  
  
let contract_address = deployer.with_private_initializer(owner, initializer);  
let contract_address = deployer.with_public_initializer(owner, initializer);  
let contract_address = deployer.without_initializer();
```

Reusable setup functions

Create a setup function to avoid repeating initialization code:

```
pub unconstrained fn setup(initial_value: Field) -> (TestEnvironment, AztecAddress, AztecAddress) {  
    let mut env = TestEnvironment::new();  
    let owner = env.create_light_account();  
    let initializer = MyContract::interface().constructor(initial_value, owner);  
    let contract_address = env.deploy("MyContract").with_private_initializer(owner, initializer);  
    (env, contract_address, owner)  
}  
  
#[test]  
unconstrained fn test_something() {  
    let (env, contract_address, owner) = setup(42);  
    // Your test logic here  
}
```

## Calling contract functionsTestEnvironment provides methods for different function types:

## Private functions```
// Call private function  
env.call_private(caller, Token::at(token_address).transfer(recipient, 100));  
  
// Returns the result  
let result = env.call_private(owner, Contract::at(address).get_private_data());
```

## Public functions```
// Call public function  
env.call_public(caller, Token::at(token_address).mint_to_public(recipient, 100));  
  
// View public state (read-only)  
let balance = env.view_public(Token::at(token_address).balance_of_public(owner));
```

## Utility/Unconstrained functions```
// Simulate utility/view functions (unconstrained)  
let total = env.simulate_utility(Token::at(token_address).balance_of_private(owner));
```

Helper function pattern

Create helper functions for common assertions:

```
pub unconstrained fn check_balance(  
    env: TestEnvironment,  
    token_address: AztecAddress,  
    owner: AztecAddress,  
    expected: u128,  
) {  
    assert_eq(  
        env.simulate_utility(Token::at(token_address).balance_of_private(owner)),  
        expected  
    );  
}
```

## Creating accountsTwo types of accounts are available:

```
// Light account - fast, limited features  
let owner = env.create_light_account();  
  
// Contract account - full features, slower  
let owner = env.create_contract_account();
```

Account type comparison

**Light accounts:**

* Fast to create
* Work for simple transfers and tests
* Cannot process authwits
* No account contract deployed

**Contract accounts:**

* Required for authwit testing
* Support account abstraction features
* Slower to create (deploys account contract)
* Needed for cross-contract authorization

Choosing account types

```
pub unconstrained fn setup(with_authwits: bool) -> (TestEnvironment, AztecAddress, AztecAddress) {  
    let mut env = TestEnvironment::new();  
    let (owner, recipient) = if with_authwits {  
        (env.create_contract_account(), env.create_contract_account())  
    } else {  
        (env.create_light_account(), env.create_light_account())  
    };  
    // ... deploy contracts ...  
    (env, owner, recipient)  
}
```

## Testing with authwits[Authwits](/developers/docs/aztec-nr/framework-description/how_to_use_authwit) allow one account to authorize another to act on its behalf.

warning

Authwits require **contract accounts**, not light accounts.

## Import authwit helpers```
use aztec::test::helpers::authwit::{  
    add_private_authwit_from_call_interface,  
    add_public_authwit_from_call_interface,  
};
```

## Private authwits```
#[test]  
unconstrained fn test_private_authwit() {  
    // Setup with contract accounts (required for authwits)  
    let (env, token_address, owner, spender) = setup(true);  
  
    // Create the call that needs authorization  
    let amount = 100;  
    let nonce = 7; // Non-zero nonce for authwit  
    let burn_call = Token::at(token_address).burn_private(owner, amount, nonce);  
  
    // Grant authorization from owner to spender  
    add_private_authwit_from_call_interface(owner, spender, burn_call);  
  
    // Spender can now execute the authorized action  
    env.call_private(spender, burn_call);  
}
```

## Public authwits```
#[test]  
unconstrained fn test_public_authwit() {  
    let (env, token_address, owner, spender) = setup(true);  
  
    // Create public action that needs authorization  
    let transfer_call = Token::at(token_address).transfer_public(owner, recipient, 100, nonce);  
  
    // Grant public authorization  
    add_public_authwit_from_call_interface(owner, spender, transfer_call);  
  
    // Execute with authorization  
    env.call_public(spender, transfer_call);  
}  
  
## Time traveling  
  
Contract calls do not advance the timestamp by default, despite each of them resulting in a block with a single transaction. Block timestamp can instead by manually manipulated by any of the following methods:  
  
```rust  
// Sets the timestamp of the next block to be mined, i.e. of the next public execution. Does not affect private execution.  
env.set_next_block_timestamp(block_timestamp);  
  
// Same as `set_next_block_timestamp`, but moving time forward by `duration` instead of advancing to a target timestamp.  
env.advance_next_block_timestamp_by(duration);  
  
// Mines an empty block at a given timestamp, causing the next public execution to occur at this time (like `set_next_block_timestamp`), but also allowing for private execution to happen using this empty block as the anchor block.  
env.mine_block_at(block_timestamp);
```

## Testing failure casesTest functions that should fail using annotations:

## Generic failure```
#[test(should_fail)]  
unconstrained fn test_unauthorized_access() {  
    let (env, contract, owner) = setup(false);  
    let attacker = env.create_light_account();  
  
    // This should fail because attacker is not authorized  
    env.call_private(attacker, Contract::at(contract).owner_only_function());  
}
```

## Specific error message```
#[test(should_fail_with = "Balance too low")]  
unconstrained fn test_insufficient_balance() {  
    let (env, token, owner, recipient) = setup(false);  
  
    // Try to transfer more than available  
    let balance = 100;  
    let transfer_amount = 101;  
  
    env.call_private(owner, Token::at(token).transfer(recipient, transfer_amount));  
}
```

## Testing authwit failures```
#[test(should_fail_with = "Unknown auth witness for message hash")]  
unconstrained fn test_missing_authwit() {  
    let (env, token, owner, spender) = setup(true);  
  
    // Try to burn without authorization  
    let burn_call = Token::at(token).burn_private(owner, 100, 1);  
  
    // No authwit granted - this should fail  
    env.call_private(spender, burn_call);  
}
```

---


# Debugging Aztec Code

Source: https://docs.aztec.network/developers/docs/aztec-nr/debugging

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to debug issues in your Aztec development environment.

## Prerequisites* Running Aztec local network
* Aztec.nr contract or aztec.js application
* Basic understanding of Aztec architecture

## Enable loggingEnable different levels of logging on the local network or node by setting `LOG_LEVEL`:

```
# Set log level (options: fatal, error, warn, info, verbose, debug, trace)  
LOG_LEVEL=debug aztec start --local-network  
  
# Different levels for different services  
LOG_LEVEL="verbose;info:sequencer" aztec start --local-network
```

## Logging in Aztec.nr contractsLog values from your contract using `debug_log`:

```
// Import debug logging  
use dep::aztec::oracle::debug_log::{ debug_log, debug_log_format, debug_log_field, debug_log_array };  
  
// Log simple messages  
debug_log("checkpoint reached");  
  
// Log field values with context  
debug_log_format("slot:{0}, hash:{1}", [storage_slot, note_hash]);  
  
// Log single field  
debug_log_field(my_field);  
  
// Log arrays  
debug_log_array(my_array);
```

note

Debug logs appear only during local execution. Private functions always execute locally, but public functions must be simulated to show logs. Use `.simulate()` or `.prove()` in TypeScript, or `env.simulate_public_function()` in TXE tests.

To see debug logs from your tests, set `LOG_LEVEL` when running:

```
LOG_LEVEL="debug" yarn run test
```

To filter specific modules, use a semicolon-delimited list:

```
LOG_LEVEL="info;debug:simulator:client_execution_context;debug:simulator:client_view_context" yarn run test
```

Log filter format

`LOG_LEVEL` accepts a semicolon-delimited list of filters. Each filter can be:

* `level` - Sets default level for all modules
* `level:module` - Sets level for a specific module
* `level:module:submodule` - Sets level for a specific submodule

```
# Default level only  
LOG_LEVEL="debug"  
  
# Default level + specific module overrides  
LOG_LEVEL="info;debug:simulator;debug:execution"  
  
# Default level + specific submodule overrides  
LOG_LEVEL="info;debug:simulator:client_execution_context;debug:simulator:client_view_context"
```

## Debugging common errors## Contract Errors| Error | Solution |
| --- | --- |
| `Aztec dependency not found` | Add to Nargo.toml: `aztec = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/aztec-nr/aztec" }` |
| `Public state writes only supported in public functions` | Move state writes to public functions |
| `Unknown contract 0x0` | Call `wallet.registerContract(...)` to register contract |
| `No public key registered for address` | Call `wallet.registerSender(...)` |
| `Failed to solve brillig function` | Check function parameters and note validity |

## Circuit Errors| Error Code | Meaning | Fix |
| --- | --- | --- |
| `2002` | Invalid contract address | Ensure contract is deployed and address is correct |
| `2005/2006` | Static call violations | Remove state modifications from static calls |
| `2017` | User intent mismatch | Verify transaction parameters match function call |
| `3001` | Unsupported operation | Check if operation is supported in current context |
| `3005` | Non-empty private call stack | Ensure private functions complete before public |
| `4007/4008` | Chain ID/version mismatch | Verify L1 chain ID and Aztec version |
| `7008` | Membership check failed | Ensure using valid historical state |
| `7009` | Array overflow | Reduce number of operations in transaction |

## Quick Fixes for Common Issues```
# Archiver sync issues - force progress with dummy transactions  
aztec-wallet send transfer --from test0 --to test0 --amount 0  
aztec-wallet send transfer --from test0 --to test0 --amount 0  
  
# L1 to L2 message pending - wait for inclusion  
# Messages need 2 blocks to be processed
```

## Debugging WASM errors## Enable debug WASM```
// In vite.config.ts or similar  
export default {  
  define: {  
    "process.env.BB_WASM_PATH": JSON.stringify("https://debug.wasm.url"),  
  },  
};
```

## Profile transactions```
import { serializePrivateExecutionSteps } from "@aztec/stdlib";  
  
// Profile the transaction  
const profileTx = await contract.methods  
  .myMethod(param1, param2)  
  .profile({ profileMode: "execution-steps" });  
  
// Serialize for debugging  
const ivcMessagePack = serializePrivateExecutionSteps(profileTx.executionSteps);  
  
// Download debug file  
const blob = new Blob([ivcMessagePack]);  
const url = URL.createObjectURL(blob);  
const link = document.createElement("a");  
link.href = url;  
link.download = "debug-steps.msgpack";  
link.click();
```

⚠️ **Warning:** Debug files may contain private data. Use only in development.

## Interpret error messages## Kernel circuit errors (2xxx)")

* **Private kernel errors (2xxx)**: Issues with private function execution
* **Public kernel errors (3xxx)**: Issues with public function execution
* **Rollup errors (4xxx)**: Block production issues
* **Generic errors (7xxx)**: Resource limits or state validation

## Transaction limitsCurrent limits that trigger `7009 - ARRAY_OVERFLOW`:

* Max new notes per tx: Check `MAX_NOTE_HASHES_PER_TX`
* Max nullifiers per tx: Check `MAX_NULLIFIERS_PER_TX`
* Max function calls: Check call stack size limits
* Max L2→L1 messages: Check message limits

## Debugging sequencer issues## Common sequencer errors| Error | Cause | Solution |
| --- | --- | --- |
| `tree root mismatch` | State inconsistency | Restart local network or check state transitions |
| `next available leaf index mismatch` | Tree corruption | Verify tree updates are sequential |
| `Public call stack size exceeded` | Too many public calls | Reduce public function calls |
| `Failed to publish block` | L1 submission failed | Check L1 connection and gas |

## Reporting issuesWhen debugging fails:

1. Collect error messages and codes
2. Generate transaction profile (if applicable)
3. Note your environment setup
4. Create issue at [aztec-packages](https://github.com/AztecProtocol/aztec-packages/issues/new)

## Quick reference## Enable verbose logging```
LOG_LEVEL=verbose aztec start --local-network
```

## Common debug imports```
use dep::aztec::oracle::debug_log::{ debug_log, debug_log_format };
```

## Check contract registration```
await wallet.getContractMetadata(myContractInstance.address);
```

## Decode L1 errorsCheck hex errors against [Errors.sol](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/core/libraries/Errors.sol)

## Tips* Always check logs before diving into circuit errors
* State-related errors often indicate timing issues
* Array overflow errors mean you hit transaction limits
* Use debug WASM for detailed stack traces
* Profile transactions when errors are unclear

## Next steps* [Circuit Architecture](/developers/docs/foundational-topics/advanced/circuits)
* [Private-Public Execution](/developers/docs/aztec-nr/framework-description/functions/public_private_calls)
* [Aztec.nr Dependencies](/developers/docs/aztec-nr/framework-description/dependencies)

---


# Framework Description

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/functions

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Functions serve as the building blocks of smart contracts. Functions can be either **public**, ie they are publicly available for anyone to see and can directly interact with public state, or **private**, meaning they are executed completely client-side in the [PXE](/developers/docs/foundational-topics/pxe). Read more about how private functions work [here](/developers/docs/aztec-nr/framework-description/functions/attributes#private-functions-private).

Currently, any function is "mutable" in the sense that it might alter state. However, we also support static calls, similarly to EVM. A static call is essentially a call that does not alter state (it keeps state static).

## Initializer functionsSmart contracts may have one, or many, initializer functions which are called when the contract is deployed.

Initializers are regular functions that set an "initialized" flag (a nullifier) for the contract. A contract can only be initialized once, and contract functions can only be called after the contract has been initialized, much like a constructor. However, if a contract defines no initializers, it can be called at any time. Additionally, you can define as many initializer functions in a contract as you want, both private and public.

## OraclesThere are also special oracle functions, which can get data from outside of the smart contract. In the context of Aztec, oracles are often used to get user-provided inputs.

## Learn more about functions* [How function visibility works in Aztec](/developers/docs/aztec-nr/framework-description/functions/visibility)
* How to write an [initializer function](/developers/docs/aztec-nr/framework-description/functions/how_to_define_functions#initializer-functions)
* [Oracles](/developers/docs/aztec-nr/framework-description/advanced/protocol_oracles) and how Aztec smart contracts might use them
* [How functions work under the hood](/developers/docs/aztec-nr/framework-description/functions/attributes)

Find a function macros reference [here](/developers/docs/aztec-nr/framework-description/macros)

---


# How to Define Functions

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/functions/how_to_define_functions

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to define different types of functions in your Aztec contracts, each serving specific purposes and execution environments.

## Prerequisites* An Aztec contract project set up with `aztec-nr` dependency
* Basic understanding of Noir programming language
* Familiarity with Aztec's execution model (private vs public)

## Define private functionsCreate functions that execute privately on user devices using the `#[external("private")]` annotation. For example:

```
#[external("private")]  
fn execute_private_action(param1: AztecAddress, param2: u128) {  
    // logic  
}
```

Private functions maintain privacy of user inputs and execution logic. Private functions only have access to private state.

## Define public functionsCreate functions that execute on the sequencer using the `#[external("public")]` annotation:

```
#[external("public")]  
fn create_item(recipient: AztecAddress, item_id: Field) {  
    // logic  
}
```

Public functions can access public state, similar to EVM contracts. Public functions do not have direct access to private state.

## Define utility functionsCreate offchain query functions using the `#[external("utility")]` annotation.

Utility functions are standalone unconstrained functions that cannot be called from private or public functions: they are meant to be called by *applications* to perform auxiliary tasks: query contract state (e.g. a token balance), process messages received offchain, etc. Example:

```
#[external("utility")]  
unconstrained fn get_private_items(  
    owner: AztecAddress,  
    page_index: u32,  
) -> ([Field; MAX_NOTES_PER_PAGE], bool) {  
    // logic  
}
```

## Define view functionsCreate read-only functions using the `#[view]` annotation combined with `#[external("private")]` or `#[external("public")]`:

```
#[external("public")]  
#[view]  
fn get_config_value() -> Field {  
    // logic  
}
```

View functions cannot modify contract state. They're akin to Ethereum's `view` functions.

## Define only-self functionsCreate contract-only functions using the `#[only_self]` annotation:

```
#[external("public")]  
#[only_self]  
fn update_counter_public(item: Field) {  
    // logic  
}
```

Internal functions are only callable within the same contract.

## Define initializer functionsCreate constructor-like functions using the `#[initializer]` annotation:

```
#[external("private")]  
#[initializer]  
fn constructor() {  
    // logic  
}
```

## Use multiple initializersDefine multiple initialization options:

1. Mark each function with `#[initializer]`
2. Choose which one to call during deployment
3. Any initializer marks the contract as initialized

## Create library methodsDefine reusable contract logic as regular functions (no special annotation needed):

```
#[contract_library_method]  
fn process_value(  
    context: &mut PrivateContext,  
    storage: Storage<&mut PrivateContext>,  
    account: AztecAddress,  
    value: u128,  
    max_items: u32,  
) -> u128 {  
    // logic  
}
```

Library methods are inlined when called and reduce code duplication.

---


# Understanding Function Context

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/functions/context

Version: Devnet (v3.0.0-devnet.20251212)

On this page

## What is the contextThe context is an object that is made available within every function in `Aztec.nr`. As mentioned in the [kernel circuit documentation](/developers/docs/foundational-topics/advanced/circuits/private_kernel). At the beginning of a function's execution, the context contains all of the kernel information that application needs to execute. During the lifecycle of a transaction, the function will update the context with each of its side effects (created notes, nullifiers etc.). At the end of a function's execution the mutated context is returned to the kernel to be checked for validity.

Behind the scenes, Aztec.nr will pass data the kernel needs to and from a circuit, this is abstracted away from the developer. In a developer's eyes; the context is a useful structure that allows access and mutate the state of the `Aztec` blockchain.

On this page, you'll learn

* The details and functionalities of the private context in Aztec.nr
* Difference between the private and public contexts and their unified APIs
* Components of the private context, such as inputs and block header.
* Elements like return values, read requests, new note hashes, and nullifiers in transaction processing
* Differences between the private and public contexts, especially the unique features and variables in the public context

## Two contexts, one APIThe `Aztec` blockchain contains two environments - public and private.

* Private, for private transactions taking place on user's devices.
* Public, for public transactions taking place on the network's sequencers.

As there are two distinct execution environments, they both require slightly differing execution contexts. Despite their differences; the API's for interacting with each are unified. Leading to minimal context switch when working between the two environments.

The following section will cover both contexts.

## The Private ContextThe code snippet below shows what is contained within the private context.

private-context

```
pub inputs: PrivateContextInputs,  
pub side_effect_counter: u32,  
  
pub min_revertible_side_effect_counter: u32,  
pub is_fee_payer: bool,  
  
pub args_hash: Field,  
pub return_hash: Field,  
  
pub include_by_timestamp: u64,  
  
pub note_hash_read_requests: BoundedVec<Scoped<Counted<Field>>, MAX_NOTE_HASH_READ_REQUESTS_PER_CALL>,  
pub nullifier_read_requests: BoundedVec<Scoped<Counted<Field>>, MAX_NULLIFIER_READ_REQUESTS_PER_CALL>,  
key_validation_requests_and_generators: BoundedVec<KeyValidationRequestAndGenerator, MAX_KEY_VALIDATION_REQUESTS_PER_CALL>,  
  
pub note_hashes: BoundedVec<Counted<NoteHash>, MAX_NOTE_HASHES_PER_CALL>,  
pub nullifiers: BoundedVec<Counted<Nullifier>, MAX_NULLIFIERS_PER_CALL>,  
  
pub private_call_requests: BoundedVec<PrivateCallRequest, MAX_PRIVATE_CALL_STACK_LENGTH_PER_CALL>,  
pub public_call_requests: BoundedVec<Counted<PublicCallRequest>, MAX_ENQUEUED_CALLS_PER_CALL>,  
pub public_teardown_call_request: PublicCallRequest,  
pub l2_to_l1_msgs: BoundedVec<Counted<L2ToL1Message>, MAX_L2_TO_L1_MSGS_PER_CALL>,
```

> [Source code: noir-projects/aztec-nr/aztec/src/context/private\_context.nr#L171-L194](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/context/private_context.nr#L171-L194)

## Private Context Broken Down## InputsThe context inputs includes all of the information that is passed from the kernel circuit into the application circuit. It contains the following values.

private-context-inputs

```
#[derive(Eq)]  
pub struct PrivateContextInputs {  
    pub call_context: CallContext,  
    pub anchor_block_header: BlockHeader,  
    pub tx_context: TxContext,  
    pub start_side_effect_counter: u32,  
}
```

> [Source code: noir-projects/aztec-nr/aztec/src/context/inputs/private\_context\_inputs.nr#L9-L17](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/context/inputs/private_context_inputs.nr#L9-L17)

As shown in the snippet, the application context is made up of 4 main structures. The call context, the block header, and the private global variables.

First of all, the call context.

call-context

```
#[derive(Deserialize, Eq, Serialize)]  
pub struct CallContext {  
    pub msg_sender: AztecAddress,  
    pub contract_address: AztecAddress,  
    pub function_selector: FunctionSelector,  
    pub is_static_call: bool,  
}
```

> [Source code: noir-projects/noir-protocol-circuits/crates/types/src/abis/call\_context.nr#L8-L16](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-protocol-circuits/crates/types/src/abis/call_context.nr#L8-L16)

The call context contains information about the current call being made:

1. Msg Sender
   * The message sender is the account (Aztec Contract) that sent the message to the current context. In the first call of the kernel circuit (often the account contract call), this value will be empty. For all subsequent calls the value will be the previous call.

> The graphic below illustrates how the message sender changes throughout the kernel circuit iterations.

![](/assets/ideal-img/sender_context_change.3d8574d.640.png)

2. Storage contract address

   * This value is the address of the current context's contract address. This value will be the value of the current contract that is being executed except for when the current call is a delegate call (Warning: This is yet to be implemented). In this case the value will be that of the sending contract.
3. Flags

   * Furthermore there are a series of flags that are stored within the application context:
     + is\_delegate\_call: Denotes whether the current call is a delegate call. If true, then the storage contract address will be the address of the sender.
     + is\_static\_call: This will be set if and only if the current call is a static call. In a static call, state changing altering operations are not allowed.

## Block HeaderAnother structure that is contained within the context is the `BlockHeader` object, which is the header of the block used to generate proofs against.

block-header

```
#[derive(Deserialize, Eq, Serialize)]  
pub struct BlockHeader {  
    pub last_archive: AppendOnlyTreeSnapshot,  
    pub state: StateReference,  
  
    // The hash of the sponge blob for this block, which commits to the tx effects added in this block.  
    // Note: it may also include tx effects from previous blocks within the same checkpoint.  
    // When proving tx effects from this block only, we must refer to the `sponge_blob_hash` in the previous block  
    // header to show that the effect was added after the previous block.  
    // The previous block header can be validated using a membership proof of the last leaf in `last_archive`.  
    pub sponge_blob_hash: Field,  
  
    pub global_variables: GlobalVariables,  
    pub total_fees: Field,  
    pub total_mana_used: Field,  
}
```

> [Source code: noir-projects/noir-protocol-circuits/crates/types/src/abis/block\_header.nr#L12-L29](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-protocol-circuits/crates/types/src/abis/block_header.nr#L12-L29)

## Transaction ContextThe private context provides access to the transaction context as well, which are user-defined values for the transaction in general that stay constant throughout its execution.

tx-context

```
#[derive(Deserialize, Eq, Serialize)]  
pub struct TxContext {  
    pub chain_id: Field,  
    pub version: Field,  
    pub gas_settings: GasSettings,  
}
```

> [Source code: noir-projects/noir-protocol-circuits/crates/types/src/abis/transaction/tx\_context.nr#L8-L15](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-protocol-circuits/crates/types/src/abis/transaction/tx_context.nr#L8-L15)

## Args HashTo allow for flexibility in the number of arguments supported by Aztec functions, all function inputs are reduced to a singular value which can be proven from within the application.

The `args_hash` is the result of pedersen hashing all of a function's inputs.

## Return ValuesThe return values are a set of values that are returned from an applications execution to be passed to other functions through the kernel. Developers do not need to worry about passing their function return values to the `context` directly as `Aztec.nr` takes care of it for you. See the documentation surrounding `Aztec.nr` [macro expansion](/developers/docs/aztec-nr/framework-description/functions/attributes#after-expansion) for more details.

```
return_values : BoundedVec\<Field, RETURN_VALUES_LENGTH\>,
```

## Include By TimestampSome data structures impose time constraints, e.g. they may make it so that a value can only be changed after a certain delay. Interacting with these in private involves creating proofs that are only valid as long as they are included before a certain future point in time. To achieve this, the `set_include_by_timestamp` function can be used to set this property:

include-by-timestamp

```
pub fn set_include_by_timestamp(&mut self, include_by_timestamp: u64) {
```

> [Source code: noir-projects/aztec-nr/aztec/src/context/private\_context.nr#L742-L744](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/context/private_context.nr#L742-L744)

A transaction that sets this value will never be included in a block with a timestamp larger than the requested value, since it would be considered invalid. This can also be used to make transactions automatically expire after some time if not included.

## Read Requests## New Note HashesNew note hashes contains an array of all of the note hashes created in the current execution context.

## New NullifiersNew nullifiers contains an array of the new nullifiers emitted from the current execution context.

## Nullified Note HashesNullified note hashes is an optimization for introduced to help reduce state growth. There are often cases where note hashes are created and nullified within the same transaction.
In these cases there is no reason that these note hashes should take up space on the node's commitment/nullifier trees. Keeping track of nullified note hashes allows us to "cancel out" and prove these cases.

## Private Call StackThe private call stack contains all of the external private function calls that have been created within the current context. Any function call objects are hashed and then pushed to the execution stack.
The kernel circuit will orchestrate dispatching the calls and returning the values to the current context.

## Public Call StackThe public call stack contains all of the external function calls that are created within the current context. Like the private call stack above, the calls are hashed and pushed to this stack. Unlike the private call stack, these calls are not executed client side. Whenever the function is sent to the network, it will have the public call stack attached to it. At this point the sequencer will take over and execute the transactions.

## New L2 to L1 msgsNew L2 to L1 messages contains messages that are delivered to the l1 outbox on the execution of each rollup.

## Public ContextThe Public Context includes all of the information passed from the `Public VM` into the execution environment. Its interface is very similar to the [Private Context](#the-private-context), however it has some minor differences (detailed below).

## Public Global VariablesThe public global variables are provided by the rollup sequencer and consequently contain some more values than the private global variables.

global-variables

```
#[derive(Deserialize, Eq, Serialize)]  
pub struct GlobalVariables {  
    pub chain_id: Field,  
    pub version: Field,  
    pub block_number: u32,  
    pub slot_number: Field,  
    pub timestamp: u64,  
    pub coinbase: EthAddress,  
    pub fee_recipient: AztecAddress,  
    pub gas_fees: GasFees,  
}
```

> [Source code: noir-projects/noir-protocol-circuits/crates/types/src/abis/global\_variables.nr#L7-L19](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-protocol-circuits/crates/types/src/abis/global_variables.nr#L7-L19)

---


# Visibility

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/functions/visibility

Version: Devnet (v3.0.0-devnet.20251212)

On this page

In Aztec there are multiple different types of visibility that can be applied to functions. Namely we have `data visibility` and `function visibility`. This page explains these types of visibility.

## Data VisibilityData visibility is used to describe whether the data (or state) used in a function is generally accessible (public) or on a need to know basis (private).

## Function visibilityThis is the kind of visibility you are more used to seeing in Solidity and more traditional programming languages. It is used to describe whether a function is callable from other contracts, or only from within the same contract.

By default, all functions are callable from other contracts, similarly to the Solidity `public` visibility. To make them only callable from the contract itself, you can mark them as `internal`. Contrary to solidity, we don't have the `external` nor `private` keywords. `external` since it is limited usage when we don't support inheritance, and `private` since we don't support inheritance and it would also be confusing with multiple types of `private`.

A good place to use `internal` is when you want a private function to be able to alter public state. As mentioned above, private functions cannot do this directly. They are able to call public functions and by making these internal we can ensure that this state manipulating function is only callable from our private function.

danger

Note that non-internal functions could be used directly as an entry-point, which currently means that the `msg_sender` would be `0`, so for now, using address `0` as a burn address is not recommended. You can learn more about this in the [Accounts concept page](/developers/docs/foundational-topics/accounts/keys).

To understand how visibility works under the hood, check out the [Inner Workings page](/developers/docs/aztec-nr/framework-description/functions/attributes).

---


# Inner Workings of Functions

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/functions/function_transforms

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Below, we go more into depth of what is happening under the hood when you create a function in an Aztec contract. The [next page](/developers/docs/aztec-nr/framework-description/functions/attributes) will give you more information about what the attributes are really doing.

## Function transformationWhen you define a function in an Aztec contract, it undergoes several transformations when it is compiled. These transformations prepare the function for execution. These transformations include:

* [Creating a context for the function](#context-creation)
* [Handling function inputs](#private-and-public-input-injection)
* [Processing return values](#return-value-handling)
* [Generating function signatures](#function-signature-generation)
* [Generating contract artifacts](#contract-artifacts)

Let's explore each of these transformations in detail.

## Context creationEvery function in an Aztec contract operates within a specific context which provides some extra information and functionality. This is either a `PrivateContext` or `PublicContext` object, depending on whether it is a private or public function. For private functions, it creates a hash of all input parameters to ensure privacy.

## Private functionsFor private functions, the context creation involves hashing all input parameters:

```
let mut args_hasher = ArgsHasher::new();  
// Hash each parameter  
args_hasher.add(param1);  
args_hasher.add(param2);  
// add all parameters  
  
let mut context = PrivateContext::new(inputs, args_hasher.hash());
```

This hashing process is important because it is used to verify the function's execution without exposing the input data.

## Public functionsFor public functions, context creation is simpler:

```
let mut context = PublicContext::new(inputs);
```

These `inputs` are explained in the [private and public input injection](#private-and-public-input-injection) further down on this page.

## Using the context in functionsOnce created, the context object provides various useful methods. Here are some common use cases:

## Accessing storageThe context allows you to interact with contract storage. eg if you have a function that calls storage like this:

```
let sender_balance = storage.balances.at(owner);
```

This calls the context to read from the appropriate storage slot.

## Interacting with other contractsThe context provides methods to call other contracts:

```
let token_contract = TokenContract::at(token);
```

Under the hood, this creates a new instance of the contract interface with the specified address.

## Private and public input injectionAn additional parameter is automatically added to every function.

The injected input is always the first parameter of the transformed function and is of type `PrivateContextInputs` for private functions or `PublicContextInputs` for public functions.

Original function definition

```
fn my_function(param1: Type1, param2: Type2) { ... }
```

Transformed function with injected input

```
fn my_function(inputs: PrivateContextInputs, param1: Type1, param2: Type2) { ... }
```

The `inputs` parameter includes:

* msg sender, ie the address of the account calling the function
* contract address
* chain ID
* block context, eg the block number & timestamp
* function selector of the function being called

This makes these inputs available to be consumed within private annotated functions.

## Return value handlingReturn values in Aztec contracts are processed differently from traditional smart contracts when using private functions.

## Private functions* The original return value is assigned to a special variable:

  ```
  let macro__returned__values = original_return_expression;
  ```
* A new `ArgsHasher` is created for the return values:

  ```
  let mut returns_hasher = ArgsHasher::new();
  ```
* The hash of the return value is set in the context:

  ```
  context.set_return_hash(returns_hasher);
  ```
* The function's return type is changed to `PrivateCircuitPublicInputs`, which is returned by calling `context.finish()` at the end of the function.

This process allows the return values to be included in the function's computation result while maintaining privacy.

## Public functionsIn public functions, the return value is directly used, and the function's return type remains as specified by the developer.

## Function signature generationUnique function signatures are generated for each contract function.

The function signature is computed like this:

```
fn compute_fn_signature_hash(fn_name: &str, parameters: &[Type]) -> u32 {  
    let signature = format!(  
        "{}({})",  
        fn_name,  
        parameters.iter().map(signature_of_type).collect::<Vec<_>>().join(",")  
    );  
    let mut keccak = Keccak::v256();  
    let mut result = [0u8; 32];  
    keccak.update(signature.as_bytes());  
    keccak.finalize(&mut result);  
    // Take the first 4 bytes of the hash and convert them to an integer  
    // If you change the following value you have to change NUM_BYTES_PER_NOTE_TYPE_ID in l1_note_payload.ts as well  
    let num_bytes_per_note_type_id = 4;  
    u32::from_be_bytes(result[0..num_bytes_per_note_type_id].try_into().unwrap())  
}
```

* A string representation of the function is created, including the function name and parameter types
* This signature string is then hashed using Keccak-256
* The first 4 bytes of the resulting hash are converted to a u32 integer

## Integration into contract interfaceThe computed function signatures are integrated into the contract interface like this:

* During contract compilation, placeholder values (0) are initially used for function selectors
* After type checking, the `update_fn_signatures_in_contract_interface()` function is called to replace these placeholders with the actual computed signatures
* For each function in the contract interface:

  + The function's parameters are extracted
  + The signature hash is computed using `compute_fn_signature_hash`
  + The placeholder in the contract interface is replaced with the computed hash

This process ensures that each function in the contract has a unique, deterministic signature based on its name and parameter types. They are inspired by Solidity's function selector mechanism.

## Contract artifactsContract artifacts in Aztec are automatically generated structures that describe the contract's interface. They provide information about the contract's functions, their parameters, and return types.

## Contract artifact generation processFor each function in the contract, an artifact is generated like this:

* A struct is created to represent the function's parameters:

  ```
  struct {function_name}_parameters {  
      // Function parameters are listed here  
  }
  ```

  This struct is only created if the function has parameters.
* An ABI struct is generated for the function:

```
 let export_struct_source = format!(  
        "  
        #[abi(functions)]  
        struct {}_abi {{  
            {}{}  
        }}",  
        func.name(),  
        parameters,  
        return_type  
    );
```

* These structs are added to the contract's types.

## Content of artifactsThe artifacts contain:

* Function name
* Parameters (if any), including their names and types
* Return type (if the function has returns)

For example, for a function `transfer(recipient: Address, amount: Field) -> bool`, the artifact would look like:

```
struct transfer_parameters {  
    recipient: Address,  
    amount: Field,  
}  
  
#[abi(functions)]  
struct transfer_abi {  
    parameters: transfer_parameters,  
    return_type: bool,  
}
```

Contract artifacts are important because:

* They provide a machine-readable description of the contract
* They can be used to generate bindings for interacting with the contract (read [here](/developers/docs/aztec-nr/how_to_compile_contract) to learn how to create TypeScript bindings)
* They help decode function return values in the simulator

## Further reading* [Function attributes and macros](/developers/docs/aztec-nr/framework-description/functions/attributes)

---


# Function Attributes and Macros

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/functions/attributes

Version: Devnet (v3.0.0-devnet.20251212)

On this page

On this page you will learn about function attributes and macros.

If you are looking for a reference of function macros, go [here](/developers/docs/aztec-nr/framework-description/macros).

Like in Solidity, external functions can be called from outside the contract.
There are 3 types of external functions differing in the execution environment they are executed in: private, public, and utility.
We will describe each type in the following sections.

## Private functions #[external("private")]]")

A private function operates on private information, and is executed by the user on their device. Annotate the function with the `#[external("private")]` attribute to tell the compiler it's a private function. This will make the [private context](/developers/docs/aztec-nr/framework-description/functions/context#the-private-context) available within the function's execution scope. The compiler will create a circuit to define this function.

`#[external("private")]` is just syntactic sugar. At compile time, the Aztec.nr framework inserts code that allows the function to interact with the [kernel](/developers/docs/foundational-topics/advanced/circuits/private_kernel).

If you are interested in what exactly the macros are doing we encourage you to run `nargo expand` on your contract.
This will display your contract's code after the transformations are performed.

(If you are using VSCode you can display the expanded code by pressing `CMD + Shift + P` and typing `nargo expand` and selecting `Noir: nargo expand on current package.)

## The expansion broken downViewing the expanded Aztec contract uncovers a lot about how Aztec contracts interact with the kernel. To aid with developing intuition, we will break down each inserted line.

**Receiving context from the kernel.**

Private function calls are able to interact with each other through orchestration from within the kernel circuits. The kernel circuit forwards information to each contract function (recall each contract function is a circuit). This information then becomes part of the private context.
For example, within each private function we can access some global variables. To access them we can call on the `self.context`, e.g. `self.context.chain_id()`. The value of the chain ID comes from the values passed into the circuit from the kernel.

The kernel checks that all of the values passed to each circuit in a function call are the same.

**Creating the function's `self.`**

Each Aztec function has access to a `self` object. Upon creation it accepts storage and context. Context is initialized from the inputs provided by the kernel, and a hash of the function's inputs.

We use the kernel to pass information between circuits. This means that the return values of functions must also be passed to the kernel (where they can be later passed on to another function).
We achieve this by pushing return values to the execution context, which we then pass to the kernel.

**Hashing the function inputs.**

Inside the kernel circuits, the inputs to functions are reduced to a single value; the inputs hash. This prevents the need for multiple different kernel circuits; each supporting differing numbers of inputs. Hashing the inputs allows to reduce all of the inputs to a single value.

**Returning the context to the kernel.**

The contract function must return information about the execution back to the kernel. This is done through a rigid structure we call the `PrivateCircuitPublicInputs`.

> *Why is it called the `PrivateCircuitPublicInputs`?*
> When verifying zk programs, return values are not computed at verification runtime, rather expected return values are provided as inputs and checked for correctness. Hence, the return values are considered public inputs.

This structure contains a host of information about the executed program. It will contain any newly created nullifiers, any messages to be sent to l2 and most importantly it will contain the return values of the function.

**Making the contract's storage available**

Each `self` has a `storage` variable exposed on it.
When a `Storage` struct is declared within a contract, the `self.storage` contains real variables.

If Storage is note defined `self.storage` contains only a placeholder value.

Any state variables declared in the `Storage` struct can now be accessed as normal struct members.

**Returning the function context to the kernel.**

This function takes the application context, and converts it into the `PrivateCircuitPublicInputs` structure. This structure is then passed to the kernel circuit.

## Utility functions #[external("utility")]]")

Contract functions marked with `#[external("utility")]` are used to perform state queries from an offchain client (from both private and public state!) or to modify local contract-related PXE state (e.g. when processing logs in Aztec.nr), and are never included in any transaction. No guarantees are made on the correctness of the result since the entire execution is unconstrained and heavily reliant on [oracle calls](https://noir-lang.org/docs/explainers/explainer-oracle).

Any programming language could be used to construct these queries, since all they do is perform arbitrary computation on data that is either publicly available from any node, or locally available from the PXE. Utility functions exist as Noir contract code because they let developers utilize the rest of the contract code directly by being part of the same Noir crate, and e.g. use the same libraries, structs, etc. instead of having to rely on manual computation of storage slots, struct layout and padding, and so on.

A reasonable mental model for them is that of a Solidity `view` function that can never be called in any transaction, and is only ever invoked via `eth_call`. Note that in these the caller assumes that the node is acting honestly by executing the true contract bytecode with correct blockchain state, the same way the Aztec version assumes the oracles are returning legitimate data. Unlike `view` functions however, `utility` functions can modify local offchain PXE state via oracle calls - this can be leveraged for example to process messages delivered offchain and then notify PXE of newly discovered notes.

When a utility function is called, it prompts the ACIR simulator to

1. generate the execution environment
2. execute the function within this environment

To generate the environment, the simulator gets the block header from the [PXE database](/developers/docs/foundational-topics/pxe#database) and passes it along with the contract address to `UtilityExecutionOracle`. This creates a context that simulates the state of the blockchain at a specific block, allowing the utility function to access and interact with blockchain data as it would appear in that block, but without affecting the actual blockchain state.

Once the execution environment is created, `runUtility` function is invoked on the simulator:

execute\_utility\_function

```
/**  
 * Runs a utility function.  
 * @param call - The function call to execute.  
 * @param authwits - Authentication witnesses required for the function call.  
 * @param scopes - Optional array of account addresses whose notes can be accessed in this call. Defaults to all  
 * accounts if not specified.  
 * @returns A return value of the utility function in a form as returned by the simulator (Noir fields)  
 */  
public async runUtility(call: FunctionCall, authwits: AuthWitness[], scopes?: AztecAddress[]): Promise<Fr[]> {  
  await verifyCurrentClassId(call.to, this.executionDataProvider);  
  
  const entryPointArtifact = await this.executionDataProvider.getFunctionArtifact(call.to, call.selector);  
  
  if (entryPointArtifact.functionType !== FunctionType.UTILITY) {  
    throw new Error(`Cannot run ${entryPointArtifact.functionType} function as utility`);  
  }  
  
  const oracle = new UtilityExecutionOracle(call.to, authwits, [], this.executionDataProvider, undefined, scopes);  
  
  try {  
    this.log.verbose(`Executing utility function ${entryPointArtifact.name}`, {  
      contract: call.to,  
      selector: call.selector,  
    });  
  
    const initialWitness = toACVMWitness(0, call.args);  
    const acirExecutionResult = await this.simulator  
      .executeUserCircuit(initialWitness, entryPointArtifact, new Oracle(oracle).toACIRCallback())  
      .catch((err: Error) => {  
        err.message = resolveAssertionMessageFromError(err, entryPointArtifact);  
        throw new ExecutionError(  
          err.message,  
          {  
            contractAddress: call.to,  
            functionSelector: call.selector,  
          },  
          extractCallStack(err, entryPointArtifact.debug),  
          { cause: err },  
        );  
      });  
  
    this.log.verbose(`Utility simulation for ${call.to}.${call.selector} completed`);  
    return witnessMapToFields(acirExecutionResult.returnWitness);  
  } catch (err) {  
    throw createSimulationError(err instanceof Error ? err : new Error('Unknown error during private execution'));  
  }  
}
```

> [Source code: yarn-project/pxe/src/contract\_function\_simulator/contract\_function\_simulator.ts#L211-L259](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/yarn-project/pxe/src/contract_function_simulator/contract_function_simulator.ts#L211-L259)

This:

1. Prepares the ACIR for execution
2. Converts `args` into a format suitable for the ACVM (Abstract Circuit Virtual Machine), creating an initial witness (witness = set of inputs required to compute the function). `args` might be an oracle to request a user's balance
3. Executes the function in the ACVM, which involves running the ACIR with the initial witness and the context. If requesting a user's balance, this would query the balance from the PXE database
4. Extracts the return values from the `partialWitness` and decodes them based on the artifact to get the final function output. The artifact is the compiled output of the contract, and has information like the function signature, parameter types, and return types

Beyond using them inside your other functions, they are convenient for providing an interface that reads storage, applies logic and returns values to a UI or test. Below is a snippet from exposing the `balance_of_private` function from a token implementation, which allows a user to easily read their balance, similar to the `balanceOf` function in the ERC20 standard.

balance\_of\_private

```
#[external("utility")]  
unconstrained fn balance_of_private(owner: AztecAddress) -> u128 {  
    self.storage.balances.at(owner).balance_of()  
}
```

> [Source code: noir-projects/noir-contracts/contracts/app/token\_contract/src/main.nr#L510-L515](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-contracts/contracts/app/token_contract/src/main.nr#L510-L515)

info

Note, that utility functions can have access to both private and (historical) public data when executed on the user's device. This is possible since these functions are not invoked as part of transactions, so we don't need to worry about preventing a contract from e.g. accidentally using stale or unverified public state.

## Public functions #[external("public")]]")

A public function is executed by the sequencer and has access to a state model that is very similar to that of the EVM and Ethereum. Even though they work in an EVM-like model for public transactions, they are able to write data into private storage that can be consumed later by a private function.

note

All data inserted into private storage from a public function will be publicly viewable (not private).

To create a public function you can annotate it with the `#[external("public")]` attribute. This will make the public context available within the function's execution scope.

set\_minter

```
#[external("public")]  
fn set_minter(minter: AztecAddress, approve: bool) {  
    assert(self.storage.admin.read().eq(self.msg_sender().unwrap()), "caller is not admin");  
    self.storage.minters.at(minter).write(approve);  
}
```

> [Source code: noir-projects/noir-contracts/contracts/app/token\_contract/src/main.nr#L147-L153](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-contracts/contracts/app/token_contract/src/main.nr#L147-L153)

Under the hood:

* Context Creation: The macro inserts code at the beginning of the function to create a`PublicContext` object:

```
let mut context = PublicContext::new(args_hasher);
```

This context provides access to public state and transaction information

* Storage Access: If the contract has a storage struct defined, the macro inserts code to initialize the storage:

```
let storage = Storage::init(&mut context);
```

* Function Body Wrapping: The original function body is wrapped in a new scope that handles the context and return value
* Visibility Control: The function is marked as pub, making it accessible from outside the contract.
* Unconstrained Execution: Public functions are marked as unconstrained, meaning they don't generate proofs and are executed directly by the sequencer.

## Constrained `view` Functions #[view]The `#[view]` attribute can be applied to a `#[external("private")]` or a `#[external("public")]` function and it guarantees that the function cannot modify any contract state (just like `view` functions in Solidity).

## `Initializer` Functions #[initializer]This is used to designate functions as initializers (or constructors) for an Aztec contract. These functions are responsible for setting up the initial state of the contract when it is first deployed. The macro does two important things:

* `assert_initialization_matches_address_preimage(context)`: This checks that the arguments and sender to the initializer match the commitments from the address preimage
* `mark_as_initialized(&mut context)`: This is called at the end of the function to emit the initialization nullifier, marking the contract as fully initialized and ensuring this function cannot be called again

Key things to keep in mind:

* A contract can have multiple initializer functions defined, but only one initializer function should be called for the lifetime of a contract instance
* Other functions in the contract will have an initialization check inserted, ie they cannot be called until the contract is initialized, unless they are marked with [`#[noinitcheck]`](#noinitcheck)

## #[noinitcheck]In normal circumstances, all functions in an Aztec contract (except initializers) have an initialization check inserted at the beginning of the function body. This check ensures that the contract has been initialized before any other function can be called. However, there may be scenarios where you want a function to be callable regardless of the contract's initialization state. This is when you would use `#[noinitcheck]`.

When a function is annotated with `#[noinitcheck]`:

* The Aztec macro processor skips the [insertion of the initialization check](#initializer-functions-initializer) for this specific function
* The function can be called at any time, even if the contract hasn't been initialized yet

## #[only\_self]External functions marked with #[only\_self] attribute can only be called by the contract itself - if other contracts try to make the call it will fail.

This attribute is commonly used when an action starts in private but needs to be completed in public. The public
function must be marked with #[only\_self] to restrict access to only the contract itself. A typical example is a private
token mint operation that needs to enqueue a call to a public function to update the publicly tracked total token
supply.

It is also useful in private functions when dealing with tasks of an unknown size but with a large upper bound (e.g. when needing to process an unknown amount of notes or nullifiers) as they allow splitting the work in multiple circuits, possibly resulting in performance improvements for low-load scenarios.

This macro inserts a check at the beginning of the function to ensure that the caller is the contract itself. This is done by adding the following assertion:

```
assert(self.msg_sender() == self.address, "Function can only be called by the same contract");
```

## Implementing notesThe `#[note]` attribute is used to define notes in Aztec contracts.

When a struct is annotated with `#[note]`, the Aztec macro applies a series of transformations and generates implementations to turn it into a note that can be used in contracts to store private data.

1. **Note Interface Implementation**: The macro automatically implements the `NoteType`, `NoteHash` and `Packable<N>` traits for the annotated struct. This includes the following methods:

   * `get_id`
   * `compute_note_hash`
   * `compute_nullifier`
   * `pack`
   * `unpack`
2. **Property Metadata**: A separate struct is generated to describe the note's fields, which is used for efficient retrieval of note data
3. **Export Information**: The note type and its ID are automatically exported

## Before expansionHere is how you could define a custom note:

```
#[note]  
struct CustomNote {  
    data: Field,  
    owner: Address,  
}
```

## After expansion```
impl NoteType for CustomNote {  
    fn get_id() -> Field {  
        // Assigned by macros by incrementing a counter  
        2  
    }  
}  
  
impl NoteHash for CustomNote {  
    fn compute_note_hash(self, storage_slot: Field) -> Field {  
        let inputs = array_concat(self.pack(), [storage_slot]);  
        poseidon2_hash_with_separator(inputs, GENERATOR_INDEX__NOTE_HASH)  
    }  
  
    fn compute_nullifier(self, context: &mut PrivateContext, note_hash_for_nullification: Field) -> Field {  
        let owner_npk_m_hash = get_public_keys(self.owner).npk_m.hash();  
        let secret = context.request_nsk_app(owner_npk_m_hash);  
        poseidon2_hash_with_separator(  
            [  
            note_hash_for_nullification,  
            secret  
        ],  
            GENERATOR_INDEX__NOTE_NULLIFIER as Field  
        )  
    }  
  
    unconstrained fn compute_nullifier_unconstrained(self, storage_slot: Field, contract_address: AztecAddress, note_nonce: Field) -> Field {  
        // We set the note_hash_counter to 0 as the note is not transient and the concept of transient note does  
        // not make sense in an unconstrained context.  
        let retrieved_note = RetrievedNote { note: self, contract_address, nonce: note_nonce, note_hash_counter: 0 };  
        let note_hash_for_nullification = compute_note_hash_for_nullification(retrieved_note, storage_slot);  
        let owner_npk_m_hash = get_public_keys(self.owner).npk_m.hash();  
        let secret = get_nsk_app(owner_npk_m_hash);  
        poseidon2_hash_with_separator(  
            [  
            note_hash_for_nullification,  
            secret  
        ],  
            GENERATOR_INDEX__NOTE_NULLIFIER as Field  
        )  
    }  
}  
  
impl CustomNote {  
    pub fn new(x: [u8; 32], y: [u8; 32], owner: AztecAddress) -> Self {  
        CustomNote { x, y, owner }  
    }  
}  
  
struct CustomNoteProperties {  
    data: aztec::note::note_getter_options::PropertySelector,  
    owner: aztec::note::note_getter_options::PropertySelector,  
}
```

Key things to keep in mind:

* Developers can override any of the auto-generated methods by specifying a note interface
* The note's fields are automatically serialized and deserialized in the order they are defined in the struct

## Storage struct #[storage]The `#[storage]` attribute is used to define the storage structure for an Aztec contract.

When a struct is annotated with `#[storage]`, the macro does this under the hood:

1. **Context Injection**: injects a `Context` generic parameter into the storage struct and all its fields. This allows the storage to interact with the Aztec context, eg when using `context.msg_sender()`
2. **Storage Implementation Generation**: generates an `impl` block for the storage struct with an `init` function. The developer can override this by implementing a `impl` block themselves
3. **Storage Slot Assignment**: automatically assigns storage slots to each field in the struct based on their serialized length
4. **Storage Layout Generation**: a `StorageLayout` struct and a global variable are generated to export the storage layout information for use in the contract artifact

## Before expansion```
#[storage]  
struct Storage {  
    balance: PublicMutable<Field>,  
    owner: PublicMutable<Address>,  
    token_map: Map<Address, Field>,  
}
```

## After expansion```
struct Storage<Context> {  
    balance: PublicMutable<Field, Context>,  
    owner: PublicMutable<Address, Context>,  
    token_map: Map<Address, Field, Context>,  
}  
  
impl<Context> Storage<Context> {  
    fn init(context: Context) -> Self {  
        Storage {  
            balance: PublicMutable::new(context, 1),  
            owner: PublicMutable::new(context, 2),  
            token_map: Map::new(context, 3, |context, slot| Field::new(context, slot)),  
        }  
    }  
}  
  
struct StorageLayout {  
    balance: dep::aztec::prelude::Storable,  
    owner: dep::aztec::prelude::Storable,  
    token_map: dep::aztec::prelude::Storable,  
}  
  
#[abi(storage)]  
global CONTRACT_NAME_STORAGE_LAYOUT = StorageLayout {  
    balance: dep::aztec::prelude::Storable { slot: 1 },  
    owner: dep::aztec::prelude::Storable { slot: 2 },  
    token_map: dep::aztec::prelude::Storable { slot: 3 },  
};
```

Key things to keep in mind:

* Only one storage struct can be defined per contract
* `Map` types and private `Note` types always occupy a single storage slot

## Further reading* [Macros reference](/developers/docs/aztec-nr/framework-description/macros)
* [How do macros work](/developers/docs/aztec-nr/framework-description/functions/attributes)

---


# Private <> Public Communication

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/functions/public_private_calls

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Disclaimer

Disclaimer

We are building Aztec as transparently as we can. The documents published here are living documents. The protocol, local network, language, and tools are all subject to change over time.

Please see [here](/developers/docs/resources/considerations/limitations) for details of known Aztec protocol and Aztec local network limitations.

If you would like to help us build Aztec:

* Contribute code on [GitHub](https://github.com/AztecProtocol); or
* Join in [forum](https://discourse.aztec.network/) discussions.

Aztec operates on a model of private and public functions that are able to work together. Private functions work by providing evidence of correct execution generated locally through kernel proofs. Public functions, on the other hand, are able to utilize the latest state to manage updates and perform alterations.

On this page, you’ll learn:

* How private and public functions work
* The role of public functions in managing state alterations and updates
* Communication and interactions between private and public functions
* How the sequencer manages the order of operations of private functions

## ObjectivesThe goal for L2 communication is to setup the most simple mechanism that will support

* *private* and *public* functions
* *private* functions that can call *private* or *public* functions
* *public* functions that can call *private* or *public* functions

Before diving into the communication abstracts for Aztec, we need to understand some of our limitations. One being that public functions (as known from Ethereum) must operate on the current state to provide meaningful utility, e.g., at the tip.
This works fine when there is only one builder (sequencer) executing it first, and then others verifying as the builder always knows the tip. On the left in the diagram below, we see a block where the transactions are applied one after another each building on the state before it. For example, if Tx 1 update storage `a = 5`, then in Tx 2 reading `a` will return `5`.

This works perfectly well when everything is public and a single builder is aware of all changes. However, in a private setting, we require the user to present evidence of correct execution as part of their transaction in the form of a kernel proof (generated locally on user device ahead of time). This way, the builder doesn't need to have knowledge of everything happening in the transaction, only the results. If we were to build this proof on the latest state, we would encounter problems. How can two different users build proofs at the same time, given that they will be executed one after the other by the sequencer? The simple answer is that they cannot, as race conditions would arise where one of the proofs would be invalidated by the other due to a change in the state root (which would nullify Merkle paths).

To avoid this issue, we permit the use of historical data as long as the data has not been nullified previously. Note, that because this must include nullifiers that were inserted after the proof generation, but before execution we need to nullify (and insert the data again) to prove that it was not nullified. Without emitting the nullifier we would need our proof to point to the current head of the nullifier tree to have the same effect, e.g., back to the race conditions we were trying to avoid.

In this model, instead of informing the builder of our intentions, we construct the proof π\piπ and then provide them with the transaction results (new note hashes and nullifiers, contract deployments and cross-chain messages) in addition to π\piπ. The builder will then be responsible for inserting these new note hashes and nullifiers into the state. They will be aware of the intermediates and can discard transactions that try to produce existing nullifiers (double spend), as doing so would invalidate the rollup proof.

On the left-hand side of the diagram below, we see the fully public world where storage is shared, while on the right-hand side, we see the private world where all reads are historical.

![](/assets/ideal-img/com-abs-1.fc9ffe4.640.png)

Given that Aztec will comprise both private and public functions, it is imperative that we determine the optimal ordering for these functions. From a logical standpoint, it is reasonable to execute the private functions first as they are executed on a state SiS\_iSi​, where i≤ni \le ni≤n, with SnS\_nSn​ representing the current state where the public functions always operate on the current state SnS\_nSn​. Prioritizing the private functions would also afford us the added convenience of enabling them to invoke the public functions, which is particularly advantageous when implementing a peer-to-pool architecture such as that employed by Uniswap.

Transactions that involve both private and public functions will follow a specific order of execution, wherein the private functions will be executed first, followed by the public functions, and then moving on to the next transaction.

It is important to note that the execution of private functions is prioritized before executing any public functions. This means that private functions cannot "wait" on the results of any of their calls to public functions. Stated differently, any calls made across domains are unilateral in nature, akin to shouting into the void with the hope that something will occur at a later time. The figure below illustrates the order of function calls on the left-hand side, while the right-hand side shows how the functions will be executed. Notably, the second private function call is independent of the output of the public function and merely occurs after its execution.

![](/assets/ideal-img/com-abs-2.6be3ffb.640.png)

Multiple of these transactions are then ordered into a L2 block by the sequencer, who will also be executing the public functions (as they require the current head). Example seen below.

![](/assets/ideal-img/com-abs-3.53e3f56.640.png)

info

Be mindful that if part of a transaction is reverting, say the public part of a call, it will revert the entire transaction. Similarly to Ethereum, it might be possible for the block builder to create a block such that your valid transaction reverts because of altered state, e.g., trade incurring too much slippage or the like.

To summarize:

* *Private* function calls are fully "prepared" and proven by the user, which provides the kernel proof along with new note hashes and nullifiers to the sequencer.
* *Public* functions altering public state (updatable storage) must be executed at the current "head" of the chain, which only the sequencer can ensure, so these must be executed separately to the *private* functions.
* *Private* and *public* functions within an Aztec transaction are therefore ordered such that first *private* functions are executed, and then *public*.

A more comprehensive overview of the interplay between private and public functions and their ability to manipulate data is presented below. It is worth noting that all data reads performed by private functions are historical in nature, and that private functions are not capable of modifying public storage. Conversely, public functions have the capacity to manipulate private storage (e.g., inserting new note hashes, potentially as part of transferring funds from the public domain to the private domain).

![](/assets/ideal-img/com-abs-4.8f27a30.640.png)

info

You can think of private and public functions as being executed by two actors that can only communicate to each other by mailbox.

So, with private functions being able to call public functions (unilaterally) we had a way to go from private to public, what about the other way? Recall that public functions CANNOT call private functions directly. Instead, you can use the append-only merkle tree to save messages from a public function call, that can later be executed by a private function. Note, only a transaction coming after the one including the message from a public function can consume it. In practice this means that unless you are the sequencer it will not be within the same rollup.

Given that private functions have the capability of calling public functions unilaterally, it is feasible to transition from a private to public function within the same transaction. However, the converse is not possible. To achieve this, the append-only merkle tree can be employed to save messages from a public function call, which can then be executed by a private function at a later point in time. It is crucial to reiterate that this can only occur at a later stage and cannot take place within the same rollup because the proof cannot be generated by the user.

info

Theoretically the builder has all the state trees after the public function has inserted a message in the public tree, and is able to create a proof consuming those messages in the same block. But it requires pending UTXO's on a block-level.

From the above, we should have a decent idea about what private and public functions can do inside the L2, and how they might interact.

## A note on L2 access controlMany applications rely on some form of access control to function well. USDC have a blacklist, where only parties not on the list should be able to transfer. And other systems such as Aave have limits such that only the pool contract is able to mint debt tokens and transfers held funds.

Access control like this cannot easily be enforced in the private domain, as reading is also nullifying (to ensure data is up to date). However, as it is possible to read historical public state, one can combine private and public functions to get the desired effect.

This concept is known as delayed public mutable state, and relies on using delays when changing public data so that it can also be read in private with currentness guarantees. Since values cannot be immediately modified but instead require delays to elapse, it is possible to privately prove that an application is using the current value *as long as the transaction gets included before some time in the future*, which would be the earliest the value could possibly change.

If the public state is only changed infrequently, and it is acceptable to have delays when doing so, then delayed public mutable state is a good solution to this problem.

---


# Contract Structure

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/contract_structure

Version: Devnet (v3.0.0-devnet.20251212)

On this page

A contract is a collection of persistent state variables and [functions](/developers/docs/aztec-nr/framework-description/functions) which may manipulate these variables. Functions and state variables within a contract's scope are said to belong to that contract. A contract can only access and modify its own state. If a contract wishes to access or modify another contract's state, it must make a call to an external function of the other contract. For anything to happen on the Aztec network, an external function of a contract needs to be called.

## ContractA contract may be declared and given a name using the `contract` keyword (see snippet below). By convention, contracts are named in `PascalCase`.

contract keyword

```
contract MyContract {  
  
    // Imports  
  
    // Storage  
  
    // Functions  
}
```

A note for vanilla Noir devs

There is no [`main()`](https://noir-lang.org/docs/getting_started/project_breakdown/#mainnr) function within a Noir `contract` scope. More than one function can be an entrypoint.

## Directory structureHere's a common layout for a basic Aztec.nr Contract project:

```
─── my_aztec_contract_project  
       ├── src  
       │     ├── main.nr       <-- your contract  
       └── Nargo.toml          <-- package and dependency management
```

* See the vanilla Noir docs for [more info on packages](https://noir-lang.org/docs/noir/modules_packages_crates/crates_and_packages).

---


# Importing Aztec.nr

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/dependencies

Version: Devnet (v3.0.0-devnet.20251212)

On this page

On this page you will find information about Aztec.nr libraries and up-to-date paths for use in your `Nargo.toml`.

## Aztec```
aztec = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/aztec-nr/aztec" }
```

This is the core Aztec library that is required for every Aztec.nr smart contract.

## Address note```
address_note = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/aztec-nr/address-note" }
```

This is a library for utilizing notes that hold addresses. Find it on [GitHub](https://github.com/AztecProtocol/aztec-packages/tree/master/noir-projects/aztec-nr/address-note/src).

## Easy private state```
easy_private_state = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/aztec-nr/easy-private-state" }
```

This is an abstraction library for using private variables like [`EasyPrivateUint` (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/6c20b45993ee9cbd319ab8351e2722e0c912f427/noir-projects/aztec-nr/easy-private-state/src/easy_private_state.nr#L17).

## Protocol Types```
protocol_types = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/noir-protocol-circuits/crates/types"}
```

This library contains types that are used in the Aztec protocol. Find it on [GitHub](https://github.com/AztecProtocol/aztec-packages/tree/master/noir-projects/noir-protocol-circuits/crates/types/src).

## Value note```
value_note = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/aztec-nr/value-note" }
```

---


# Declaring Contract Storage

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/how_to_define_storage

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to declare storage and use various storage types provided by Aztec.nr for managing contract state.

## Prerequisites* An Aztec contract project set up with `aztec-nr` dependency
* Understanding of Aztec's private and public state model
* Familiarity with Noir struct syntax
* Basic knowledge of maps and data structures

For storage concepts, see [storage overview](/developers/docs/foundational-topics/state_management).

## Define your storage struct## Create a storage struct with #[storage]Declare storage using a struct annotated with `#[storage]`. For example:

```
#[storage]  
struct Storage<Context> {  
    // The admin of the contract  
    admin: PublicMutable<AztecAddress, Context>,  
}
```

## Context parameterThe `Context` parameter provides execution mode information.

## Access storage in functionsUse the `storage` keyword to access your storage variables in contract functions.

## Use maps for key-value storageMaps store key-value pairs where keys are `Field` elements and values can be any type.

You can import `Map` as:

```
use dep::aztec::state_vars::Map;
```

## Understand map structure* Keys: Always `Field` or serializable types
* Values: Any type, including other maps
* Multiple maps: Supported in the same contract

## Declare private mapsSpecify the note type for private storage maps:

```
private_items: Map<AztecAddress, PrivateSet<MyNote, Context>, Context>,
```

## Declare public mapsUse `PublicState` for public storage maps:

```
authorized_users: Map<AztecAddress, PublicMutable<bool, Context>, Context>,
```

## Access map valuesUse the `.at()` method to access values by key:

```
assert(storage.authorized_users.at(context.msg_sender()).read(), "caller is not authorized");
```

tip

This is equivalent to Solidity's `authorized_users[msg.sender]` pattern.

## Use private storage typesAztec.nr provides three private state variable types:

* `PrivateMutable<NoteType>`: Single mutable private value
* `PrivateImmutable<NoteType>`: Single immutable private value
* `PrivateSet<NoteType>`: Collection of private notes

All private storage operates on note types rather than arbitrary data types. Learn how to implement custom notes and use them with Maps [here](/developers/docs/aztec-nr/framework-description/how_to_implement_custom_notes)

## PrivateMutablePrivateMutable is a private state variable that is unique in a way. When a PrivateMutable is initialized, a note is created to represent its value. Updating the value means to destroy the current note, and to create a new one with the updated value.

Like for public state, we define the struct to have context and a storage slot. You can view the implementation [here](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/state_vars/private_mutable.nr).

An example of `PrivateMutable` usage in contracts is keeping track of important values. The `PrivateMutable` is added to the `Storage` struct as follows:

```
// #[storage]  
// ...etc  
my_value: PrivateMutable<MyNote, Context>,
```

## `initialize`As mentioned, the PrivateMutable should be initialized to create the first note and value. When this function is called, a nullifier of the storage slot is created, preventing this PrivateMutable from being initialized again.

Unlike public states, which have a default initial value of `0` (or many zeros, in the case of a struct, array or map), a private state (of type `PrivateMutable`, `PrivateImmutable` or `PrivateSet`) does not have a default initial value. The `initialize` method (or `insert`, in the case of a `PrivateSet`) must be called.

## `is_initialized`An unconstrained method to check whether the PrivateMutable has been initialized or not. It takes an optional owner and returns a boolean. You can view the implementation [here (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/state_vars/private_mutable.nr).

```
let is_initialized = my_value.is_initialized();
```

## `replace`To update the value of a `PrivateMutable`, we can use the `replace` method. The method takes a function (or closure) that transforms the current note into a new one.

When called, the method will:

* Nullify the old note
* Apply the transform function to produce a new note
* Insert the new note into the data tree

An example of this is seen in an example card game, where an update function is passed in to transform the current note into a new one (in this example, updating a `CardNote` data):

```
let new_note = MyNote::new(new_value, owner);  
storage.my_value.replace(&mut new_note).deliver(encode_and_encrypt_note(&mut context, owner));
```

info

Calling `deliver(encode_and_encrypt_note())` on the `replace` method will encrypt the new note and post it to the data availability layer so that the note information is retrievable by the recipient.

If two people are trying to modify the PrivateMutable at the same time, only one will succeed as we don't allow duplicate nullifiers! Developers should put in place appropriate access controls to avoid race conditions (unless a race is intended!).

## `get_note`This function allows us to get the note of a PrivateMutable, essentially reading the value.

```
let note = my_value.get_note()
```

info

To ensure that a user's private execution always uses the latest value of a PrivateMutable, the `get_note` function will nullify the note that it is reading. This means that if two people are trying to use this function with the same note, only one will succeed (no duplicate nullifiers allowed).

This also makes read operations indistinguishable from write operations and allows the sequencer to verifying correct execution without learning anything about the value of the note.

## `view_note`Functionally similar to [`get_note`](#get_note), but executed in unconstrained functions and can be used by the wallet to fetch notes for use by front-ends etc.

## PrivateImmutable`PrivateImmutable` represents a unique private state variable that, as the name suggests, is immutable. Once initialized, its value cannot be altered. You can view the implementation [here (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/state_vars/private_immutable.nr).

## `initialize`When this function is invoked, it creates a nullifier for the storage slot, ensuring that the PrivateImmutable cannot be initialized again.

Set the value of an PrivateImmutable by calling the `initialize` method:

```
#[external("private")]  
fn initialize_private_immutable(my_value: u8) {  
    let new_note = MyNote::new(my_value, context.msg_sender().unwrap());  
  
    storage.my_private_immutable.initialize(new_note).deliver(encode_and_encrypt_note(  
        &mut context,  
        context.msg_sender(),  
    ));  
}
```

info

Calling `deliver(encode_and_encrypt_note())` on `initialize` will encrypt the new note and post it to the data availability layer so that the note information is retrievable by the recipient.

Once initialized, an PrivateImmutable's value remains unchangeable. This method can only be called once.

## `is_initialized`An unconstrained method to check if the PrivateImmutable has been initialized. Takes an optional owner and returns a boolean. You can find the implementation [here (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/state_vars/private_immutable.nr).

## `get_note`Similar to the `PrivateMutable`, we can use the `get_note` method to read the value of an PrivateImmutable.

Use this method to retrieve the value of an initialized PrivateImmutable.

```
#[external("private")]  
fn get_immutable_note() -> MyNote {  
    storage.my_private_immutable.get_note()  
}
```

Unlike a `PrivateMutable`, the `get_note` function for an PrivateImmutable doesn't nullify the current note in the background. This means that multiple accounts can concurrently call this function to read the value.

This function will throw if the `PrivateImmutable` hasn't been initialized.

## `view_note`Functionally similar to `get_note`, but executed unconstrained and can be used by the wallet to fetch notes for use by front-ends etc.

## PrivateSet`PrivateSet` is used for managing a collection of notes. All notes in a `PrivateSet` are of the same `NoteType`. But whether these notes all belong to one entity, or are accessible and editable by different entities, is up to the developer.

For example, adding a mapping of private items to storage, indexed by `AztecAddress`:

```
private_items: Map<AztecAddress, PrivateSet<MyNote, Context>, Context>,
```

## `insert`Allows us to modify the storage by inserting a note into the `PrivateSet`.

A hash of the note will be generated, and inserted into the note hash tree, allowing us to later use in contract interactions. Recall that the content of the note should be shared with the owner to allow them to use it, as mentioned this can be done via an encrypted log or offchain via web2, or completely offline.

```
storage.set.at(aztec_address).insert(new_note).deliver(encode_and_encrypt_note(&mut context, aztec_address));
```

info

Calling `deliver(encode_and_encrypt_note())` on `insert` will encrypt the new note and post it to the data availability layer so that the note information is retrievable by the recipient.

## `pop_notes`This function pops (gets, removes and returns) the notes the account has access to based on the provided filter.

The kernel circuits are constrained to a maximum number of notes this function can return at a time. Check [here (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-protocol-circuits/crates/types/src/constants.nr) and look for `MAX_NOTE_HASH_READ_REQUESTS_PER_CALL` for the up-to-date number.

Because of this limit, we should always consider using the second argument `NoteGetterOptions` to limit the number of notes we need to read and constrain in our programs. This is quite important as every extra call increases the time used to prove the program and we don't want to spend more time than necessary.

An example of such options is using the filter functions from the value note library (like `filter_notes_min_sum`) to get "enough" notes to cover a given value. Essentially, this function will return just enough notes to cover the amount specified such that we don't need to read all our notes. For users with a lot of notes, this becomes increasingly important.

```
use value_note::filter::filter_notes_min_sum;  
  
// etc...  
let options = NoteGetterOptions::with_filter(filter_notes_min_sum, subtrahend as Field);  
let notes = self.set.pop_notes(options);
```

## `get_notes`This function has the same behavior as `pop_notes` above but it does not delete the notes.

## `remove`Will remove a note from the `PrivateSet` if it previously has been read from storage, e.g. you have fetched it through a `get_notes` call. This is useful when you want to remove a note that you have previously read from storage and do not have to read it again.

Note that if you obtained the note you are about to remove via `get_notes` it's much better to use `pop_notes` as `pop_notes` results in significantly fewer constraints since it doesn't need to check that the note has been previously read, as it reads and deletes at once.

## `view_notes`Functionally similar to [`get_notes`](#get_notes), but executed unconstrained and can be used by the wallet to fetch notes for use by front-ends etc.

```
let mut options = NoteViewerOptions::new();  
let notes = set.view_notes(options.set_offset(offset));
```

There's also a limit on the maximum number of notes that can be returned in one go. To find the current limit, refer to [this file (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/note/constants.nr) and look for `MAX_NOTES_PER_PAGE`.

The key distinction is that this method is unconstrained. It does not perform a check to verify if the notes actually exist, which is something the [`get_notes`](#get_notes) method does under the hood. Therefore, it should only be used in an unconstrained contract function.

This function requires a `NoteViewerOptions`. The `NoteViewerOptions` is essentially similar to the [`NoteGetterOptions`](#notegetteroptions), except that it doesn't take a custom filter.

## Use public storage typesAztec.nr provides two public state variable types that work similarly to Ethereum's storage model:

* `PublicMutable<T>`: Mutable public value that can be updated
* `PublicImmutable<T>`: Immutable public value that can only be set once

Both types are generic over any serializable type `T`, allowing you to store simple values like integers and booleans, as well as complex structs. Public storage is transparent - all values are visible to anyone observing the blockchain.

## PublicMutableStore mutable public state using `PublicMutable<T>` for values that need to be updated throughout the contract's lifecycle.

info

An example using a larger struct can be found in the [lending example](https://github.com/AztecProtocol/aztec-packages/tree/v3.0.0-devnet.20251212/noir-projects/noir-contracts/contracts/app/lending_contract)'s use of an [`Asset`](https://github.com/AztecProtocol/aztec-packages/tree/v3.0.0-devnet.20251212/noir-projects/noir-contracts/contracts/app/lending_contract/src/asset.nr).

For example, to add `config_value` public state variable into our storage struct, we can define it as:

```
config_value: PublicMutable<MyStruct, Context>,
```

To add a group of `authorized_users` that are able to perform actions in our contract, and we want them in public storage:

```
authorized_users: Map<AztecAddress, PublicMutable<bool, Context>, Context>,
```

## `read`On the `PublicMutable` structs we have a `read` method to read the value at the location in storage. For our `config_value` example from earlier, this could be used as follows to check that the stored value matches the `msg_sender()`:

```
let admin = storage.admin.read();  
assert(admin == context.msg_sender().unwrap(), "caller is not admin");
```

## `write`We have a `write` method on the `PublicMutable` struct that takes the value to write as an input and saves this in storage. It uses the serialization method to serialize the value which inserts (possibly multiple) values into storage:

```
storage.admin.write(new_admin);
```

## PublicImmutable`PublicImmutable` is a type that is initialized from public once, typically during a contract deployment, but which can later be read from public, private and utility execution contexts. This state variable is useful for stuff that you would usually have in `immutable` values in Solidity, e.g. this can be the name of a contract or its version number.

Just like the `PublicMutable` it is generic over the variable type `T`. The type must implement the `Serialize` and `Deserialize` traits.

```
my_public_immutable: PublicImmutable<MyStruct, Context>,
```

You can find the details of `PublicImmutable` in the implementation [here (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/state_vars/public_immutable.nr).

## `new`Is done exactly like the `PublicMutable` struct, but with the `PublicImmutable` struct.

```
my_public_immutable: PublicImmutable<MyStruct, Context>,
```

## `initialize`This function sets the immutable value. It can only be called once.

```
storage.my_public_immutable.initialize(my_value);
```

warning

A `PublicImmutable`'s storage **must** only be set once via `initialize`. Attempting to override this by manually accessing the underlying storage slots breaks all properties of the data structure, rendering it useless.

```
#[external("public")]  
fn initialize_public_immutable(my_value: u8) {  
    let mut new_struct = MyStruct { account: context.msg_sender().unwrap(), value: my_value };  
    storage.my_public_immutable.initialize(new_struct);  
}
```

## `read`Returns the stored immutable value. This function is available in public, private and utility contexts.

```
#[external("utility")]  
unconstrained fn get_public_immutable() -> MyStruct {  
    storage.my_public_immutable.read()  
}
```

## Use custom structs in public storageBoth `PublicMutable` and `PublicImmutable` are generic over any serializable type, which means you can store custom structs in public storage. This is useful for storing configuration data, game state, or any other structured data that needs to be publicly visible.

## Define a custom struct for storageTo use a custom struct in public storage, it must implement the `Packable` trait. You can automatically derive this along with other useful traits:

```
use dep::aztec::protocol_types::{  
    address::AztecAddress,  
    traits::{Deserialize, Packable, Serialize}  
};  
  
// Required derives for public storage:  
// - Packable: Required for all public storage  
// - Serialize: Required for returning from functions  
// - Deserialize: Required for receiving as parameters  
#[derive(Deserialize, Packable, Serialize)]  
pub struct Asset {  
    pub interest_accumulator: u128,  
    pub last_updated_ts: u64,  
    pub loan_to_value: u128,  
    pub oracle: AztecAddress,  
}
```

Common optional derives include:

* `Eq`: For equality comparisons between structs

## Store custom structsOnce defined, use your custom struct in storage declarations:

```
#[storage]  
struct Storage<Context> {  
    // Single custom struct  
    config: PublicMutable<Asset, Context>,  
  
    // Map of custom structs  
    assets: Map<Field, PublicMutable<Asset, Context>, Context>,  
  
    // Immutable custom struct (like contract config)  
    initial_config: PublicImmutable<Asset, Context>,  
}
```

## Read and write custom structsWork with custom structs using the same `read()` and `write()` methods as built-in types:

```
#[public]  
fn update_asset(asset_id: Field, new_accumulator: u128) {  
    // Read the current struct  
    let mut asset = storage.assets.at(asset_id).read();  
  
    // Modify fields  
    asset.interest_accumulator = new_accumulator;  
    asset.last_updated_ts = context.timestamp();  
  
    // Write back the updated struct  
    storage.assets.at(asset_id).write(asset);  
}  
  
#[public]  
fn get_asset(asset_id: Field) -> Asset {  
    storage.assets.at(asset_id).read()  
}
```

You can also create and store new struct instances:

```
#[public]  
fn initialize_asset(  
    interest_accumulator: u128,  
    loan_to_value: u128,  
    oracle: AztecAddress  
) {  
    let last_updated_ts = context.timestamp() as u64;  
  
    storage.assets.at(0).write(  
        Asset {  
            interest_accumulator,  
            last_updated_ts,  
            loan_to_value,  
            oracle,  
        }  
    );  
}
```

## Use custom structs in nested mapsCustom structs work seamlessly with nested map structures:

```
#[derive(Deserialize, Eq, Packable, Serialize)]  
pub struct Game {  
    pub started: bool,  
    pub finished: bool,  
    pub current_round: u32,  
}  
  
#[storage]  
struct Storage<Context> {  
    // Map game_id -> player_address -> Game struct  
    games: Map<Field, Map<AztecAddress, PublicMutable<Game, Context>, Context>, Context>,  
}  
  
#[public]  
fn start_game(game_id: Field, player: AztecAddress) {  
    let game = Game {  
        started: true,  
        finished: false,  
        current_round: 0,  
    };  
    storage.games.at(game_id).at(player).write(game);  
}
```

## Delayed Public MutableThis storage type is used if you want to use public values in private execution.

A typical use case is some kind of system configuration, such as a protocol fee or access control permissions. These values are public (known by everyone) and mutable. Reading them in private however is tricky: private execution is always asynchronous and performed over *historical* state, and hence one cannot easily prove that a given public value is current.

Alternative approaches

A naive way to solve this is to enqueue a public call that will assert the current public value, but this leaks *which* public value is being read, severely reducing privacy. Even if the value itself is already public, the fact that we're using it because we're interacting with some related contract is not. For example, we may leak that we're interacting with a certain DeFi protocol by reading its fee.

An alternative approach is to create notes in public that are then nullified in private, but this introduces contention: only a single user may use the note and therefore read the state, since nullifying it will prevent all others from doing the same. In some schemes there's only one account that will read the state anyway, but this is not the general case.

Delayed Public Mutable state works around this by introducing **delays**:

* Instead, a value change is be scheduled ahead of time, and some minimum amount of time must pass between the scheduling and the new value taking effect.
* This means that we can privately prove that a historical public value cannot possibly change before some point in the future (due to the minimum delay), and therefore that our transaction will be valid **as long as it gets included before this future time**.
* In other words, we're saying "this value is public but can't change until \_\_\_".

This results in the following key properties of `DelayedPublicMutable` state:

* public values can only be changed after a certain delay has passed, never immediately
* the scheduling of value changes is itself public, including both the new value and the time at which the change will take effect
* transactions that read `DelayedPublicMutable` state become invalid after some time if not included in a block

Privacy Consideration

While `DelayedPublicMutable` state variables are much less leaky than the assertion in public approach, they do reveal some information to external observers by setting the `include_by_timestamp` property of the transaction request. The impact of this can be mitigated with proper selection of the delay value and schedule times.

## Choosing DelaysThe `include_by_timestamp` transaction property will be set to a value close to the current timestamp plus the duration of the delay in seconds. The exact value depends on the anchor block over which the private proof is constructed. For example, if current timestamp is `X` and a `DelayedPublicMutable` state variable has a delay of 3000 seconds, then transactions that read this value privately will set `include_by_timestamp` to a value close to 'X + 3000' (clients building proofs on older state will select a lower `include_by_timestamp`).

These delays can be changed during the contract lifetime as the application's needs evolve.

Delay duration

Applications using similar delays will therefore be part of the same privacy set. It is recommended to look for industry standards for these delays. For example:

* 12 hours for time-sensitive operations, such as emergency mechanisms
* 5 days for middle-of-the-road operations
* 2 weeks for operations that require lengthy public scrutiny.

Smaller delays are fine too. As a rule of thumb, the smaller the delay, the smaller the privacy set, so your mileage may vary.

Additionally, you may choose to coordinate and constrain your transactions to set `include_by_timestamp` to a value lower than would be strictly needed by the applications you interact with (if any!) using some common delay, and by doing so prevent privacy leakage.

Note that wallets can also warn users that a value change will soon take place and that sending a transaction at that time might result in reduced privacy, allowing them to choose to wait until after the epoch.

info

Even though only transactions that interact with `DelayedPublicMutable` state *need* to set the `include_by_timestamp` property, there is no reason why transactions that do not wouldn't also set this value.

If indeed most applications converge on a small set of delays, then wallets could opt to select any of those to populate the `include_by_timestamp` field, as if they were interacting with a `DelayedPublicMutable` state variable with that delay.

This prevents the network-wide privacy set from being split between transactions that read `DelayedPublicMutable` state and those that don't, which is beneficial to everyone.

## DelayedPublicMutableUnlike other state variables, `DelayedPublicMutable` receives not only a type parameter for the underlying datatype, but also a `DELAY` type parameter with the value change delay as a number of seconds.

```
my_delayed_value: DelayedPublicMutable<MyType, MY_DELAY, Context>,
```

note

`DelayedPublicMutable` requires that the underlying type `T` implements both the `ToField` and `FromField` traits, meaning it must fit in a single `Field` value. There are plans to extend support by requiring instead an implementation of the `Serialize` and `Deserialize` traits, therefore allowing for multi-field variables, such as complex structs.

Since `DelayedPublicMutable` lives in public storage, by default its contents are zeroed-out. Intialization is performed by calling `schedule_value_change`, resulting in initialization itself being delayed.

## `schedule_value_change`This is the means by which a `DelayedPublicMutable` variable mutates its contents. It schedules a value change for the variable at a future timestamp after the `DELAY` has elapsed from the current timestamp, at which point the scheduled value becomes the current value automatically and without any further action, both in public and in private. If a pending value change was scheduled but not yet effective (because insufficient time had elapsed), then the previous schedule value change is replaced with the new one and eliminated. There can only be one pending value change at a time.

This function can only be called in public, typically after some access control check:

```
#[external("public")]  
fn set_my_value(new_value: MyType) {  
    assert_eq(storage.admin.read(), context.msg_sender().unwrap(), "caller is not admin");  
    storage.my_delayed_value.schedule_value_change(new_value);  
}
```

If one wishes to schedule a value change from private, simply enqueue a public call to a public `internal` contract function. Recall that **all scheduled value changes, including the new value and scheduled timestamp are public**.

warning

A `DelayedPublicMutable`'s storage **must** only be mutated via `schedule_value_change`. Attempting to override this by manually accessing the underlying storage slots breaks all properties of the data structure, rendering it useless.

## `get_current_value`Returns the current value in a public, private or utility execution context. Once a value change is scheduled via `schedule_value_change` and the delay time passes, this automatically returns the new value.

```
storage.my_delayed_value.get_current_value()
```

Also, calling in private will set the `include_by_timestamp` property of the transaction request, introducing a new validity condition to the entire transaction: it cannot be included in any block with a timestamp larger than `include_by_timestamp`.

```
let current_value = storage.my_delayed_value.get_current_value();
```

## `get_scheduled_value`Returns the last scheduled value change, along with the timestamp at which the scheduled value becomes the current value. This may either be a pending change, if the timestamp is in the future, or the last executed scheduled change if the timestamp is in the past (in which case there are no pending changes).

```
storage.my_delayed_value.get_scheduled_value()
```

It is not possible to call this function in private: doing so would not be very useful at it cannot be asserted that a scheduled value change will not be immediately replaced if `shcedule_value_change` where to be called.

---


# Calling Other Contracts

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/how_to_call_contracts

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to call functions in other contracts from your Aztec smart contracts, enabling contract composability and interaction.

## Prerequisites* An Aztec contract project with dependencies properly configured
* Access to the target contract's source code or ABI
* Understanding of Aztec contract compilation and deployment

## Add the target contract as a dependencyAdd the contract you want to call to your `Nargo.toml` dependencies:

```
other_contract = { git="https://github.com/your-repo/", tag="v1.0.0", directory="path/to/contract" }
```

## Import the contract interfaceImport the contract at the top of your contract file:

```
use other_contract::OtherContract;
```

## Call contract functionsUse this pattern to call functions in other contracts:

1. Specify the contract address: `Contract::at(contract_address)`
2. Call the function: `.function_name(param1, param2)`
3. Execute the call: `.call(&mut context)`

## Make private function callsCall private functions directly using `.call()`:

```
OtherContract::at(contract_address).private_function(param1, param2).call(&mut context);
```

## Make public-to-public callsCall public functions from other public functions using `.call()`:

```
let result = OtherContract::at(contract_address)  
    .public_function(param1, param2, param3)  
    .call(&mut context);
```

## Make private-to-public callsEnqueue public functions to be executed after private execution completes:

```
OtherContract::at(contract_address)  
    .public_function(param1, param2)  
    .enqueue(&mut context);
```

info

Public functions always execute after private execution completes. Learn more in the [concepts overview](/developers/docs/foundational-topics).

## Use other call typesExplore additional call types for specialized use cases in the [call types reference](/developers/docs/foundational-topics/call_types).

---


# Implementing custom notes

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/how_to_implement_custom_notes

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to create custom note types for storing specialized private data in your Aztec contracts. Notes are the fundamental data structure in Aztec when working with private state.

## Prerequisites* Basic understanding of [Aztec private state](/developers/docs/foundational-topics/state_management)
* Familiarity with [notes and UTXOs](/developers/docs/foundational-topics/state_management)
* Aztec development environment set up

## Why create custom notes?You may want to create your own note type if you need to:

* Use a specific type of private data or struct not already implemented in Aztec.nr
* Experiment with custom note hashing and nullifier schemes
* Store multiple pieces of related data together (e.g., a card in a game with multiple attributes)
* Optimize storage by combining data that's used together

Built-in Note Types

Aztec.nr provides pre-built note types for common use cases:

**ValueNote** - For numeric values like token balances:

```
# In Nargo.toml  
value_note = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/smart-contracts/value-note" }
```

```
use value_note::value_note::ValueNote;  
let note = ValueNote::new(100, owner);
```

**AddressNote** - For storing Aztec addresses:

```
# In Nargo.toml  
address_note = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/smart-contracts/address-note" }
```

```
use address_note::address_note::AddressNote;  
let note = AddressNote::new(stored_address, owner);
```

If these don't meet your needs, continue reading to create your own custom note type.

## Standard note implementation## Creating a custom note structDefine your custom note with the `#[note]` macro:

```
use aztec::{  
    macros::notes::note,  
    oracle::random::random,  
    protocol_types::{address::AztecAddress, traits::Packable},  
};  
  
// The #[note] macro marks this struct as a note type  
// Required traits:  
// - Eq: Allows equality comparisons between notes  
// - Packable: Enables efficient packing/unpacking of the note's data  
#[derive(Eq, Packable)]  
#[note]  
pub struct CustomNote {  
    // Application-specific data  
    value: Field,  
    data: u32,  
    // Required fields for all notes  
    owner: AztecAddress,  // Used for access control and nullifier generation  
    randomness: Field,    // Prevents brute-force attacks on note contents  
}
```

The `#[note]` macro automatically implements other required traits for your note type (ex. the `NoteHash` trait).

## Required fieldsEvery custom note needs these essential fields:

1. **Application data**: Your specific fields (e.g., `value`, `amount`, `token_id`)
2. **Owner**: Used for nullifier generation and access control (must be `AztecAddress` type)
3. **Randomness**: Prevents brute-force attacks on note contents (must be `Field` type)

The order of fields doesn't matter, but convention is to put application data first, then owner, then randomness:

```
#[derive(Eq, Packable)]  
#[note]  
pub struct MyNote {  
    // Application-specific data  
    data: Field,  
    amount: u128,  
  
    // Required fields  
    owner: AztecAddress,  
    randomness: Field,  
}
```

## Why randomness mattersWithout randomness, note contents can be guessed through brute force. For example, if you know someone's Aztec address, you could try hashing it with many potential values to find which note hash in the tree belongs to them.

## Why owner is importantThe `owner` field provides two critical functions:

1. **Access control**: Ensures only the owner can spend the note
2. **Privacy from sender**: Prevents the sender from tracking when a note is spent

Without using the owner's nullifier key, a sender could derive the nullifier offchain and monitor when it appears in the nullifier tree, breaking privacy.

## Implementing note methodsA note is just a Struct, so you can add whatever methods you need. For example, you can add a constructor and helper methods:

```
impl CustomNote {  
    pub fn new(value: Field, data: u32, owner: AztecAddress) -> Self {  
        // Safety: We use randomness to preserve privacy. The sender already knows  
        // the full note pre-image, so we trust them to cooperate in random generation  
        let randomness = unsafe { random() };  
  
        CustomNote { value, data, owner, randomness }  
    }  
  
    pub fn get_value(self) -> Field {  
        self.value  
    }  
  
    pub fn get_data(self) -> u32 {  
        self.data  
    }  
}
```

## Custom note with custom hashingFor complete control over note hashing and nullifier generation, use the `#[custom_note]` macro:

```
use dep::aztec::{  
    context::PrivateContext,  
    macros::notes::custom_note,  
    note::note_interface::NoteHash,  
    protocol_types::{  
        constants::{GENERATOR_INDEX__NOTE_HASH, GENERATOR_INDEX__NOTE_NULLIFIER},  
        hash::poseidon2_hash_with_separator,  
        traits::Packable,  
    },  
};  
  
// TransparentNote for public-to-private transitions  
// No owner field needed - security comes from secret knowledge  
#[derive(Eq, Packable)]  
#[custom_note]  
pub struct TransparentNote {  
    amount: u128,  
    secret_hash: Field,  // Hash of a secret that must be known to spend  
}  
  
impl NoteHash for TransparentNote {  
    fn compute_note_hash(self, storage_slot: Field) -> Field {  
        let inputs = self.pack().concat([storage_slot]);  
        poseidon2_hash_with_separator(inputs, GENERATOR_INDEX__NOTE_HASH)  
    }  
  
    // Custom nullifier that doesn't use owner's key  
    // Security is enforced by requiring the secret preimage  
    fn compute_nullifier(  
        self,  
        _context: &mut PrivateContext,  
        note_hash_for_nullification: Field,  
    ) -> Field {  
        poseidon2_hash_with_separator(  
            [note_hash_for_nullification],  
            GENERATOR_INDEX__NOTE_NULLIFIER as Field,  
        )  
    }  
  
    unconstrained fn compute_nullifier_unconstrained(  
        self,  
        note_hash_for_nullification: Field  
    ) -> Field {  
        self.compute_nullifier(zeroed(), note_hash_for_nullification)  
    }  
}
```

This pattern is useful for "shielding" tokens - creating notes in public that can be redeemed in private by anyone who knows the secret.

## Basic usage in storageBefore diving into Maps, let's understand basic custom note usage.

## Declare storage```
use dep::aztec::state_vars::{PrivateSet, PrivateImmutable};  
  
#[storage]  
struct Storage<Context> {  
    // Collection of notes for a single owner  
    balances: PrivateSet<CustomNote, Context>,  
  
    // Single immutable configuration  
    config: PrivateImmutable<ConfigNote, Context>,  
}
```

## Insert notes```
use dep::aztec::messages::message_delivery::MessageDelivery;  
  
#[external("private")]  
fn create_note(value: Field, data: u32) {  
    let owner = context.msg_sender().unwrap();  
    let note = CustomNote::new(value, data, owner);  
  
    storage.balances  
        .insert(note)  
        .emit(&mut context, owner, MessageDelivery.CONSTRAINED_ONCHAIN);  
}
```

## Read notes```
use dep::aztec::note::note_getter_options::NoteGetterOptions;  
  
#[external("private")]  
fn get_notes() -> BoundedVec<CustomNote, MAX_NOTES_PER_PAGE> {  
    storage.balances.get_notes(NoteGetterOptions::new())  
}  
  
#[external("private")]  
fn find_note_by_value(target_value: Field) -> CustomNote {  
    let options = NoteGetterOptions::new()  
        .select(CustomNote::properties().value, target_value, Option::none())  
        .set_limit(1);  
  
    let notes = storage.balances.get_notes(options);  
    assert(notes.len() == 1, "Note not found");  
    notes.get(0)  
}
```

## Transfer notes```
#[external("private")]  
fn transfer_note(to: AztecAddress, value: Field) {  
    // Find and remove from sender  
    let note = find_note_by_value(value);  
    storage.balances.remove(note);  
  
    // Create new note for recipient  
    let new_note = CustomNote::new(note.value, note.data, to);  
    storage.balances.insert(new_note)  
        .emit(&mut context, to, MessageDelivery.CONSTRAINED_ONCHAIN);  
}
```

## Using custom notes with MapsMaps are essential for organizing custom notes by key in private storage. They allow you to efficiently store and retrieve notes based on addresses, IDs, or other identifiers.

## Common Map patterns```
use dep::aztec::{  
    macros::notes::note,  
    oracle::random::random,  
    protocol_types::{address::AztecAddress, traits::Packable},  
    state_vars::{Map, PrivateMutable, PrivateSet},  
};  
  
#[derive(Eq, Packable)]  
#[note]  
pub struct CardNote {  
    points: u32,  
    strength: u32,  
    owner: AztecAddress,  
    randomness: Field,  
}  
  
impl CardNote {  
    pub fn new(points: u32, strength: u32, owner: AztecAddress) -> Self {  
        let randomness = unsafe { random() };  
        CardNote { points, strength, owner, randomness }  
    }  
}  
  
#[storage]  
struct Storage<Context> {  
    // Map from player address to their collection of cards  
    card_collections: Map<AztecAddress, PrivateSet<CardNote, Context>, Context>,  
  
    // Map from player address to their active card  
    active_cards: Map<AztecAddress, PrivateMutable<CardNote, Context>, Context>,  
  
    // Nested maps: game_id -> player -> cards  
    game_cards: Map<Field, Map<AztecAddress, PrivateSet<CardNote, Context>, Context>, Context>,  
}
```

Common patterns:

* `Map<AztecAddress, PrivateSet<CustomNote>>` - Multiple notes per user (like token balances, card collections)
* `Map<AztecAddress, PrivateMutable<CustomNote>>` - Single note per user (like user profile, active state)
* `Map<Field, Map<AztecAddress, PrivateSet<CustomNote>>>` - Nested organization (game sessions, channels)

## Inserting into mapped PrivateSetsTo add notes to a mapped PrivateSet:

```
use dep::aztec::messages::message_delivery::MessageDelivery;  
  
#[external("private")]  
fn add_card_to_collection(player: AztecAddress, points: u32, strength: u32) {  
    let card = CardNote::new(points, strength, player);  
  
    // Insert into the player's collection  
    storage.card_collections  
        .at(player)  
        .insert(card)  
        .emit(&mut context, player, MessageDelivery.CONSTRAINED_ONCHAIN);  
}
```

## Using mapped PrivateMutableFor PrivateMutable in a Map, handle both initialization and updates:

```
use dep::aztec::messages::message_delivery::MessageDelivery;  
  
#[external("private")]  
fn set_active_card(player: AztecAddress, points: u32, strength: u32) {  
    // Check if already initialized  
    let is_initialized = storage.active_cards.at(player).is_initialized();  
  
    if is_initialized {  
        // Replace existing card  
        storage.active_cards  
            .at(player)  
            .replace(|_old_card| CardNote::new(points, strength, player))  
            .emit(&mut context, player, MessageDelivery.CONSTRAINED_ONCHAIN);  
    } else {  
        // Initialize for first time  
        let card = CardNote::new(points, strength, player);  
        storage.active_cards  
            .at(player)  
            .initialize(card)  
            .emit(&mut context, player, MessageDelivery.CONSTRAINED_ONCHAIN);  
    }  
}
```

## Reading from mapped PrivateSets```
use dep::aztec::note::note_getter_options::NoteGetterOptions;  
  
#[external("private")]  
fn get_player_cards(player: AztecAddress) -> BoundedVec<CardNote, MAX_NOTES_PER_PAGE> {  
    // Get all cards for this player  
    storage.card_collections  
        .at(player)  
        .get_notes(NoteGetterOptions::new())  
}  
  
#[external("private")]  
fn get_total_points(player: AztecAddress) -> u32 {  
    let options = NoteGetterOptions::new();  
    let notes = storage.card_collections.at(player).get_notes(options);  
  
    let mut total = 0;  
    for i in 0..notes.len() {  
        let card = notes.get(i);  
        total += card.points;  
    }  
    total  
}
```

## Reading from mapped PrivateMutable```
#[external("private")]  
fn get_active_card(player: AztecAddress) -> CardNote {  
    storage.active_cards.at(player).get_note()  
}
```

## Filtering notes in MapsFilter notes by their fields when reading from maps:

```
use dep::aztec::{note::note_getter_options::NoteGetterOptions, utils::comparison::Comparator};  
  
#[external("private")]  
fn find_strong_cards(player: AztecAddress, min_strength: u32) -> BoundedVec<CardNote, MAX_NOTES_PER_PAGE> {  
    let options = NoteGetterOptions::new()  
        .select(CardNote::properties().strength, Comparator.GTE, min_strength)  
        .set_limit(10);  
  
    storage.card_collections.at(player).get_notes(options)  
}
```

## Working with nested MapsNavigate nested map structures to organize data hierarchically:

```
use dep::aztec::messages::message_delivery::MessageDelivery;  
  
#[external("private")]  
fn add_card_to_game(  
    game_id: Field,  
    player: AztecAddress,  
    points: u32,  
    strength: u32  
) {  
    let card = CardNote::new(points, strength, player);  
  
    // Navigate nested maps: game_cards[game_id][player]  
    storage.game_cards  
        .at(game_id)  
        .at(player)  
        .insert(card)  
        .emit(&mut context, player, MessageDelivery.CONSTRAINED_ONCHAIN);  
}  
  
#[external("private")]  
fn get_game_cards(  
    game_id: Field,  
    player: AztecAddress  
) -> BoundedVec<CardNote, MAX_NOTES_PER_PAGE> {  
    storage.game_cards  
        .at(game_id)  
        .at(player)  
        .get_notes(NoteGetterOptions::new())  
}
```

## Further reading* [What the `#[note]` macro does](/developers/docs/aztec-nr/framework-description/functions/attributes#implementing-notes)
* [Note lifecycle and nullifiers](/developers/docs/foundational-topics/advanced/storage/indexed_merkle_tree)
* [Advanced note patterns](/developers/docs/aztec-nr/framework-description/advanced/how_to_retrieve_filter_notes)
* [Note portals for L1 communication](/developers/docs/aztec-nr/framework-description/how_to_communicate_cross_chain)
* [Macros reference](/developers/docs/aztec-nr/framework-description/macros)
* [Keys, including npk\_m\_hash](/developers/docs/foundational-topics/accounts/keys)

---


# Emitting Events

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/how_to_emit_event

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to emit events and logs from your Aztec contracts to communicate with offchain applications.

## Prerequisites* An Aztec contract project set up with `aztec-nr` dependency
* Understanding of private vs public functions in Aztec
* Basic knowledge of event handling in blockchain applications

## Emit private events## Emit encrypted eventsUse encrypted events to send private data to specific recipients:

```
// Import from aztec.nr  
use aztec::event::event_emission::emit_event_in_private;  
  
emit_event_in_private(  
    MyEvent { param1, param2, param3 },  
    &mut context,  
    recipient,  
    MessageDelivery.UNCONSTRAINED_ONCHAIN,  
);
```

note

Developer can choose whether to emit encrypted events or not. Emitting the events means that they will be posted to Ethereum, in blobs, and will inherit the availability guarantees of Ethereum. Developers may choose not to emit events and to share information with recipients offchain, or through alternative mechanisms that are to be developed (e.g. alternative, cheaper data availability solutions).

The `MessageDelivery` enum provides three modes:

* `MessageDelivery.CONSTRAINED_ONCHAIN` (value: 1): Constrained encryption, guarantees correct recipient
* `MessageDelivery.UNCONSTRAINED_ONCHAIN` (value: 2): Faster but trusts sender, may lose events if tagged incorrectly
* `MessageDelivery.UNCONSTRAINED_OFFCHAIN` (value: 3): Lowest cost, requires custom offchain infrastructure

## Event processingEvents are automatically discovered and decrypted by the wallet when contract functions are invoked.

## Emit public eventsEmit structured public events using the `emit` function:

```
// Import from aztec.nr  
use aztec::event::event_emission::emit_event_in_public;  
  
emit_event_in_public(  
    MyPublicEvent { field1: values[0], field2: values[1] },  
    &mut context,  
);
```

## Emit public logs## Emit unstructured dataEmit unstructured public logs using `emit_public_log`:

```
context.emit_public_log(my_value);  
context.emit_public_log([1, 2, 3]);  
context.emit_public_log("My message");
```

## Query public eventsQuery public events from offchain applications:

```
const fromBlock = await node.getBlockNumber();  
const logFilter = {  
  fromBlock,  
  toBlock: fromBlock + 1,  
};  
const publicLogs = (await node.getPublicLogs(logFilter)).logs;
```

## Consider costsEvent data is published to Ethereum as blobs, which incurs costs. Consider:

* Encrypted events are optional - use alternative communication methods if needed
* Future alternatives for data availability may become available
* Balance event utility with cost implications

---


# Aztec macros

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/macros

Version: Devnet (v3.0.0-devnet.20251212)

On this page

## All Aztec macrosIn addition to the function macros in Noir, Aztec also has its own macros for specific functions. An Aztec contract function can be annotated with more than 1 macro.
It is also worth mentioning Noir's `unconstrained` function type [here (Noir docs page)](https://noir-lang.org/docs/noir/concepts/unconstrained/).

* `#[aztec]` - Defines a contract, placed above `contract ContractName{}`
* `#[external("...")]` - Whether the function is to be callable from outside the contract. There are 3 types of external functions: `#[external("public")]`, `#[external("private")]` or `#[external("utility")]` - The type of external defines whether the function is to be executed from a public, private or utility context (see Further Reading)
* `#[initializer]` - If one or more functions are marked as an initializer, then one of them must be called before any non-initializer functions
* `#[noinitcheck]` - The function is able to be called before an initializer (if one exists)
* `#[view]` - Makes calls to the function static
* `#[only_self]` - Available only for `external` functions - any external caller except the current contract is rejected.
* `#[internal]` - Function can only be called from within the contract and the call itself is inlined (e.g. akin to EVM's JUMP and not EVM's CALL)
* `#[note]` - Creates a custom note
* `#[storage]` - Defines contract storage

## Further reading[How do Aztec macros work?](/developers/docs/aztec-nr/framework-description/functions/function_transforms)

---


# Global Variables

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/globals

Version: Devnet (v3.0.0-devnet.20251212)

On this page

For developers coming from solidity, this concept will be similar to how the global `block` variable exposes a series of block values. The idea is the same in Aztec. Developers can access a namespace of values made available in each function.

`Aztec` has two execution environments, Private and Public. Each execution environment contains a different global variables object.

## Private Global Variablestx-context

```
#[derive(Deserialize, Eq, Serialize)]  
pub struct TxContext {  
    pub chain_id: Field,  
    pub version: Field,  
    pub gas_settings: GasSettings,  
}
```

> [Source code: noir-projects/noir-protocol-circuits/crates/types/src/abis/transaction/tx\_context.nr#L8-L15](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-protocol-circuits/crates/types/src/abis/transaction/tx_context.nr#L8-L15)

The private global variables are equal to the transaction context and contain:

## Chain IdThe chain id differs depending on which Aztec instance you are on ( NOT the Ethereum hardfork that the rollup is settling to ). On original deployment of the network, this value will be 1.

```
context.chain_id();
```

## VersionThe version number indicates which Aztec hardfork you are on. The Genesis block of the network will have the version number 1.

```
context.version();
```

## Gas SettingsThe gas limits set by the user for the transaction, the max fee per gas, and the inclusion fee.

## Public Global Variablesglobal-variables

```
#[derive(Deserialize, Eq, Serialize)]  
pub struct GlobalVariables {  
    pub chain_id: Field,  
    pub version: Field,  
    pub block_number: u32,  
    pub slot_number: Field,  
    pub timestamp: u64,  
    pub coinbase: EthAddress,  
    pub fee_recipient: AztecAddress,  
    pub gas_fees: GasFees,  
}
```

> [Source code: noir-projects/noir-protocol-circuits/crates/types/src/abis/global\_variables.nr#L7-L19](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-protocol-circuits/crates/types/src/abis/global_variables.nr#L7-L19)

The public global variables contain the values present in the `private global variables` described above, with the addition of:

## TimestampThe timestamp is the unix timestamp in which the block has been executed. The value is provided by the block's proposer (therefore can have variance). This value will always increase.

```
context.timestamp();
```

## Block NumberThe block number is a sequential identifier that labels each individual block of the network. This value will be the block number of the block the accessing transaction is included in.
The block number of the genesis block will be 1, with the number increasing by 1 for every block after.

```
context.block_number();
```

*Why do the available global variables differ per execution environment?*

The global variables are constrained by the proving environment. In the case of public functions, they are executed on a sequencer that will know the timestamp and number of the next block ( as they are the block producer ).
In the case of private functions, we cannot be sure which block our transaction will be included in, hence we can not guarantee values for the timestamp or block number.

---


# Aztec<>Ethereum Messaging

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/ethereum-aztec-messaging

Version: Devnet (v3.0.0-devnet.20251212)

On this page

In Aztec, what we call *portals* are the key element in facilitating communication between L1 and L2. While typical L2 solutions rely on synchronous communication with L1, Aztec's privacy-first nature means this is not possible. You can learn more about why in the previous section.

Traditional L1 <-> L2 communication might involve direct calls between L2 and L1 contracts. However, in Aztec, due to the privacy components and the way transactions are processed (kernel proofs built on historical data), direct calls between L1 and L2 would not be possible if we want to maintain privacy.

Portals are the solution to this problem, acting as bridges for communication between the two layers. These portals can transmit messages from public functions in L1 to private functions in L2 and vice versa, thus enabling messaging while maintaining privacy.

This page covers:

* How portals enable privacy communication between L1 and L2
* How messages are sent, received, and processed
* Message Boxes and how they work
* How and why linking of contracts between L1 and L2 occurs

## ObjectiveThe goal is to set up a minimal-complexity mechanism, that will allow a base-layer (L1) and the Aztec Network (L2) to communicate arbitrary messages such that:

* L2 functions can `call` L1 functions.
* L1 functions can `call` L2 functions.
* The rollup-block size have a limited impact by the messages and their size.

## High Level OverviewThis document will contain communication abstractions that we use to support interaction between *private* functions, *public* functions and Layer 1 portal contracts.

Fundamental restrictions for Aztec:

* L1 and L2 have very different execution environments, stuff that is cheap on L1 is most often expensive on L2 and vice versa. As an example, `keccak256` is cheap on L1, but very expensive on L2.
* *Private* function calls are fully "prepared" and proven by the user, which provides the kernel proof along with commitments and nullifiers to the sequencer.
* *Public* functions altering public state (updatable storage) must be executed at the current "head" of the chain, which only the sequencer can ensure, so these must be executed separately to the *private* functions.
* *Private* and *public* functions within Aztec are therefore ordered such that first *private* functions are executed, and then *public*. For a more detailed description of why, see above.
* Messages are consumables, and can only be consumed by the recipient. See [Message Boxes](#message-boxes) for more information.

With the aforementioned restrictions taken into account, cross-chain messages can be operated in a similar manner to when *public* functions must transmit information to *private* functions. In such a scenario, a "message" is created and conveyed to the recipient for future use. It is worth noting that any call made between different domains (*private, public, cross-chain*) is unilateral in nature. In other words, the caller is unaware of the outcome of the initiated call until told when some later rollup is executed (if at all). This can be regarded as message passing, providing us with a consistent mental model across all domains, which is convenient.

As an illustration, suppose a private function adds a cross-chain call. In such a case, the private function would not have knowledge of the result of the cross-chain call within the same rollup (since it has yet to be executed).

Similarly to the ordering of private and public functions, we can also reap the benefits of intentionally ordering messages between L1 and L2. When a message is sent from L1 to L2, it has been "emitted" by an action in the past (an L1 interaction), allowing us to add it to the list of consumables at the "beginning" of the block execution. This practical approach means that a message could be consumed in the same block it is included. In a sophisticated setup, rollup nnn could send an L2 to L1 message that is then consumed on L1, and the response is added already in n+1n+1n+1. However, messages going from L2 to L1 will be added as they are emitted.

info

Because everything is unilateral and async, the application developer have to explicitly handle failure cases such that user can gracefully recover. Example where recovering is of utmost importance is token bridges, where it is very inconvenient if the locking of funds on one domain occur, but never the minting or unlocking on the other.

## Components## PortalA "portal" refers to the part of an application residing on L1, which is associated with a particular L2 address (the confidential part of the application). It could be a contract or even an EOA on L1.

## Message BoxesIn a logical sense, a Message Box functions as a one-way message passing mechanism with two ends, one residing on each side of the divide, i.e., one component on L1 and another on L2. Essentially, these boxes are utilized to transmit messages between L1 and L2 via the rollup contract. The boxes can be envisaged as multi-sets that enable the same message to be inserted numerous times, a feature that is necessary to accommodate scenarios where, for instance, "deposit 10 eth to A" is required multiple times. The diagram below provides a detailed illustration of how one can perceive a message box in a logical context.

![](/assets/ideal-img/com-abs-5.6eafe17.640.png)

* Here, a `sender` will insert a message into the `pending` set, the specific constraints of the actions depend on the implementation domain, but for now, say that anyone can insert into the pending set.
* At some point, a rollup will be executed, in this step messages are "moved" from pending on Domain A, to ready on Domain B. Note that consuming the message is "pulling & deleting" (or nullifying). The action is atomic, so a message that is consumed from the pending set MUST be added to the ready set, or the state transition should fail. A further constraint on moving messages along the way, is that only messages where the `sender` and `recipient` pair exists in a leaf in the contracts tree are allowed!
* When the message has been added to the ready set, the `recipient` can consume the message as part of a function call.

A difference when compared to other cross-chain setups, is that Aztec is "pulling" messages, and that the message doesn't need to be calldata for a function call. For other rollups, execution is happening FROM the "message bridge", which then calls the L1 contract. For Aztec, you call the L1 contract, and it should then consume messages from the message box.

Why? *Privacy*! When pushing, we would be needing full `calldata`. Which for functions with private inputs is not really something we want as that calldata for L1 -> L2 transactions are committed to on L1, e.g., publicly sharing the inputs to a private function.

By instead pulling, we can have the "message" be something that is derived from the arguments instead. This way, a private function to perform second half of a deposit, leaks the "value" deposited and "who" made the deposit (as this is done on L1), but the new owner can be hidden on L2.

To support messages in both directions we require two of these message boxes (one in each direction). However, due to the limitations of each domain, the message box for sending messages into the rollup and sending messages out are not fully symmetrical. In reality, the setup looks closer to the following:

![](/assets/ideal-img/com-abs-6.046847f.640.png)

info

The L2 -> L1 pending messages set only exist logically, as it is practically unnecessary. For anything to happen to the L2 state (e.g., update the pending messages), the state will be updated on L1, meaning that we could just as well insert the messages directly into the ready set.

## Rollup ContractThe rollup contract has a few very important responsibilities. The contract must keep track of the *L2 rollup state root*, perform *state transitions* and ensure that the data is available for anyone else to synchronize to the current state.

To ensure that *state transitions* are performed correctly, the contract will derive public inputs for the **rollup circuit** based on the input data, and then use a *verifier* contract to validate that inputs correctly transition the current state to the next. All data needed for the public inputs to the circuit must be from the rollup block, ensuring that the block is available. For a valid proof, the *rollup state root* is updated and it will emit an *event* to make it easy for anyone to find the data.

As part of *state transitions* where cross-chain messages are included, the contract must "move" messages along the way, e.g., from "pending" to "ready".

## Kernel CircuitFor L2 to L1 messages, the public inputs of a user-proof will contain a dynamic array of messages to be added, of size at most `MAX_MESSAGESTACK_DEPTH`, limited to ensure it is not impossible to include the transaction. The circuit must ensure, that all messages have a `sender/recipient` pair, and that those pairs exist in the contracts tree and that the `sender` is the L2 contract that actually emitted the message.
For consuming L1 to L2 messages the circuit must create proper nullifiers.

## Rollup CircuitThe rollup circuit must ensure that, provided two states SSS and S′S'S′ and the rollup block BBB, applying BBB to SSS using the transition function must give us S′S'S′, e.g., T(S,B)↦S′T(S, B) \mapsto S'T(S,B)↦S′. If this is not the case, the constraints are not satisfied.

For the sake of cross-chain messages, this means inserting and nullifying L1 →\rightarrow→ L2 in the trees, and publish L2 →\rightarrow→ L1 messages on chain. These messages should only be inserted if the `sender` and `recipient` match an entry in the contracts leaf (as checked by the kernel).

## MessagesWhile a message could theoretically be arbitrarily long, we want to limit the cost of the insertion on L1 as much as possible. Therefore, we allow the users to send 32 bytes of "content" between L1 and L2. If 32 suffices, no packing required. If the 32 is too "small" for the message directly, the sender should simply pass along a `sha256(content)` instead of the content directly (note that this hash should fit in a field element which is ~254 bits. More info on this below). The content can then either be emitted as an event on L2 or kept by the sender, who should then be the only entity that can "unpack" the message.
In this manner, there is some way to "unpack" the content on the receiving domain.

The message that is passed along, require the `sender/recipient` pair to be communicated as well (we need to know who should receive the message and be able to check). By having the pending messages be a contract on L1, we can ensure that the `sender = msg.sender` and let only `content` and `recipient` be provided by the caller. Summing up, we can use the structs seen below, and only store the commitment (`sha256(LxToLyMsg)`) on chain or in the trees, this way, we need only update a single storage slot per message.

```
struct L1Actor {  
	address: actor,  
	uint256: chainId,  
}  
  
struct L2Actor {  
	bytes32: actor,  
	uint256: version,  
}  
  
struct L1ToL2Msg {  
	L1Actor: sender,  
	L2Actor: recipient,  
	bytes32: content,  
	bytes32: secretHash,  
}  
  
struct L2ToL1Msg {  
	L2Actor: sender,  
	L1Actor: recipient,  
	bytes32: content,  
}
```

info

The `bytes32` elements for `content` and `secretHash` hold values that must fit in a field element (~ 254 bits).

info

The nullifier computation should include the index of the message in the message tree to ensure that it is possible to send duplicate messages (e.g., 2 x deposit of 500 dai to the same account).

To make it possible to hide when a specific message is consumed, the `L1ToL2Msg` is extended with a `secretHash` field, where the `secretPreimage` is used as part of the nullifier computation. This way, it is not possible for someone just seeing the `L1ToL2Msg` on L1 to know when it is consumed on L2.

## Combined ArchitectureThe following diagram shows the overall architecture, combining the earlier sections.

![](/assets/ideal-img/com-abs-7.e7283b1.640.png)

---


# Data Structures

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/ethereum-aztec-messaging/data_structures

Version: Devnet (v3.0.0-devnet.20251212)

On this page

The `DataStructures` are structs that we are using throughout the message infrastructure and registry.

**Links**: [Implementation (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/core/libraries/DataStructures.sol).

## `L1Actor`An entity on L1, specifying the address and the chainId for the entity. Used when specifying sender/recipient with an entity that is on L1.

l1\_actor

```
/**  
 * @notice Actor on L1.  
 * @param actor - The address of the actor  
 * @param chainId - The chainId of the actor  
 */  
struct L1Actor {  
  address actor;  
  uint256 chainId;  
}
```

> [Source code: l1-contracts/src/core/libraries/DataStructures.sol#L11-L22](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/core/libraries/DataStructures.sol#L11-L22)

| Name | Type | Description |
| --- | --- | --- |
| `actor` | `address` | The L1 address of the actor |
| `chainId` | `uint256` | The chainId of the actor. Defines the blockchain that the actor lives on. |

## `L2Actor`An entity on L2, specifying the address and the version for the entity. Used when specifying sender/recipient with an entity that is on L2.

l2\_actor

```
/**  
 * @notice Actor on L2.  
 * @param actor - The aztec address of the actor  
 * @param version - Ahe Aztec instance the actor is on  
 */  
struct L2Actor {  
  bytes32 actor;  
  uint256 version;  
}
```

> [Source code: l1-contracts/src/core/libraries/DataStructures.sol#L24-L35](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/core/libraries/DataStructures.sol#L24-L35)

| Name | Type | Description |
| --- | --- | --- |
| `actor` | `bytes32` | The aztec address of the actor. |
| `version` | `uint256` | The version of Aztec that the actor lives on. |

## `L1ToL2Message`A message that is sent from L1 to L2.

l1\_to\_l2\_msg

```
/**  
 * @notice Struct containing a message from L1 to L2  
 * @param sender - The sender of the message  
 * @param recipient - The recipient of the message  
 * @param content - The content of the message (application specific) padded to bytes32 or hashed if larger.  
 * @param secretHash - The secret hash of the message (make it possible to hide when a specific message is consumed on  
 * L2).  
 * @param index - Global leaf index on the L1 to L2 messages tree.  
 */  
struct L1ToL2Msg {  
  L1Actor sender;  
  L2Actor recipient;  
  bytes32 content;  
  bytes32 secretHash;  
  uint256 index;  
}
```

> [Source code: l1-contracts/src/core/libraries/DataStructures.sol#L37-L55](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/core/libraries/DataStructures.sol#L37-L55)

| Name | Type | Description |
| --- | --- | --- |
| `sender` | `L1Actor` | The actor on L1 that is sending the message. |
| `recipient` | `L2Actor` | The actor on L2 that is to receive the message. |
| `content` | `field (~254 bits)` | The field element containing the content to be sent to L2. |
| `secretHash` | `field (~254 bits)` | The hash of a secret pre-image that must be known to consume the message on L2. Use [`computeSecretHash` (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/yarn-project/aztec.js/src/utils/secrets.ts) to compute it from a secret. |

## `L2ToL1Message`A message that is sent from L2 to L1.

l2\_to\_l1\_msg

```
/**  
 * @notice Struct containing a message from L2 to L1  
 * @param sender - The sender of the message  
 * @param recipient - The recipient of the message  
 * @param content - The content of the message (application specific) padded to bytes32 or hashed if larger.  
 * @dev Not to be confused with L2ToL1Message in Noir circuits  
 */  
struct L2ToL1Msg {  
  DataStructures.L2Actor sender;  
  DataStructures.L1Actor recipient;  
  bytes32 content;  
}
```

> [Source code: l1-contracts/src/core/libraries/DataStructures.sol#L57-L70](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/core/libraries/DataStructures.sol#L57-L70)

| Name | Type | Description |
| --- | --- | --- |
| `sender` | `L2Actor` | The actor on L2 that is sending the message. |
| `recipient` | `L1Actor` | The actor on L1 that is to receive the message. |
| `content` | `field (~254 bits)` | The field element containing the content to be consumed by the portal on L1. |

---


# Inbox

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/ethereum-aztec-messaging/inbox

Version: Devnet (v3.0.0-devnet.20251212)

On this page

The `Inbox` is a contract deployed on L1 that handles message passing from L1 to the rollup (L2)

**Links**: [Interface (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/core/interfaces/messagebridge/IInbox.sol), [Implementation (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/core/messagebridge/Inbox.sol).

## `sendL2Message()`Sends a message from L1 to L2.

send\_l1\_to\_l2\_message

```
/**  
 * @notice Inserts a new message into the Inbox  
 * @dev Emits `MessageSent` with data for easy access by the sequencer  
 * @param _recipient - The recipient of the message  
 * @param _content - The content of the message (application specific)  
 * @param _secretHash - The secret hash of the message (make it possible to hide when a specific message is consumed  
 * on L2)  
 * @return The key of the message in the set and its leaf index in the tree  
 */  
function sendL2Message(DataStructures.L2Actor memory _recipient, bytes32 _content, bytes32 _secretHash)  
  external  
  returns (bytes32, uint256);
```

> [Source code: l1-contracts/src/core/interfaces/messagebridge/IInbox.sol#L35-L48](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/core/interfaces/messagebridge/IInbox.sol#L35-L48)

| Name | Type | Description |
| --- | --- | --- |
| Recipient | `L2Actor` | The recipient of the message. This **MUST** match the rollup version and an Aztec contract that is **attached** to the contract making this call. If the recipient is not attached to the caller, the message cannot be consumed by it. |
| Content | `field` (~254 bits) | The content of the message. This is the data that will be passed to the recipient. The content is limited to be a single field for rollup purposes. If the content is small enough it can just be passed along, otherwise it should be hashed and the hash passed along (you can use our [`Hash` (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/core/libraries/Hash.sol) utilities with `sha256ToField` functions) |
| Secret Hash | `field` (~254 bits) | A hash of a secret that is used when consuming the message on L2. Keep this preimage a secret to make the consumption private. To consume the message the caller must know the pre-image (the value that was hashed) - so make sure your app keeps track of the pre-images! Use [`computeSecretHash` (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/yarn-project/aztec.js/src/utils/secrets.ts) to compute it from a secret. |
| ReturnValue | `bytes32` | The message hash, used as an identifier |

## Edge cases* Will revert with `Inbox__ActorTooLarge(bytes32 actor)` if the recipient is larger than the field size (~254 bits).
* Will revert with `Inbox__ContentTooLarge(bytes32 content)` if the content is larger than the field size (~254 bits).
* Will revert with `Inbox__SecretHashTooLarge(bytes32 secretHash)` if the secret hash is larger than the field size (~254 bits).

## `consume()`Allows the `Rollup` to consume multiple messages in a single transaction.

consume

```
/**  
 * @notice Consumes the current tree, and starts a new one if needed  
 * @dev Only callable by the rollup contract  
 * @dev In the first iteration we return empty tree root because first checkpoint's messages tree is always  
 * empty because there has to be a 1 checkpoint lag to prevent sequencer DOS attacks  
 *  
 * @param _toConsume - The checkpoint number to consume  
 *  
 * @return The root of the consumed tree  
 */  
function consume(uint256 _toConsume) external returns (bytes32);
```

> [Source code: l1-contracts/src/core/interfaces/messagebridge/IInbox.sol#L50-L62](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/core/interfaces/messagebridge/IInbox.sol#L50-L62)

| Name | Type | Description |
| --- | --- | --- |
| ReturnValue | `bytes32` | Root of the consumed tree. |

## Edge cases* Will revert with `Inbox__Unauthorized()` if `msg.sender != ROLLUP` (rollup contract is sometimes referred to as state transitioner in the docs).

---


# Outbox

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/ethereum-aztec-messaging/outbox

Version: Devnet (v3.0.0-devnet.20251212)

On this page

The `Outbox` is a contract deployed on L1 that handles message passing from the rollup and to L1.

**Links**: [Interface (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/core/interfaces/messagebridge/IOutbox.sol), [Implementation (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/core/messagebridge/Outbox.sol).

## `insert()`Inserts the root of a merkle tree containing all of the L2 to L1 messages in a checkpoint specified by checkpointNumber.

outbox\_insert

```
/**  
 * @notice Inserts the root of a merkle tree containing all of the L2 to L1 messages in  
 * a checkpoint specified by _checkpointNumber.  
 * @dev Only callable by the rollup contract  
 * @dev Emits `RootAdded` upon inserting the root successfully  
 * @param _checkpointNumber - The checkpoint number in which the L2 to L1 messages reside  
 * @param _root - The merkle root of the tree where all the L2 to L1 messages are leaves  
 */  
function insert(uint256 _checkpointNumber, bytes32 _root) external;
```

> [Source code: l1-contracts/src/core/interfaces/messagebridge/IOutbox.sol#L19-L29](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/core/interfaces/messagebridge/IOutbox.sol#L19-L29)

| Name | Type | Description |
| --- | --- | --- |
| `_checkpointNumber` | `uint256` | The checkpoint Number in which the L2 to L1 messages reside |
| `_root` | `bytes32` | The merkle root of the tree where all the L2 to L1 messages are leaves |
| `_minHeight` | `uint256` | The minimum height of the merkle tree that the root corresponds to |

## Edge cases* Will revert with `Outbox__Unauthorized()` if `msg.sender != ROLLUP_CONTRACT`.
* Will revert with `Errors.Outbox__RootAlreadySetAtCheckpoint(uint256 checkpointNumber)` if the root for the specific checkpoint has already been set.
* Will revert with `Errors.Outbox__InsertingInvalidRoot()` if the rollup is trying to insert bytes32(0) as the root.

## `consume()`Allows a recipient to consume a message from the `Outbox`.

outbox\_consume

```
/**  
 * @notice Consumes an entry from the Outbox  
 * @dev Only useable by portals / recipients of messages  
 * @dev Emits `MessageConsumed` when consuming messages  
 * @param _message - The L2 to L1 message  
 * @param _checkpointNumber - The checkpoint number specifying the checkpoint that contains the message we want to  
 * consume  
 * @param _leafIndex - The index inside the merkle tree where the message is located  
 * @param _path - The sibling path used to prove inclusion of the message, the _path length directly depends  
 * on the total amount of L2 to L1 messages in the checkpoint. i.e. the length of _path is equal to the depth of the  
 * L1 to L2 message tree.  
 */  
function consume(  
  DataStructures.L2ToL1Msg calldata _message,  
  uint256 _checkpointNumber,  
  uint256 _leafIndex,  
  bytes32[] calldata _path  
) external;
```

> [Source code: l1-contracts/src/core/interfaces/messagebridge/IOutbox.sol#L31-L50](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/core/interfaces/messagebridge/IOutbox.sol#L31-L50)

| Name | Type | Description |
| --- | --- | --- |
| `_message` | `L2ToL1Msg` | The L2 to L1 message we want to consume |
| `_checkpointNumber` | `uint256` | The checkpoint number specifying the checkpoint that contains the message we want to consume |
| `_leafIndex` | `uint256` | The index inside the merkle tree where the message is located |
| `_path` | `bytes32[]` | The sibling path used to prove inclusion of the message, the \_path length directly depends |

## Edge cases* Will revert with `Outbox__InvalidRecipient(address expected, address actual);` if `msg.sender != _message.recipient.actor`.
* Will revert with `Outbox__InvalidChainId()` if `block.chainid != _message.recipient.chainId`.
* Will revert with `Outbox__NothingToConsumeAtCheckpoint(uint256 checkpointNumber)` if the root for the checkpoint has not been set yet.
* Will revert with `Outbox__AlreadyNullified(uint256 checkpointNumber, uint256 leafIndex)` if the message at leafIndex for the checkpoint has already been consumed.
* Will revert with `Outbox__InvalidPathLength(uint256 expected, uint256 actual)` if the supplied height is less than the existing minimum height of the L2 to L1 message tree, or the supplied height is greater than the maximum (minimum height + log2(maximum messages)).
* Will revert with `MerkleLib__InvalidRoot(bytes32 expected, bytes32 actual, bytes32 leaf, uint256 leafIndex)` if unable to verify the message existence in the tree. It returns the message as a leaf, as well as the index of the leaf to expose more info about the error.

## `hasMessageBeenConsumedAtCheckpointAndIndex()`Checks to see if an index of the L2 to L1 message tree for a specific checkpoint has been consumed.

outbox\_has\_message\_been\_consumed\_at\_checkpoint\_and\_index

```
/**  
 * @notice Checks to see if an L2 to L1 message in a specific checkpoint has been consumed  
 * @dev - This function does not throw. Out-of-bounds access is considered valid, but will always return false  
 * @param _checkpointNumber - The checkpoint number specifying the checkpoint that contains the message we want to  
 * check  
 * @param _leafId - The unique id of the message leaf  
 */  
function hasMessageBeenConsumedAtCheckpoint(uint256 _checkpointNumber, uint256 _leafId) external view returns (bool);
```

> [Source code: l1-contracts/src/core/interfaces/messagebridge/IOutbox.sol#L52-L61](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/core/interfaces/messagebridge/IOutbox.sol#L52-L61)

| Name | Type | Description |
| --- | --- | --- |
| `_checkpointNumber` | `uint256` | The checkpoint number specifying the checkpoint that contains the index of the message we want to check |
| `_leafIndex` | `uint256` | The index of the message inside the merkle tree |

## Edge cases* This function does not throw. Out-of-bounds access is considered valid, but will always return false.

---


# Registry

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/ethereum-aztec-messaging/registry

Version: Devnet (v3.0.0-devnet.20251212)

On this page

The registry is a contract deployed on L1, that contains addresses for the `Rollup`. It also keeps track of the different versions that have been deployed and let you query prior deployments easily.

**Links**: [Interface (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/governance/interfaces/IRegistry.sol), [Implementation (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/governance/Registry.sol).

## `numberOfVersions()`Retrieves the number of versions that have been deployed.

registry\_number\_of\_versions

```
function numberOfVersions() external view returns (uint256);
```

> [Source code: l1-contracts/src/governance/interfaces/IRegistry.sol#L25-L27](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/governance/interfaces/IRegistry.sol#L25-L27)

| Name | Description |
| --- | --- |
| ReturnValue | The number of versions that have been deployed |

## `getCanonicalRollup()`Retrieves the current rollup contract.

registry\_get\_canonical\_rollup

```
function getCanonicalRollup() external view returns (IHaveVersion);
```

> [Source code: l1-contracts/src/governance/interfaces/IRegistry.sol#L17-L19](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/governance/interfaces/IRegistry.sol#L17-L19)

| Name | Description |
| --- | --- |
| ReturnValue | The current rollup |

## `getRollup(uint256 _version)`Retrieves the rollup contract for a specfic version.

registry\_get\_rollup

```
function getRollup(uint256 _chainId) external view returns (IHaveVersion);
```

> [Source code: l1-contracts/src/governance/interfaces/IRegistry.sol#L21-L23](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/src/governance/interfaces/IRegistry.sol#L21-L23)

| Name | Description |
| --- | --- |
| ReturnValue | The current rollup |

---


# Enabling Authentication Witnesses

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/how_to_use_authwit

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Authentication witnesses (authwit) allow other contracts to execute actions on behalf of your account. This guide shows you how to implement and use authwits in your Aztec smart contracts.

## Prerequisites* An Aztec contract project set up with `aztec-nr` dependency
* Understanding of private and public functions in Aztec
* Access to the `authwit` library in your contract

For conceptual background, see [Authentication Witnesses](/developers/docs/foundational-topics/advanced/authwit).

## Set up the authwit libraryAdd the `authwit` library to your `Nargo.toml` file:

```
[dependencies]  
aztec = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="noir-projects/smart-contracts/aztec" }
```

Import the authwit library in your contract:

```
use aztec::authwit::auth::compute_authwit_nullifier;
```

## Implement authwit in private functions## Validate authentication in a private functionCheck if the current call is authenticated using the `authorize_once` macro:

```
#[authorize_once("from", "authwit_nonce")]  
#[external("private")]  
fn execute_private_action(  
    from: AztecAddress,  
    to: AztecAddress,  
    value: u128,  
    authwit_nonce: Field,  
) {  
    storage.values.at(from).sub(from, value).deliver(encode_and_encrypt_note(&mut context, from));  
    storage.values.at(to).add(to, value).deliver(encode_and_encrypt_note(&mut context, to));  
}
```

This allows anyone with a valid authwit (created by `from`) to execute an action on its behalf.

## Set approval state from contractsEnable contracts to approve actions on their behalf by updating the public auth registry:

1. Compute the message hash using `compute_authwit_message_hash_from_call`
2. Set the authorization using `set_authorized`

This pattern is commonly used in bridge contracts (like the [uniswap example contract](https://github.com/AztecProtocol/aztec-packages/tree/next/noir-projects/noir-contracts/contracts/app/uniswap_contract)) where one contract needs to authorize another to perform actions on its behalf:

```
#[external("public")]  
#[only_self]  
fn _approve_and_execute_action(  
    target_contract: AztecAddress,  
    bridge_contract: AztecAddress,  
    value: u128,  
) {  
    // Since we will authorize and instantly execute the action, all in public, we can use the same nonce  
    // every interaction. In practice, the authwit should be squashed, so this is also cheap!  
    let authwit_nonce = 0xdeadbeef;  
  
    let selector = FunctionSelector::from_signature("execute_action((Field),u128,Field)");  
    let message_hash = compute_authwit_message_hash_from_call(  
        bridge_contract,  
        target_contract,  
        context.chain_id(),  
        context.version(),  
        selector,  
        [context.this_address().to_field(), value as Field, authwit_nonce],  
    );  
  
    // We need to make a call to update it.  
    set_authorized(&mut context, message_hash, true);  
  
    let this_address = storage.my_address.read();  
    // Execute the action!  
    OtherContract::at(bridge_contract)  
        .execute_external_action(this_address, value, this_address, authwit_nonce)  
        .call(&mut context)  
}
```

---


# Communicating Cross-Chain

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/how_to_communicate_cross_chain

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to implement cross-chain communication between Ethereum (L1) and Aztec (L2) contracts using portal contracts.

## Prerequisites* An Aztec contract project set up with `aztec-nr` dependency
* Understanding of Aztec L1/L2 architecture
* Access to Ethereum development environment for L1 contracts
* Deployed portal contract on L1 (see [token bridge tutorial](/developers/docs/tutorials/js_tutorials/token_bridge))

## Send messages from L1 to L2## Send a message from your L1 portal contractUse the `Inbox` contract to send messages from L1 to L2. Call `sendL2Message` with these parameters:

| Parameter | Type | Description |
| --- | --- | --- |
| `actor` | `L2Actor` | Your L2 contract address and rollup version |
| `contentHash` | `bytes32` | Hash of your message content (use `Hash.sha256ToField`) |
| `secretHash` | `bytes32` | Hash of a secret for message consumption |

In your Solidity contract:

```
import {IInbox} from "@aztec/core/interfaces/messagebridge/IInbox.sol";  
import {DataStructures} from "@aztec/core/libraries/DataStructures.sol";  
import {Hash} from "@aztec/core/libraries/crypto/Hash.sol";  
  
// ... initialize inbox, get rollupVersion from rollup contract ...  
  
DataStructures.L2Actor memory actor = DataStructures.L2Actor(l2ContractAddress, rollupVersion);  
  
// Hash your message content with a unique function signature  
bytes32 contentHash = Hash.sha256ToField(  
    abi.encodeWithSignature("your_action_name(uint256,address)", param1, param2)  
);  
  
// Send the message  
(bytes32 key, uint256 index) = inbox.sendL2Message(actor, contentHash, secretHash);
```

## Consume the message in your L2 contractTo consume a message coming from L1, use the `consume_l1_to_l2_message` function within the context:

* The `content_hash` must match the hash that was sent from L1
* The `secret` is the pre-image of the `secretHash` sent from L1
* The `sender` is the L1 portal contract address
* The `message_leaf_index` helps the RPC find the correct message
* If the content or secret doesn't match, the transaction will revert
* "Consuming" a message pushes a nullifier to prevent double-spending

```
#[external("public")]  
fn consume_message_from_l1(  
    secret: Field,  
    message_leaf_index: Field,  
    // your function parameters  
) {  
    // Recreate the same content hash as on L1  
    let content_hash = /* compute your content hash */;  
  
    // Consume the L1 message  
    context.consume_l1_to_l2_message(  
        content_hash,  
        secret,  
        portal_address, // Your L1 portal contract address  
        message_leaf_index  
    );  
  
    // Execute your contract logic here  
}
```

## Send messages from L2 to L1## Send a message from your L2 contractUse `message_portal` in your `context` to send messages from L2 to L1:

```
#[external("public")]  
fn send_message_to_l1(  
    // your function parameters  
) {  
    // Note: This can be called from both public and private functions  
    // Create your message content (must fit in a single Field)  
    let content = /* compute your content hash */;  
  
    // Send message to L1 portal  
    context.message_portal(portal_address, content);  
}
```

## Consume the message in your L1 portalUse the `Outbox` to consume L2 messages on L1:

```
import {IOutbox} from "@aztec/core/interfaces/messagebridge/IOutbox.sol";  
import {DataStructures} from "@aztec/core/libraries/DataStructures.sol";  
import {Hash} from "@aztec/core/libraries/crypto/Hash.sol";  
  
function consumeMessageFromL2(  
    // your parameters  
    uint256 _l2BlockNumber,  
    uint256 _leafIndex,  
    bytes32[] calldata _path  
) external {  
    // Recreate the message structure  
    DataStructures.L2ToL1Msg memory message = DataStructures.L2ToL1Msg({  
        sender: DataStructures.L2Actor(l2ContractAddress, rollupVersion),  
        recipient: DataStructures.L1Actor(address(this), block.chainid),  
        content: Hash.sha256ToField(  
            abi.encodeWithSignature(  
                "your_action_name(address,uint256,address)",  
                param1, param2, param3  
            )  
        )  
    });  
  
    // Consume the message  
    outbox.consume(message, _l2BlockNumber, _leafIndex, _path);  
  
    // Execute your L1 logic here  
}
```

info

You can get the witness for the l2 to l1 message as follows:

```
import { computeL2ToL1MembershipWitness } from "@aztec/stdlib/messaging";  
import { computeL2ToL1MessageHash } from "@aztec/stdlib/hash";  
  
// Compute the message hash  
const l2ToL1Message = computeL2ToL1MessageHash({  
  l2Sender: l2ContractAddress,  
  l1Recipient: EthAddress.fromString(portalAddress),  
  content: messageContent,  
  rollupVersion: new Fr(version),  
  chainId: new Fr(chainId),  
});  
  
const witness = await computeL2ToL1MembershipWitness(  
  node,  
  exitReceipt.blockNumber!,  
  l2ToL1Message  
);
```

## Best practices## Structure messages properlyUse function signatures to prevent message misinterpretation:

```
// ❌ Ambiguous format  
bytes memory message = abi.encode(_value, _contract, _recipient);  
  
// ✅ Clear function signature  
bytes memory message = abi.encodeWithSignature(  
  "execute_action(uint256,address,address)",  
  _value, _contract, _recipient  
);
```

## Use designated callersControl message execution order with designated callers:

```
bytes memory message = abi.encodeWithSignature(  
  "execute_action(uint256,address,address)",  
  _value, _recipient,  
  _withCaller ? msg.sender : address(0)  
);
```

## Example implementations* [Token Portal (L1)](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/l1-contracts/test/portals/TokenPortal.sol)
* [Token Bridge (L2)](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-contracts/contracts/app/token_bridge_contract/src/main.nr)

## Next stepsFollow the [cross-chain messaging tutorial](/developers/docs/tutorials/js_tutorials/token_bridge) for a complete implementation example.

---


# Contract Artifacts

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/contract_artifact

Version: Devnet (v3.0.0-devnet.20251212)

On this page

After compiling a contract you'll get a Contract Artifact file, that contains the data needed to interact with a specific contract, including its name, functions that can be executed, and the interface and code of those functions. Since private functions are not published in the Aztec network, you'll need this artifact file to be able to call private functions of contracts.

The artifact file can be used with `aztec.js` to instantiate contract objects and interact with them.

## Contract Artifact StructureThe structure of a contract artifact is as follows:

```
{  
  "name": "CardGame",  
  "functions": [  
    {  
      "name": "constructor",  
      "functionType": "private",  
      "isInternal": false,  
      "parameters": [],  
      "returnTypes": [],  
      "bytecode": "...",  
      "verificationKey": "..."  
    },  
    {  
      "name": "on_card_played",  
      "functionType": "public",  
      "isInternal": true,  
      "parameters": [  
        {  
          "name": "game",  
          "type": {  
            "kind": "integer",  
            "sign": "unsigned",  
            "width": 32  
          },  
          "visibility": "private"  
        },  
        {  
          "name": "player",  
          "type": {  
            "kind": "field"  
          },  
          "visibility": "private"  
        },  
        {  
          "name": "card_as_field",  
          "type": {  
            "kind": "field"  
          },  
          "visibility": "private"  
        }  
      ],  
      "returnTypes": [  
        ...  
      ],  
      "bytecode": "...",  
      "verificationKey": "..."  
    },  
   ...  
  ]  
}
```

## `name`It is a simple string that matches the name that the contract developer used for this contract in noir. It's used for logs and errors.

## `functions`A contract is a collection of several functions that can be called. Each function has the following properties:

## `function.name`A simple string that matches the name that the contract developer used for this function in noir. For logging and debugging purposes.

## `function.functionType`The function type can have one of the following values:

* Private: The function is ran and proved locally by the clients, and its bytecode not published to the network.
* Public: The function is ran and proved by the sequencer, and its bytecode is published to the network.
* Utility: The function is ran locally by the clients to generate digested information useful for the user. It cannot be called in a transaction.

## `function.isInternal`The is internal property is a boolean that indicates whether the function is internal to the contract and cannot be called from outside.

## `function.parameters`Each function can have multiple parameters that are arguments to execute the function. Parameters have a name, and type (like integers, strings, or complex types like arrays and structures).

## `function.returnTypes`The return types property defines the types of values that the function returns after execution.

## `function.bytecode`The bytecode is a string representing the compiled ACIR of the function, ready for execution on the network.

## `function.verificationKey`The verification key is an optional property that contains the verification key of the function. This key is used to verify the proof of the function execution.

## `debug` (Optional)Although not significant for non-developer users, it is worth mentioning that there is a debug section in the contract artifact which helps contract developers to debug and test their contracts. This section mainly contains debug symbols and file maps that link back to the original source code.

## Understanding Parameter and Return TypesTo make the most of the functions, it's essential to understand the types of parameters and return values. Here are some common types you might encounter:

* `field`: A basic type representing a field element in the finite field of the curve used in the Aztec protocol.
* `boolean`: A simple true/false value.
* `integer`: Represents whole numbers. It has attributes defining its sign (positive or negative) and width (the number of bits representing the integer).
* `array`: Represents a collection of elements, all of the same type. It has attributes defining its length and the type of elements it holds.
* `string`: Represents a sequence of characters with a specified length.
* `struct`: A complex type representing a structure with various fields, each having a specific type and name.

---


# Contract Upgrades

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/contract_upgrades

Version: Devnet (v3.0.0-devnet.20251212)

On this page

For familiarity we've used terminology like "deploying a contract instance of a contract class". When considering how it works with contract upgrades it helps to be more specific.

Each contract instance refers to a class id for its code. Upgrading a contract's implementation is achieved by updating its current class id to a new class id, whilst retaining the "original class id" for reasons explained below.

## Original class idA contract keeps track of the original contract class that it was instantiated with, which is the "original" class id. It is this original class that is used when calculating and verifying the contract's [address](/developers/docs/foundational-topics/contract_creation#instance-address).
This variable remains unchanged even if a contract is upgraded.

## Current class idWhen a contract is first deployed, its current class ID is set equal to its original class ID. The current class ID determines which code implementation the contract actually executes.

During an upgrade:

* The original class ID remains unchanged
* The current class ID is updated to refer to the new implementation
* All contract state/data is preserved

## How to upgradeContract upgrades in Aztec have to be initiated by the contract that wishes to be upgraded calling the `ContractInstanceRegistry`:

```
use dep::aztec::protocol_types::contract_class_id::ContractClassId;  
use contract_instance_registry::ContractInstanceRegistry;  
  
#[external("private")]  
fn update_to(new_class_id: ContractClassId) {  
    ContractInstanceRegistry::at(CONTRACT_INSTANCE_REGISTRY_CONTRACT_ADDRESS)  
        .update(new_class_id)  
        .enqueue(&mut context);  
}
```

The `update` function in the registry is a public function, so you can enqueue it from a private function like the example or call it from a public function directly.

note

Recall that `#[external("private")]` means calling this function preserves privacy, and it still CAN be called externally by anyone.
So the `update_to` function above allows anyone to update the contract that implements it. A more complete implementation should have a proper authorization systems to secure contracts from malicious upgrades.

Contract upgrades are implemented using a DelayedPublicMutable storage variable in the `ContractInstanceRegistry`, since the upgrade applies to both public and private functions.
This means that they have a delay before entering into effect. The default delay is `86400` seconds (one day) but can be configured by the contract:

```
use dep::aztec::protocol_types::contract_class_id::ContractClassId;  
use contract_instance_registry::ContractInstanceRegistry;  
  
#[external("private")]  
fn set_update_delay(new_delay: u64) {  
   ContractInstanceRegistry::at(CONTRACT_INSTANCE_REGISTRY_CONTRACT_ADDRESS)  
      .set_update_delay(new_delay)  
      .enqueue(&mut context);  
}
```

Where `new_delay` is denominated in seconds. However, take into account that changing the update delay also has as its delay that is the previous delay. So the first delay change will take `86400` seconds to take into effect.

info

The update delay cannot be set lower than `600` seconds

When sending a transaction, the expiration timestamp of your tx will be the timestamp of the current block number you're simulating with + the minimum of the update delays that you're interacting with.
If your tx interacts with a contract that can be upgraded in 1000 seconds and another one that can be upgraded in 10000 seconds, the expiration timestamp (include\_by\_timestamp property on the tx) will be current block timestamp + 1000.
Note that this can be even lower if there is an upgrade pending in one of the contracts you're interacting with.
If the contract you interacted with will upgrade in 100 seconds, the expiration timestamp of your tx will be current block timestamp + 99 seconds.
Other DelayedPublicMutable storage variables read in your tx might reduce this expiration timestamp further.

note

Only deployed contract instances can upgrade or change its upgrade delay currently. This restriction might be lifted in the future.

## Upgrade Process1. **Register New Implementation**

   * First, register the new contract class if it contains public functions
   * The new implementation must maintain state variable compatibility with the original contract
2. **Perform Upgrade**

   * Call the update function with the new contract class ID
   * The contract's original class ID remains unchanged
   * The current class ID is updated to the new implementation
   * All contract state and data are preserved
3. **Verify Upgrade**

   * After upgrade, the contract will execute functions from the new implementation
   * The contract's address remains the same since it's based on the original class ID
   * Existing state variables and their values are preserved

## How to interact with an upgraded contractThe PXE in the wallet stores the contract instances and classes in a local database. When a contract is updated, in order to interact with it we need to pass the new artifact to the PXE in the wallet, since the protocol doesn't publish artifacts.
Consider this contract as an example:

```
#[aztec]  
contract Updatable {  
...  
  
    #[external("private")]  
    fn update_to(new_class_id: ContractClassId) {  
        ContractInstanceRegistry::at(CONTRACT_INSTANCE_REGISTRY_CONTRACT_ADDRESS).update(new_class_id).enqueue(  
            &mut context,  
        );  
    }  
...
```

You'd upgrade it in aztec.js doing something similar to this:

```
const contract = await UpdatableContract.deploy(wallet, ...args)  
  .send()  
  .deployed();  
const updatedContractClassId = (  
  await getContractClassFromArtifact(UpdatedContractArtifact)  
).id;  
await contract.methods.update_to(updatedContractClassId).send().wait();
```

Now, when the update has happened, calling `at` with the new contract artifact will automatically update the contract instance in the wallet if it's outdated:

```
// 'at' will call wallet updateContract if outdated  
const updatedContract = await UpdatedContract.at(address, wallet);
```

If you try to call `at` with a different contract that is not the current version, it'll fail

```
// throws when trying to update the wallet instance to RandomContract  
// since the current one is UpdatedContract  
await RandomContract.at(address, wallet);
```

## Security Considerations1. **Access Control**

   * Implement proper access controls for upgrade functions
   * Consider customizing the upgrades delay for your needs using `set_update_delay`
2. **State Compatibility**

   * Ensure new implementation is compatible with existing state
   * Maintain the same storage layout to prevent data corruption
3. **Testing**

   * Test upgrades thoroughly in a development environment
   * Verify all existing functionality works with the new implementation

## Example```
contract Updatable {  
    #[external("private")]  
    fn update_to(new_class_id: ContractClassId) {  
        // TODO: Add access control  
        assert(context.msg_sender() == owner, "Unauthorized");  
  
        // Perform the upgrade  
        ContractInstanceRegistry::at(CONTRACT_INSTANCE_REGISTRY_CONTRACT_ADDRESS)  
            .update(new_class_id)  
            .enqueue(&mut context);  
    }  
  
    #[external("private")]  
    fn set_update_delay(new_delay: u64) {  
        // TODO: Add access control  
        ContractInstanceRegistry::at(CONTRACT_INSTANCE_REGISTRY_CONTRACT_ADDRESS)  
            .set_update_delay(new_delay)  
            .enqueue(&mut context);  
    }  
}
```

---


# Advanced Topics

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/advanced/how_to_retrieve_filter_notes

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to retrieve and filter notes from private storage using `NoteGetterOptions`.

## Prerequisites* Aztec contract with note storage
* Understanding of note structure and properties
* Familiarity with PropertySelector and Comparator

## Set up basic note retrieval## Step 1: Create default options```
let mut options = NoteGetterOptions::new();
```

This returns up to `MAX_NOTE_HASH_READ_REQUESTS_PER_CALL` notes without filtering.

## Step 2: Retrieve notes from storage```
let notes = storage.my_notes.at(owner).get_notes(options);
```

## Filter notes by properties## Step 1: Select notes with specific field values```
// Assuming MyNote has an 'owner' field  
let mut options = NoteGetterOptions::new();  
options = options.select(  
    MyNote::properties().owner,  
    Comparator.EQ,  
    owner  
);
```

## Step 2: Apply multiple selection criteria```
let mut options = NoteGetterOptions::new();  
options = options  
    .select(MyNote::properties().value, Comparator.EQ, value)  
    .select(MyNote::properties().owner, Comparator.EQ, owner);
```

tip

Chain multiple `select` calls to filter by multiple fields. Remember to call `get_notes(options)` after applying all your selection criteria to retrieve the filtered notes.

## Sort retrieved notes## Sort and paginate results```
let mut options = NoteGetterOptions::new();  
options = options  
    .select(MyNote::properties().owner, Comparator.EQ, owner)  
    .sort(MyNote::properties().value, SortOrder.DESC)  
    .set_limit(10)     // Max 10 notes  
    .set_offset(20);   // Skip first 20
```

## Apply custom filtersFilter Performance

Database `select` is more efficient than custom filters. Use custom filters only for complex logic.

## Create and use a custom filter```
fn filter_above_threshold(  
    notes: [Option<RetrievedNote<Note>>; MAX_NOTES],  
    min: Field,  
) -> [Option<RetrievedNote<Note>>; MAX_NOTES] {  
    let mut result = [Option::none(); MAX_NOTES];  
    let mut count = 0;  
  
    for note in notes {  
        if note.is_some() & (note.unwrap().note.value >= min) {  
            result[count] = note;  
            count += 1;  
        }  
    }  
    result  
}  
  
// Use the filter  
let options = NoteGetterOptions::with_filter(filter_above_threshold, min_value);
```

Note Limits

Maximum notes per call: `MAX_NOTE_HASH_READ_REQUESTS_PER_CALL` (currently 128)

Available Comparators

* `EQ`: Equal to
* `NEQ`: Not equal to
* `LT`: Less than
* `LTE`: Less than or equal
* `GT`: Greater than
* `GTE`: Greater than or equal

## Use comparators effectively## Available comparators```
// Equal to  
options.select(MyNote::properties().value, Comparator.EQ, target_value)  
  
// Greater than or equal  
options.select(MyNote::properties().value, Comparator.GTE, min_value)  
  
// Less than  
options.select(MyNote::properties().value, Comparator.LT, max_value)
```

## Call from TypeScript with comparator```
// Pass comparator from client  
contract.methods.read_notes(Comparator.GTE, 5).simulate({ from: defaultAddress })
```

## View notes without constraints```
use dep::aztec::note::note_viewer_options::NoteViewerOptions;  
  
#[external("utility")]  
unconstrained fn view_notes(comparator: u8, value: Field) -> auto {  
    let mut options = NoteViewerOptions::new();  
    options = options.select(MyNote::properties().value, comparator, value);  
    storage.my_notes.view_notes(options)  
}
```

Viewer vs Getter

* `NoteGetterOptions`: For constrained functions (private/public)
* `NoteViewerOptions`: For unconstrained viewing (utilities)

## Query notes with different status## Set status to include nullified notes```
let mut options = NoteGetterOptions::new();  
options.set_status(NoteStatus.ACTIVE_OR_NULLIFIED);
```

Note Status Options

* `NoteStatus.ACTIVE`: Only active (non-nullified) notes (default)
* `NoteStatus.ACTIVE_OR_NULLIFIED`: Both active and nullified notes

## Optimize note retrievalBest Practices

1. **Use select over filter** - Database-level filtering is more efficient
2. **Set limits early** - Reduce unnecessary note processing
3. **Sort before limiting** - Get the most relevant notes first
4. **Batch operations** - Retrieve all needed notes in one call

## Example: Optimized retrieval```
// Get highest value note for owner  
let mut options = NoteGetterOptions::new();  
options = options  
    .select(MyNote::properties().owner, Comparator.EQ, owner)  
    .sort(MyNote::properties().value, SortOrder.DESC)  
    .set_limit(1);  
  
let notes = storage.my_notes.at(owner).get_notes(options);  
assert(notes.len() > 0, "No notes found");  
let highest_note = notes.get(0);
```

## Next steps* Learn about [custom note implementations](/developers/docs/aztec-nr/framework-description/how_to_implement_custom_notes)
* Explore [note discovery mechanisms](/developers/docs/foundational-topics/advanced/storage/note_discovery)
* Understand [note lifecycle](/developers/docs/foundational-topics/advanced/storage/indexed_merkle_tree)

---


# Partial Notes

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/advanced/partial_notes

Version: Devnet (v3.0.0-devnet.20251212)

On this page

## What are Partial Notes?Partial notes are notes created with incomplete data, usually during private execution, which can be completed with additional information that becomes available later, usually during public execution.

Let's say, for example, we have a `UintNote`:

uint\_note\_def

```
#[derive(Deserialize, Eq, Serialize, Packable)]  
#[custom_note]  
pub struct UintNote {  
    /// The number stored in the note.  
    pub value: u128,  
}
```

> [Source code: noir-projects/aztec-nr/uint-note/src/uint\_note.nr#L27-L34](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-nightly.20251210/noir-projects/aztec-nr/uint-note/src/uint_note.nr#L27-L34)

The `UintNote` struct itself only contains the `value` field. Additional fields including `owner`, `randomness`, and `storage_slot` are passed as parameters during note hash computation.

When creating the note locally during private execution, the `owner` and `storage_slot` are known, but the `value` potentially is not (e.g., it depends on some onchain dynamic variable). First, a **partial note** can be created during private execution that commits to the `owner`, `randomness`, and `storage_slot`, and then the note is *"completed"* to create a full note by later adding the `value` field, usually during public execution.

![](/assets/ideal-img/partial-notes.a3c6195.640.png)

## Use CasesPartial notes are useful when a e.g., part of the note struct is a value that depends on dynamic, public onchain data that isn't available during private execution, such as:

* AMM swap prices
* Current gas prices
* Time-dependent interest accrual

## ImplementationAll notes in Aztec use the partial note format internally. This ensures that notes produce identical note hashes regardless of whether they were created as complete notes (with all fields known in private) or as partial notes (completed later in public). By having all notes follow the same two-phase hash commitment process, the protocol maintains consistency and allows notes created through different flows to behave identically.

## Note Structure ExampleThe `UintNote` struct contains only the `value` field:

uint\_note\_def

```
#[derive(Deserialize, Eq, Serialize, Packable)]  
#[custom_note]  
pub struct UintNote {  
    /// The number stored in the note.  
    pub value: u128,  
}
```

> [Source code: noir-projects/aztec-nr/uint-note/src/uint\_note.nr#L27-L34](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-nightly.20251210/noir-projects/aztec-nr/uint-note/src/uint_note.nr#L27-L34)

## Two-Phase Commitment Process**Phase 1: Partial Commitment (Private Execution)**

The private fields (`owner`, `randomness`, and `storage_slot`) are committed during local, private execution:

compute\_partial\_commitment

```
fn compute_partial_commitment(  
    owner: AztecAddress,  
    storage_slot: Field,  
    randomness: Field,  
) -> Field {  
    poseidon2_hash_with_separator(  
        [owner.to_field(), storage_slot, randomness],  
        GENERATOR_INDEX__NOTE_HASH,  
    )  
}
```

> [Source code: noir-projects/aztec-nr/uint-note/src/uint\_note.nr#L160-L171](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-nightly.20251210/noir-projects/aztec-nr/uint-note/src/uint_note.nr#L160-L171)

This creates a partial note commitment:

```
partial_commitment = H(owner, storage_slot, randomness)
```

**Phase 2: Note Completion (Public Execution)**

The note is completed by hashing the partial commitment with the public value:

compute\_complete\_note\_hash

```
fn compute_complete_note_hash(self, value: u128) -> Field {  
    // Here we finalize the note hash by including the (public) value into the partial note commitment. Note that we  
    // use the same generator index as we used for the first round of poseidon - this is not an issue.  
    poseidon2_hash_with_separator(  
        [self.commitment, value.to_field()],  
        GENERATOR_INDEX__NOTE_HASH,  
    )  
}
```

> [Source code: noir-projects/aztec-nr/uint-note/src/uint\_note.nr#L284-L293](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-nightly.20251210/noir-projects/aztec-nr/uint-note/src/uint_note.nr#L284-L293)

The resulting structure is a nested commitment:

```
note_hash = H(H(owner, storage_slot, randomness), value)  
          = H(partial_commitment, value)
```

## Universal Note FormatAll notes in Aztec use the partial note format internally, even when all data is known during private execution. This ensures consistent note hash computation regardless of how the note was created.

When a note is created with all fields known (including `owner`, `storage_slot`, `randomness`, and `value`):

1. A partial commitment is computed from the private fields (`owner`, `storage_slot`, `randomness`)
2. The partial commitment is immediately completed with the `value` field

compute\_note\_hash

```
fn compute_note_hash(  
    self,  
    owner: AztecAddress,  
    storage_slot: Field,  
    randomness: Field,  
) -> Field {  
    // Partial notes can be implemented by having the note hash be either the result of multiscalar multiplication  
    // (MSM), or two rounds of poseidon. MSM results in more constraints and is only required when multiple variants  
    // of partial notes are supported. Because UintNote has just one variant (where the value is public), we use  
    // poseidon instead.  
  
    // We must compute the same note hash as would be produced by a partial note created and completed with the same  
    // values, so that notes all behave the same way regardless of how they were created. To achieve this, we  
    // perform both steps of the partial note computation.  
  
    // First we create the partial note from a commitment to the private content (including storage slot).  
    let partial_note = PartialUintNote {  
        commitment: compute_partial_commitment(owner, storage_slot, randomness),  
    };  
  
    // Then compute the completion note hash. In a real partial note this step would be performed in public.  
    partial_note.compute_complete_note_hash(self.value)  
}
```

> [Source code: noir-projects/aztec-nr/uint-note/src/uint\_note.nr#L37-L61](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-nightly.20251210/noir-projects/aztec-nr/uint-note/src/uint_note.nr#L37-L61)

This two-step process ensures that notes with identical field values produce identical note hashes, regardless of whether they were created as partial notes or complete notes.

![](/assets/ideal-img/shrek.a645f50.640.jpeg)

## Partial Notes in PracticeTo understand how to use partial notes in practice, [this AMM contract](https://github.com/AztecProtocol/aztec-packages/tree/next/noir-projects/noir-contracts/contracts/app/amm_contract) uses partial notes to initiate and complete the swap of `token1` to `token2`. Since the exchange rate is onchain, it cannot be known ahead of time while executing in private so a full note cannot be created. Instead, a partial note is created for the `owner` swapping the tokens. This partial note is then completed during public execution once the exchange rate can be read.

---


# Profiling and Optimizing Contracts

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/advanced/how_to_profile_transactions

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to profile your Aztec transactions to identify bottlenecks and optimize gas usage.

## Prerequisites* `aztec` command installed ([see installation](/developers/getting_started_on_local_network))
* `aztec-wallet` installed
* Aztec contract deployed and ready to test
* Basic understanding of proving and gate counts

## Profile with aztec-wallet## Step 1: Import test accounts```
aztec-wallet import-test-accounts
```

## Step 2: Deploy your contract```
aztec-wallet deploy MyContractArtifact \  
  --from accounts:test0 \  
  --args <constructor_args> \  
  -a mycontract
```

## Step 3: Set up initial state```
aztec-wallet send setup_state \  
  -ca mycontract \  
  --args <setup_args> \  
  -f test0
```

## Step 4: Profile a transactionInstead of `send`, use `profile` with the same parameters:

```
aztec-wallet profile private_function \  
  -ca mycontract \  
  --args <function_args> \  
  -f accounts:test0
```

## Step 5: Analyze the output```
Gate count per circuit:  
   SchnorrAccount:entrypoint                          Gates: 21,724     Acc: 21,724  
   private_kernel_init                                Gates: 45,351     Acc: 67,075  
   MyContract:private_function                        Gates: 31,559     Acc: 98,634  
   private_kernel_inner                               Gates: 78,452     Acc: 177,086  
   private_kernel_reset                               Gates: 91,444     Acc: 268,530  
   private_kernel_tail                                Gates: 31,201     Acc: 299,731  
  
Total gates: 299,731
```

The output shows:

* Gate count per circuit component
* Accumulated gate count
* Total gates for the entire transaction

## Profile with aztec.jsProfile Modes

* `gates`: Shows gate counts per circuit
* `execution-steps`: Detailed execution trace
* `full`: Complete profiling information

## Step 1: Profile a transaction```
const result = await contract.methods  
  .my_function(args)  
  .profile({  
    from: address,  
    profileMode: 'gates',  
    skipProofGeneration: false  
  });  
  
console.log('Gate count:', result.gateCount);
```

## Step 2: Profile deployment```
const deploy = await Contract.deploy(args).profile({ from: address, profileMode: 'full' });
```

Experimental

Flamegraph generation is experimental and may not be available in all versions.

## Generate flamegraphs (if available)")

## Generate and view```
# Compile first  
aztec compile  
  
# Generate flamegraph  
aztec flamegraph target/contract.json function_name  
  
# Serve locally  
SERVE=1 aztec flamegraph target/contract.json function_name
```

Reading Flamegraphs

* **Width** = Time in operation
* **Height** = Call depth
* **Wide sections** = Optimization targets

## Common optimizationsKey Metrics

* **Gate count**: Circuit complexity
* **Kernel overhead**: Per-function cost
* **Storage access**: Read/write operations

Optimization Pattern

Batch operations to reduce kernel circuit overhead.

```
// ❌ Multiple kernel invocations  
for i in 0..3 {  
    transfer_single(amounts[i], recipients[i]);  
}  
  
// ✅ Single kernel invocation  
for i in 0..3 {  
    let note = Note::new(amounts[i], recipients[i]);  
    storage.notes.at(recipients[i]).insert(note);  
}
```

Storage Optimization

Group storage reads to reduce overhead.

```
// Read once, use multiple times  
let values = [storage.v1.get(), storage.v2.get(), storage.v3.get()];  
for v in values {  
    assert(v > 0);  
}
```

## Minimize note operationsNote Aggregation

Combine multiple small notes into fewer larger ones to reduce proving overhead.

```
// ❌ Many small notes = high overhead  
for value in values {  
    storage.notes.insert(Note::new(value, owner));  
}  
  
// ✅ Single aggregated note = lower overhead  
let total = values.reduce(|a, b| a + b);  
storage.notes.insert(Note::new(total, owner));
```

## Profile different scenarios## Profile with different inputs```
# Small values  
aztec-wallet profile function -ca mycontract --args 10 -f test0  
  
# Large values  
aztec-wallet profile function -ca mycontract --args 1000000 -f test0
```

## Profile execution modes```
// Profile gates only  
await contract.methods.function().profile({ profileMode: 'gates' });  
  
// Profile execution steps  
await contract.methods.function().profile({ profileMode: 'execution-steps' });  
  
// Full profile  
await contract.methods.function().profile({ profileMode: 'full' });
```

## Skip proof generation for faster iteration```
await contract.methods.function().profile({  
  profileMode: 'gates',  
  skipProofGeneration: true  // Faster but less accurate  
});
```

## Interpret profiling results## Gate count guidelines* **< 50,000 gates**: Excellent performance
* **50,000 - 200,000 gates**: Acceptable for most use cases
* **200,000 - 500,000 gates**: May cause delays, consider optimizing
* **> 500,000 gates**: Requires optimization for production

## Common optimization targets1. **private\_kernel\_inner** - Reduce nested function calls
2. **private\_kernel\_reset** - Minimize note nullifications
3. **Contract functions** - Optimize computation logic
4. **private\_kernel\_tail** - Reduce public function calls

## Best practices## Development workflow1. **Profile early** - Establish baseline metrics
2. **Profile often** - Check impact of changes
3. **Profile realistically** - Use production-like data
4. **Document findings** - Track optimization progress

## Optimization priorities1. **User-facing functions** - Optimize most-used features first
2. **Critical paths** - Focus on transaction bottlenecks
3. **Batch operations** - Combine related operations
4. **Cache calculations** - Store reusable results

## Next steps* Learn about [gas optimization techniques](/developers/docs/foundational-topics/transactions)
* Review [benchmarking best practices](/developers/docs/aztec-nr/how_to_test_contracts)

---


# Writing Efficient Contracts

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/advanced/writing_efficient_contracts

Version: Devnet (v3.0.0-devnet.20251212)

On this page

## Writing functionsOn Ethereum L1, all data is public and all execution is completely reproducible. The Aztec L2 takes on the challenge of execution of private functions on private data. This is done client side, along with the generation of corresponding proofs, so that the network can verify the proofs and append any encrypted data/nullifiers (privacy preserving state update).

This highlights a key difference with how public vs private functions are written.

Writing efficiently

* **Public functions** can be written intuitively - optimising for execution/gas as one would for EVM L2s
* **Private functions** are optimized differently, as they are compiled to a circuit to be proven locally (see [Thinking in Circuits](https://noir-lang.org/docs/explainers/explainer-writing-noir))

## Assessing efficiencyOn Aztec (like other L2s) there are several costs/limit to consider...

* L1 costs - execution, blobs, events
* L2 costs - public execution, data, logs
* Local limits - proof generation time, execution

## Local Proof generationSince proof generation is a significant local burden, being mindful of the gate-count of private functions is important. The gate-count is a proportionate indicator of the memory and time required to prove locally, so should not be ignored.

## Noir for circuitsAn explanation of efficient use of Noir for circuits should be considered for each subsection under [writing efficient Noir](https://noir-lang.org/docs/explainers/explainer-writing-noir#writing-efficient-noir-for-performant-products) to avoid hitting local limits. The general theme is to use language features that favour the underlying primitives and representation of a circuit from code.

A couple of examples:

* Since the underlying cryptography uses an equation made of additions and multiplications, these are more efficient (wrt gate count) in Noir than say bit-shifting.
* Unconstrained functions by definition do not constrain their operations/output, so do not contribute to gate count. Using them carefully can bring in some savings, but the results must then be constrained so that proofs are meaningful for your application.

Tradeoffs and caveats

Each optimisation technique has its own tradeoffs and caveats so should be carefully considered with the full details in the linked [section](https://noir-lang.org/docs/explainers/explainer-writing-noir#writing-efficient-noir-for-performant-products).

## Overhead of nested Private CallsWhen private functions are called, the overhead of a "kernel circuit" is added each time, so be mindful of calling/nesting too many private functions. This may influence the design towards larger private functions rather than conventionally atomic functions.

## Profiling using FlameGraphMeasuring the gate count across a private function is explained in the [profiling guide](/developers/docs/aztec-nr/framework-description/advanced/how_to_profile_transactions).

## L2 Data costsOf the L2 costs, the public/private data being updated is most significant. As L2 functions create notes, nullifiers, encrypted logs, all of this get posted into blobs on ethereum and will be quite expensive

## L1 LimitsWhile most zk rollups don't leverage the zero-knowledge property like Aztec, they do leverage the succinctness property.
That is, what is stored in an L1 contract is simply a hash.

For data availability, blobs are utilized since data storage is often cheaper here than in contracts. Like other L2s such costs are factored into the L2 fee mechanisms. These limits can be seen and iterated on when a transaction is simulated/estimated.

## Examples for private functions (reducing gate count)")

After the first section about generating a flamegraph for an Aztec function, each section shows an example of different optimisation techniques.

## Inspecting with FlamegraphYou can see the params for the Aztec's flamegraph using: `aztec help flamegraph`

For example, the resulting flamegraph (as an .svg file) of a counter's increment function can be generated and served with: `SERVE=1 aztec flamegraph target/counter-Counter.json increment`

![](/assets/ideal-img/flamegraph-counter.facab27.640.png)

To get a sense of things, here is a table of gate counts for common operations:

| Gates | Operation |
| --- | --- |
| ~75 | Hashing 3 fields with Poseidon2 |
| 3500 | Reading a value from a tree (public data tree, note hash tree, nullifier tree) |
| 4000 | Reading a delayed public mutable read |
| X000 | Calculating sha256 |
| X000 | Constrained encryption of a log of Y fields |
| X000 | Constrained encryption and tag a log of Y fields |

## Optimization: use arithmetic instead of non-arithmetic operationsBecause the underlying equation in the proving backend makes use of multiplication and addition, these operations incur less gates than bit-shifting or bit-masking.

For example:

```
comptime global TWO_POW_32: Field = 2.pow_32(16);  
// ...  
{  
    #[external("private")]  
    fn mul_inefficient(number: Field) -> u128 {  
        number as u128 << 16 as u8  
    } // 5244 gates  
  
    #[external("private")]  
    fn mul_efficient(number: Field) -> u128 {  
        (number * TWO_POW_32) as u128  
    } // 5184 gates (60 gates less)  
}
```

When comparing the flamegraph of the two functions, the inefficient shift example has a section of gates not present in the multiplication example. This difference equates to a saving of 60 gates.

In the same vein bitwise `AND`/`OR`, and inequality relational operators (`>`, `<`) are expensive. Try avoid these in your circuits.

For example, use boolean equality effectively instead of `>=`:

```
{  
    #[external("private")]  
    fn sum_from_inefficient(from: u32, array: [u32; 1000]) -> u32 {  
        let mut sum: u32 = 0;  
        for i in 0..1000 {  
            if i >= from { // condition based on `>=` each time (higher gate count)  
                sum += array[i];  
            }  
        }  
        sum  
    } // 44317 gates  
  
    #[external("private")]  
    fn sum_from_efficient(from: u32, array: [u32; 1000]) -> u32 {  
        let mut sum: u32 = 0;  
        let mut do_sum = false;  
        for i in 0..1000 {  
            if i == from { // latches boolean at transition (equality comparison)  
                do_sum = true;  
            }  
            if do_sum { // condition based on boolean true (lower gate count)  
                sum += array[i];  
            }  
        }  
        sum  
    } // 45068 gates (751 gates less)  
}
```

So for a loop of 1000 iterations, 751 gates were saved by:

* Adding an equivalence check and a boolean assignment
* Replacing `>=` with a boolean equivalence check

Difference with Rust

Such designs with boolean flags lend themselves well into logical comparisons too since `&&` and `||` do not exist. With booleans, using `&` and `|` can give you the required logic efficiently. For more points specific to the Noir language, see [this](https://noir-lang.org/docs/explainers/explainer-writing-noir#translating-from-rust) section.

## Optimization: Loop designSince private functions are circuits, their size must be known at compile time, which is equivalent to its execution trace.
See [this example](https://github.com/noir-lang/noir-examples/blob/master/noir_by_example/loops/noir/src/main.nr#L11) for how to use loops when dynamic execution lengths (ie variable number of loops) is not possible.

## Optimization: considered use of `unconstrained` functions## Example - calculating square rootConsider the following example of an implementation of the `sqrt` function:

```
use dep::aztec::macros::aztec;  
  
#[aztec]  
pub contract OptimisationExample {  
    use dep::aztec::macros::{functions::{external, initializer}, storage::storage};  
  
    #[storage]  
    struct Storage<Context> {}  
  
    #[external("public")]  
    #[initializer]  
    fn constructor() {}  
  
    #[external("private")]  
    fn sqrt_inefficient(number: Field) -> Field {  
        super::sqrt_constrained(number)  
    }  
  
    #[external("private")]  
    fn sqrt_efficient(number: Field) -> Field {  
        // Safety: calculate in unconstrained function, then constrain the result  
        let x = unsafe { super::sqrt_unconstrained(number) };  
        assert(x * x == number, "x*x should be number");  
        x  
    }  
  
}  
  
fn sqrt_constrained(number: Field) -> Field {  
    let MAX_LEN = 100;  
  
    let mut guess = number;  
    let mut guess_squared = guess * guess;  
    for _ in 1..MAX_LEN as u32 + 1 {  
        // only use square root part of circuit when required, otherwise use alternative part of circuit that does nothing  
        // Note: both parts of the circuit exist MAX_LEN times in the circuit, regardless of whether the square root part is used or not  
        if (guess_squared != number) {  
            guess = (guess + number / guess) / 2;  
            guess_squared = guess * guess;  
        }  
    }  
  
    guess  
}  
  
unconstrained fn sqrt_unconstrained(number: Field) -> Field {  
    let mut guess = number;  
    let mut guess_squared = guess * guess;  
    while guess_squared != number {  
        guess = (guess + number / guess) / 2;  
        guess_squared = guess * guess;  
    }  
    guess  
}
```

The two implementations after the contract differ in one being constrained vs unconstrained, as well as the loop implementation (which has other design considerations).
Measuring the two, we find the `sqrt_inefficient` to require around 1500 extra gates compared to `sqrt_efficient`.

To see each flamegraph:

* `SERVE=1 aztec flamegraph target/optimisation_example-OptimisationExample.json sqrt_inefficient`
* `SERVE=1 aztec flamegraph target/optimisation_example-OptimisationExample.json sqrt_efficient`
* (if you make changes to the code, you will need to compile and regenerate the flamegraph, then refresh in your browser to use the latest svg file)

Note: this is largely a factor of the loop size choice based on the maximum size of `number` you are required to be calculating the square root of. For larger numbers, the loop would have to be much larger, so perform in an unconstrained way (then constraining the result) is much more efficient.

## Example - sorting an arrayLike with sqrt, we have the inefficient function that does the sort with constrained operations, and the efficient function that uses the unconstrained sort function then constrains the result.

```
//...  
{  
    #[external("private")]  
    fn sort_inefficient(array: [u32; super::ARRAY_SIZE]) -> [u32; super::ARRAY_SIZE] {  
        let mut sorted_array = array;  
        for i in 0..super::ARRAY_SIZE as u32 {  
            for j in 0..super::ARRAY_SIZE as u32 {  
                if sorted_array[i] < sorted_array[j] {  
                    let temp = sorted_array[i as u32];  
                    sorted_array[i as u32] = sorted_array[j as u32];  
                    sorted_array[j as u32] = temp;  
                }  
            }  
        }  
        sorted_array  
    } // 6823 gates for 10 elements, 127780 gates for 100 elements  
  
    #[external("private")]  
    fn sort_efficient(array: [u32; super::ARRAY_SIZE]) -> [u32; super::ARRAY_SIZE] {  
        // Safety: calculate in unconstrained function, then constrain the result  
        let sorted_array = unsafe { super::sort_array(array) };  
        // constrain that sorted_array elements are sorted  
        for i in 0..super::ARRAY_SIZE as u32 {  
            assert(sorted_array[i] <= sorted_array[i + 1], "array should be sorted");  
        }  
        sorted_array  
    } // 5870 gates (953 gates less) for 10 elements, 12582 gates for 100 elements (115198 gates less)  
}  
  
unconstrained fn sort_array(array: [u32; ARRAY_SIZE]) -> [u32; ARRAY_SIZE] {  
    let mut sorted_array = array;  
    for i in 0..ARRAY_SIZE as u32 {  
        for j in 0..ARRAY_SIZE as u32 {  
            if sorted_array[i] < sorted_array[j] {  
                let temp = sorted_array[i as u32];  
                sorted_array[i as u32] = sorted_array[j as u32];  
                sorted_array[j as u32] = temp;  
            }  
        }  
    }  
    sorted_array  
}
```

Like before, the flamegraph command can be used to present the gate counts of the private functions, highlighting that 953 gates could be saved.

Note: The stdlib provides a highly optimized version of sort on arrays, `array.sort()`, which saves even more gates.

```
    #[external("private")]  
    fn sort_stdlib(array: [u32; super::ARRAY_SIZE]) -> [u32; super::ARRAY_SIZE] {  
        array.sort();  
    } // 5943 gates (880 gates less) for 10 elements, 13308 gates for 100 elements (114472 gates less)
```

## Example - refactoring arraysIn the same vein, refactoring is inefficient when done constrained, and more efficient to do unconstrained then constrain the output.

```
{  
    #[external("private")]  
    fn refactor_inefficient(array: [u32; super::ARRAY_SIZE]) -> [u32; super::ARRAY_SIZE] {  
        let mut compacted_array = [0; super::ARRAY_SIZE];  
        let mut index = 0;  
        for i in 0..super::ARRAY_SIZE as u32 {  
            if (array[i] != 0) {  
                compacted_array[index] = array[i];  
                index += 1;  
            }  
        }  
        compacted_array  
    } // 6570 gates for 10 elements, 93071 gates for 100 elements  
  
    #[external("private")]  
    fn refactor_efficient(array: [u32; super::ARRAY_SIZE]) -> [u32; super::ARRAY_SIZE] {  
        let compacted_array = unsafe { super::refactor_array(array) };  
        // count non-zero elements in array  
        let mut count = 0;  
        for i in 0..super::ARRAY_SIZE as u32 {  
            if (array[i] != 0) {  
                count += 1;  
            }  
        }  
        // count non-zero elements in compacted_array  
        let mut count_compacted = 0;  
        for i in 0..super::ARRAY_SIZE as u32 {  
            if (compacted_array[i] != 0) {  
                count_compacted += 1;  
            } else {  
                assert(compacted_array[i] == 0, "trailing compacted_array elements should be 0");  
            }  
        }  
        assert(count == count_compacted, "count should be equal to count_compacted");  
        compacted_array  
    } // 5825 gates (745 gates less), 12290 gates for 100 elements (80781 gates less)  
}  
  
unconstrained fn refactor_array(array: [u32; ARRAY_SIZE]) -> [u32; ARRAY_SIZE] {  
    let mut compacted_array = [0; ARRAY_SIZE];  
    let mut index = 0;  
    for i in 0..ARRAY_SIZE as u32 {  
        if (array[i] != 0) {  
            compacted_array[index] = array[i];  
            index += 1;  
        }  
    }  
    compacted_array  
}
```

## Optimizing: Reducing L2 readsIf a struct has many fields to be read, we can design an extra variable maintained as the hash of all values within it (like a checksum). When it comes to reading, we can now do an unconstrained read (incurring no read requests), and then check the hash of the result against that stored for the struct. This final check is thus only one read request rather than one per variable.

Leverage unconstrained functions

When needing to make use of large private operations (eg private execution or many read requests), use of [unconstrained functions](https://noir-lang.org/docs/explainers/explainer-writing-noir#leverage-unconstrained-execution) wisely to reduce the gate count of private functions.

---


# Proving Historic State

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/advanced/how_to_prove_history

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to prove historical state transitions and note inclusion using Aztec's Archive tree.

## Prerequisites* An Aztec contract project set up with `aztec-nr` dependency
* Understanding of Aztec's note and nullifier system
* Knowledge of Merkle tree concepts

## Understand what you can proveYou can create proofs for these elements at any past block height:

* Note inclusion/exclusion
* Nullifier inclusion/exclusion
* Note validity (included and not nullified)
* Public value existence
* Contract deployment

Use cases include:

* Timestamp verification in private contexts
* Eligibility verification based on historical note ownership
* Item ownership verification
* Public data existence proofs
* Contract deployment verification

Historical Proofs

Prove state at any past block using the Archive tree. Useful for timestamps, eligibility checks, and ownership verification.

## Retrieve notes for proofs```
use aztec::note::note_getter_options::NoteGetterOptions;  
  
let options = NoteGetterOptions::new();  
let notes = storage.notes.at(owner).get_notes(options);  
  
// Access first note as retrieved_note  
let retrieved_note = notes.get(0);
```

## Prove note inclusion```
use dep::aztec::history::note_validity::ProveNoteValidity;  
  
// Get block header for historical proof  
let header = context.get_block_header();  
  
// Prove note existed and wasn't nullified  
// Requires: RetrievedNote, storage_slot, context  
header.prove_note_validity(retrieved_note, storage_slot, &mut context);
```

tip

Use `prove_note_validity` to verify both inclusion and non-nullification in one call.

## Prove nullifier inclusion```
use dep::aztec::history::nullifier_inclusion::ProveNullifierInclusion;  
use dep::aztec::protocol_types::hash::compute_siloed_nullifier;  
  
// Compute nullifier (requires note hash)  
let nullifier = note.compute_nullifier(&mut context, note_hash_for_nullification);  
let siloed_nullifier = compute_siloed_nullifier(context.this_address(), nullifier);  
  
// Prove nullifier was included  
context.get_block_header().prove_nullifier_inclusion(siloed_nullifier);
```

Additional Proofs

Other available proofs:

* Note inclusion without validity check
* Nullifier non-inclusion (prove something wasn't nullified)
* Public data inclusion at historical blocks

---


# Using Capsules

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/advanced/how_to_use_capsules

Version: Devnet (v3.0.0-devnet.20251212)

On this page

What are Capsules?

Capsules provide per-contract non-volatile storage in the PXE. Data is:

* Stored locally (not onchain)
* Scoped per contract address
* Persistent until explicitly deleted
* Useful for caching computation results

## Available functions* `store` - Store data at a slot
* `load` - Retrieve data from a slot
* `delete` - Remove data at a slot
* `copy` - Copy contiguous entries between slots

## Basic usage```
use dep::aztec::oracle::capsules;  
  
// Store data at a slot  
unconstrained fn store_data(context: &mut PrivateContext) {  
    capsules::store(context.this_address(), slot, value);  
}  
  
// Load data (returns Option<T>)  
unconstrained fn load_data(context: &mut PrivateContext) -> Option<MyStruct> {  
    capsules::load(context.this_address(), slot)  
}  
  
// Delete data at a slot  
unconstrained fn delete_data(context: &mut PrivateContext) {  
    capsules::delete(context.this_address(), slot);  
}  
  
// Copy multiple contiguous slots  
unconstrained fn copy_data(context: &mut PrivateContext) {  
    // Copy 3 slots from src_slot to dst_slot  
    capsules::copy(context.this_address(), src_slot, dst_slot, 3);  
}
```

Safety

All capsule operations are `unconstrained`. Data loaded from capsules should be validated in constrained contexts. Contracts can only access their own capsules - attempts to access other contracts' capsules will fail.

## CapsuleArray for dynamic storage```
use dep::aztec::capsules::CapsuleArray;  
  
unconstrained fn manage_array(context: &mut PrivateContext) {  
    // Create/access array at base_slot  
    let array = CapsuleArray::at(context.this_address(), base_slot);  
  
    // Array operations  
    array.push(value);              // Append to end  
    let value = array.get(index);   // Read at index  
    let length = array.len();       // Get current size  
    array.remove(index);            // Delete & shift elements  
  
    // Iterate over all elements  
    array.for_each(|index, value| {  
        // Process each element  
        if some_condition(value) {  
            array.remove(index); // Safe to remove current element  
        }  
    });  
}
```

Use Cases

* Caching expensive computations between simulation and execution
* Storing intermediate proof data
* Managing dynamic task lists
* Persisting data across multiple transactions

Storage Layout

CapsuleArray stores the length at the base slot, with elements in consecutive slots:

* Slot N: array length
* Slot N+1: element at index 0
* Slot N+2: element at index 1
* And so on...

---


# Oracle Functions

Source: https://docs.aztec.network/developers/docs/aztec-nr/framework-description/advanced/protocol_oracles

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This page goes over what oracles are in Aztec and how they work.

Looking for a hands-on guide? You can learn how to use oracles in a smart contract [here](/developers/docs/aztec-nr/framework-description/advanced/how_to_use_capsules).

An oracle is something that allows us to get data from the outside world into our contracts. The most widely-known types of oracles in blockchain systems are probably Chainlink price feeds, which allow us to get the price of an asset in USD taking non-blockchain data into account.

While this is one type of oracle, the more general oracle, allows us to get any data into the contract. In the context of oracle functions or oracle calls in Aztec, it can essentially be seen as user-provided arguments, that can be fetched at any point in the circuit, and don't need to be an input parameter.

**Why is this useful? Why don't just pass them as input parameters?**
In the world of EVM, you would just read the values directly from storage and call it a day. However, when we are working with circuits for private execution, this becomes more tricky as you cannot just read the storage directly from your state tree, because there are only commitments (e.g. hashes) there. The pre-images (content) of your commitments need to be provided to the function to prove that you actually allowed to modify them.

If we fetch the notes using an oracle call, we can keep the function signature independent of the underlying data and make it easier to use. A similar idea, applied to the authentication mechanism is used for the Authentication Witnesses that allow us to have a single function signature for any wallet implementation, see [AuthWit](/developers/docs/aztec-nr/framework-description/how_to_use_authwit) for more information on this.

Oracles introduce **non-determinism** into a circuit, and thus are `unconstrained`. It is important that any information that is injected into a circuit through an oracle is later constrained for correctness. Otherwise, the circuit will be **under-constrained** and potentially insecure!

`Aztec.nr` has a module dedicated to its oracles. If you are interested, you can view them by following the link below:

oracles-module

```
/// Oracles module
```

> [Source code: noir-projects/aztec-nr/aztec/src/oracle/mod.nr#L1-L3](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/aztec-nr/aztec/src/oracle/mod.nr#L1-L3)

## Inbuilt oracles* [`debug_log`](https://github.com/AztecProtocol/aztec-packages/blob/master/noir-projects/aztec-nr/aztec/src/oracle/debug_log.nr) - Provides a couple of debug functions that can be used to log information to the console. Read more about debugging [here](/developers/docs/aztec-nr/debugging).
* [`auth_witness`](https://github.com/AztecProtocol/aztec-packages/blob/master/noir-projects/aztec-nr/authwit/src/auth_witness.nr) - Provides a way to fetch the authentication witness for a given address. This is useful when building account contracts to support approve-like functionality.
* [`get_l1_to_l2_message`](https://github.com/AztecProtocol/aztec-packages/blob/master/noir-projects/aztec-nr/aztec/src/oracle/get_l1_to_l2_message.nr) - Useful for application that receive messages from L1 to be consumed on L2, such as token bridges or other cross-chain applications.
* [`notes`](https://github.com/AztecProtocol/aztec-packages/blob/master/noir-projects/aztec-nr/aztec/src/oracle/notes.nr) - Provides a lot of functions related to notes, such as fetches notes from storage etc, used behind the scenes for value notes and other pre-build note implementations.
* [`logs`](https://github.com/AztecProtocol/aztec-packages/blob/master/noir-projects/aztec-nr/aztec/src/oracle/logs.nr) - Provides the to log encrypted and unencrypted data.

Find a full list [on GitHub](https://github.com/AztecProtocol/aztec-packages/tree/master/noir-projects/aztec-nr/aztec/src/oracle).

Please note that it is **not** possible to write a custom oracle for your dapp. Oracles are implemented in the PXE, so all users of your dapp would have to use a PXE with your custom oracle included. If you want to inject some arbitrary data that does not have a dedicated oracle, you can use [capsules](/developers/docs/aztec-nr/framework-description/advanced/how_to_use_capsules).

---


# Aztec.nr API Reference

Source: https://docs.aztec.network/developers/docs/aztec-nr/api

Version: Devnet (v3.0.0-devnet.20251212)

On this page

The Aztec.nr API reference documentation is auto-generated from the source code using `nargo doc`.

## View the API Documentation**[Open Aztec.nr API Reference →](/aztec-nr-api/devnet/all)**

The API reference includes documentation for all public modules, functions, structs, and types in the aztec-nr workspace:

## Core Crates* [**noir\_aztec**](/aztec-nr-api/devnet/noir_aztec/) - Core Aztec contract framework including:
  + [`context`](/aztec-nr-api/devnet/noir_aztec/context/) - Private and public execution contexts
  + [`state_vars`](/aztec-nr-api/devnet/noir_aztec/state_vars/) - State variable types (PrivateMutable, PublicMutable, Map, etc.)
  + [`note`](/aztec-nr-api/devnet/noir_aztec/note/) - Note interfaces and utilities
  + [`authwit`](/aztec-nr-api/devnet/noir_aztec/authwit/) - Authentication witness support
  + [`history`](/aztec-nr-api/devnet/noir_aztec/history/) - Historical state proofs
  + [`messages`](/aztec-nr-api/devnet/noir_aztec/messages/) - Cross-chain messaging
  + [`oracle`](/aztec-nr-api/devnet/noir_aztec/oracle/) - Oracle interfaces
  + [`macros`](/aztec-nr-api/devnet/noir_aztec/macros/) - Contract macros and attributes
  + [`hash`](/aztec-nr-api/devnet/noir_aztec/hash/) - Hash functions and utilities
  + [`keys`](/aztec-nr-api/devnet/noir_aztec/keys/) - Key management utilities
  + [`event`](/aztec-nr-api/devnet/noir_aztec/event/) - Event emission and interfaces
  + [`test`](/aztec-nr-api/devnet/noir_aztec/test/) - Testing utilities
  + [`utils`](/aztec-nr-api/devnet/noir_aztec/utils/) - General utilities

## Note Types* [**address\_note**](/aztec-nr-api/devnet/address_note/) - Note type for storing Aztec addresses
* [**value\_note**](/aztec-nr-api/devnet/value_note/index.html) - Note type for storing field values
* [**uint\_note**](/aztec-nr-api/devnet/uint_note/) - Note type for storing unsigned integers

## Utilities* [**compressed\_string**](/aztec-nr-api/devnet/compressed_string/) - Compressed string utilities for efficient storage
* [**easy\_private\_state**](/aztec-nr-api/devnet/easy_private_state/index.html) - Simplified private state management

---


# Aztec.js

Source: https://docs.aztec.network/developers/docs/aztec-js

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Aztec.js is a library that provides APIs for managing accounts and interacting with contracts on the Aztec network. It communicates with the [Private eXecution Environment (PXE)](/developers/docs/foundational-topics/pxe) through a `PXE` implementation, allowing developers to easily register new accounts, deploy contracts, view functions, and send transactions.

## Installing```
npm install @aztec/aztec.js
```

## FlowThese are some of the important functions you'll need to use in your Aztec.js:

* [Create an account with `@aztec/accounts`](/developers/docs/aztec-js/how_to_create_account)
* [Deploy a contract](/developers/docs/aztec-js/how_to_deploy_contract)
* [Simulate a function call](/developers/docs/aztec-js/how_to_simulate_function)
* [Send a transaction](/developers/docs/aztec-js/how_to_send_transaction)

---


# Getting Started

Source: https://docs.aztec.network/developers/docs/aztec-js/how_to_connect_to_local_network

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to connect your application to the Aztec local network and interact with the network.

## Prerequisites* Running Aztec local network (see [Quickstart](/developers/getting_started_on_local_network)) on port 8080
* Node.js installed
* TypeScript project set up

## Install Aztec.js## Install the Aztec.js package```
yarn add @aztec/aztec.js@3.0.0-devnet.20251212
```

## Create a Node ClientThe local network is essentially a one-node network. Just like on a real network, you need to interface with it:

```
const node = createAztecNodeClient("http://localhost:8080");  
const l1Contracts = await node.getL1ContractAddresses();
```

As the name implies, we want to know the L1 Contracts addresses for our wallet.

## Create a TestWalletYou will need to create your own TestWallet to connect to local network accounts. Let's create a TestWallet:

```
import { createAztecNodeClient } from "@aztec/aztec.js/node";  
import { TestWallet } from "@aztec/test-wallet/server";  
  
export async function setupWallet(): Promise<TestWallet> {  
  const nodeUrl = "http://localhost:8080";  
  const node = createAztecNodeClient(nodeUrl);  
  const wallet = await TestWallet.create(node);  
  return wallet;  
}
```

## Verify the connectionGet node information to confirm your connection:

```
const nodeInfo = await pxe.getNodeInfo();  
console.log("Connected to local network version:", nodeInfo.nodeVersion);  
console.log("Chain ID:", nodeInfo.l1ChainId);
```

## Get local network accountsThe local network has some accounts pre-funded with fee-juice to pay for gas. You can import them and create accounts:

```
import { getInitialTestAccountsData } from "@aztec/accounts/testing";  
  
const [aliceAccount, bobAccount] = await getInitialTestAccountsData();  
await wallet.createSchnorrAccount(aliceAccount.secret, aliceAccount.salt);  
await wallet.createSchnorrAccount(bobAccount.secret, bobAccount.salt);
```

## Check account balancesVerify that the accounts have fee juice for transactions:

```
import { getFeeJuiceBalance } from "@aztec/aztec.js/utils";  
  
const aliceBalance = await getFeeJuiceBalance(aliceAccount.address, node);  
console.log(`Alice's fee juice balance: ${aliceBalance}`);
```

## Next steps* [Create an account](/developers/docs/aztec-js/how_to_create_account) - Deploy new accounts on the network
* [Deploy a contract](/developers/docs/aztec-js/how_to_deploy_contract) - Deploy your smart contracts
* [Send transactions](/developers/docs/aztec-js/how_to_send_transaction) - Execute contract functions

---


# Creating Accounts

Source: https://docs.aztec.network/developers/docs/aztec-js/how_to_create_account

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide walks you through creating and deploying a new account contract in Aztec.

## Prerequisites* Running Aztec local network or testnet
* Node.js and TypeScript environment
* `@aztec/aztec.js` package installed
* Understanding of [account concepts](/developers/docs/foundational-topics/accounts)

## Install dependencies```
yarn add @aztec/aztec.js@3.0.0-devnet.20251212 @aztec/accounts@3.0.0-devnet.20251212
```

## Create account keysEvery account on Aztec requires a secret, a salt, and a signing key.

```
import { Fr, GrumpkinScalar } from "@aztec/aztec.js";  
  
const secretKey = Fr.random();  
const salt = new Fr(0);  
const signingPrivateKey = GrumpkinScalar.random();
```

These keys serve the following purposes:

* `secretKey`: Derives encryption keys for private state
* `signingPrivateKey`: Signs transactions

## Create a walletYou need a Wallet to hold your account contract. Use `TestWallet` since most third-party wallets implement the same interface:

```
// for use in the browser  
import { TestWallet } from "@aztec/test-wallet/client/lazy";  
// for use on a server  
import { TestWallet } from "@aztec/test-wallet/server";  
import { createAztecNodeClient } from "@aztec/aztec.js/node";  
  
const nodeUrl = process.env.AZTEC_NODE_URL || "http://localhost:8080";  
const node = createAztecNodeClient(nodeUrl);  
const wallet = await TestWallet.create(node);
```

## Deploy the account## Get Fee JuiceOn the local network, all test accounts come pre-funded with Fee Juice. [Import them](/developers/docs/aztec-js/how_to_connect_to_local_network) to start using them immediately.

On testnet, accounts start without Fee Juice. You can either [use an account that has Fee Juice](/developers/docs/aztec-js/how_to_pay_fees#pay-with-fee-juice) or [use the Sponsored Fee Payment Contract](/developers/docs/aztec-js/how_to_pay_fees#sponsored-fee-payment-contracts).

## Register and deploy accountsTest accounts on the local network are already deployed but need to be registered in the wallet:

```
// on the local network, you can get the initial test accounts data using getInitialTestAccountsData  
const [initialAccountData] = await getInitialTestAccountsData();  
// add the funded account to the wallet  
const initialAccount = await wallet.createSchnorrAccount(  
  initialAccountData.secret,  
  initialAccountData.salt  
);
```

Other accounts require deployment. To deploy an account that already has Fee Juice:

```
const anotherAccount = await wallet.createSchnorrAccount(  
  accountWithFeeJuice.secret,  
  accountWithFeeJuice.salt  
);  
const deployMethod = await anotherAccount.getDeployMethod();  
  
// using the default fee payment method (Fee Juice)  
await deployMethod  
  .send({  
    from: AztecAddress.ZERO, // the zero address is used because there's no account to send from: the transaction itself will create the account!  
  })  
  .wait();
```

To deploy using the Sponsored FPC:

```
// deploy an account with random salt and secret  
const anotherAccount = await wallet.createSchnorrAccount(  
  Fr.random(),  
  Fr.random()  
);  
const deployMethod = await anotherAccount.getDeployMethod();  
await deployMethod  
  .send({  
    from: AztecAddress.ZERO,  
    fee: { paymentMethod: sponsoredPaymentMethod },  
  })  
  .wait();
```

info

See the [guide on fees](/developers/docs/aztec-js/how_to_pay_fees) for setting up `sponsoredPaymentMethod`.

## Next steps* [Deploy contracts](/developers/docs/aztec-js/how_to_deploy_contract) with a new account
* [Send transactions](/developers/docs/aztec-js/how_to_send_transaction) from an account
* Learn about [account abstraction](/developers/docs/foundational-topics/accounts)
* Implement [authentication witnesses](/developers/docs/aztec-js/how_to_use_authwit)

---


# Deploying Contracts

Source: https://docs.aztec.network/developers/docs/aztec-js/how_to_deploy_contract

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to deploy compiled contracts to Aztec using the generated TypeScript interfaces.

## Prerequisites* Compiled contract artifacts (see [How to Compile](/developers/docs/aztec-nr/how_to_compile_contract))
* Running Aztec local network
* Funded wallet for deployment fees
* TypeScript project set up

## Generate TypeScript bindings## Compile and generate code```
# Compile the contract  
aztec compile  
  
# Generate TypeScript interface  
aztec codegen ./target/my_contract-MyContract.json -o src/artifacts
```

info

The codegen command creates a TypeScript class with typed methods for deployment and interaction. This provides type safety and autocompletion in your IDE.

## Deploy a contract## Step 1: Import and connect```
import { MyContract } from "./artifacts/MyContract";
```

## Step 2: Deploy the contractDeploying the contract really depends on how you're paying for it. If paying using an account's fee juice (like a test account on the local network):

```
// Deploy with constructor arguments  
const contract = await MyContract.deploy(  
  deployer_wallet,  
  constructorArg1,  
  constructorArg2  
)  
  .send({ from: testAccount.address }) // testAccount has fee juice and is registered in the deployer_wallet  
  .deployed();
```

On the testnet, you'll likely not have funds in `testAccount` to pay for fee Juice. You want to instead pay fees using the [Sponsored Fee Payment Contract method](/developers/docs/aztec-js/how_to_pay_fees), for example:

```
const contract = await MyContract.deploy(  
  wallet,  
  constructorArg1,  
  constructorArg2  
)  
  .send({ from: alice.address, fee: { paymentMethod: sponsoredPaymentMethod } }) // using the Sponsored FPC  
  .deployed();
```

## Use deployment options## Deploy with custom saltBy default, the deployment's salt is random, but you can specify it (for example, if you want to get a deterministic address):

```
import { Fr } from "@aztec/aztec.js/fields";  
  
const salt = Fr.random();  
  
const contract = await MyContract.deploy(wallet, arg1, arg2)  
  .send({  
    from: testAccount.address,  
    contractAddressSalt: salt,  
  })  
  .deployed();
```

## Deploy universallyDeploy to the same address across networks:

```
const contract = await MyContract.deploy(wallet, arg1, arg2)  
  .send({  
    from: testAccount.address,  
    universalDeploy: true,  
    contractAddressSalt: salt,  
  })  
  .deployed();
```

info

Universal deployment excludes the sender from address computation, allowing the same address on any network with the same salt.

## Skip initializationDeploy without running the constructor:

```
const contract = await MyContract.deploy(wallet)  
  .send({  
    from: testAccount.address,  
    skipInitialization: true,  
  })  
  .deployed();  
  
// Initialize later  
await contract.methods  
  .initialize(arg1, arg2)  
  .send({ from: testAccount.address })  
  .wait();
```

## Calculate deployment address## Get address before deployment```
import { Fr } from "@aztec/aztec.js/fields";  
  
const salt = Fr.random();  
const deployer = testAccount.address;  
  
// Calculate address without deploying  
const deployer = MyContract.deploy(wallet, arg1, arg2);  
const instance = await deployer.getInstance();  
const address = instance.address;  
  
console.log(`Contract will deploy at: ${address}`);
```

warning

This is an advanced pattern. For most use cases, deploy the contract directly and get the address from the deployed instance.

## Monitor deployment progress## Track deployment transaction```
const deployTx = MyContract.deploy(wallet, arg1, arg2).send({  
  from: testAccount.address,  
});  
  
// Get transaction hash immediately  
const txHash = await deployTx.getTxHash();  
console.log(`Deployment tx: ${txHash}`);  
  
// Wait for the transaction to be mined  
const receipt = await deployTx.wait();  
console.log(`Deployed in block ${receipt.blockNumber}`);  
  
// Get the deployed contract instance  
const contract = await deployTx.deployed();  
console.log(`Contract address: ${contract.address}`);
```

## Deploy multiple contracts## Deploy contracts with dependencies```
// Deploy first contract  
const token = await TokenContract.deploy(  
  wallet,  
  wallet.address,  
  "MyToken",  
  "MTK",  
  18n  
)  
  .send({ from: testAccount.address })  
  .deployed();  
  
// Deploy second contract with reference to first  
const vault = await VaultContract.deploy(  
  wallet,  
  token.address // Pass first contract's address  
)  
  .send({ from: wallet.address })  
  .deployed();
```

## Deploy contracts in parallel```
// Start all deployments simultaneously  
const deployments = [  
  Contract1.deploy(wallet, arg1).send({ from: testAccount.address }),  
  Contract2.deploy(wallet, arg2).send({ from: testAccount.address }),  
  Contract3.deploy(wallet, arg3).send({ from: testAccount.address }),  
];  
  
// Wait for all to complete  
const receipts = await Promise.all(deployments.map((d) => d.wait()));  
  
// Get deployed contract instances  
const contracts = await Promise.all(deployments.map((d) => d.deployed()));
```

tip

Parallel deployment is faster but be aware of nonce management if deploying many contracts from the same account.

## Verify deployment## Check contract registrationAt the moment the easiest way to get contract data is by querying a wallet directly:

```
// Get contract metadata  
const metadata = await wallet.getContractMetadata(myContractInstance.address);  
if (metadata) {  
  console.log("Contract metadata found");  
}
```

## Verify contract is callable```
try {  
  // Try calling a view function  
  const result = await contract.methods  
    .get_version()  
    .simulate({ from: testAccount.address });  
  console.log("Contract is callable, version:", result);  
} catch (error) {  
  console.error("Contract not accessible:", error.message);  
}
```

## Register deployed contracts## Add existing contract to walletIf a contract was deployed by another account:

```
import { loadContractArtifact } from "@aztec/stdlib/abi";  
  
const artifact = loadContractArtifact(MyContract.artifact);  
const contract = await MyContract.at(contractAddress, wallet);  
  
// To register an existing contract instance, you need to know  
// its exact deployment parameters. The registerContract method  
// requires both the artifact and instance details.  
// This is typically handled automatically when deploying.  
await wallet.registerContract({  
  instance: contract.instance,  
  artifact: artifact,  
});
```

warning

You need the exact deployment parameters (salt, initialization hash, etc.) to correctly register an externally deployed contract.

For example:

```
import { getContractInstanceFromInstantiationParams } from "@aztec/stdlib/contracts";  
const contract = await getContractInstanceFromInstantiationParams(  
  contractArtifact,  
  {  
    publicKeys: PublicKeys.default(),  
    constructorArtifact: initializer,  
    constructorArgs: parameters,  
    deployer: from,  
    salt,  
  }  
);
```

## Next steps* [Send transactions](/developers/docs/aztec-js/how_to_send_transaction) to interact with your contract
* [Simulate functions](/developers/docs/aztec-js/how_to_simulate_function) to read contract state
* [Use authentication witnesses](/developers/docs/aztec-js/how_to_use_authwit) for delegated calls

---


# Sending Transactions

Source: https://docs.aztec.network/developers/docs/aztec-js/how_to_send_transaction

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to send transactions to smart contracts on Aztec.

## Prerequisites* Deployed contract with its address and ABI
* Funded account wallet
* Running Aztec local network or connected to a network
* Understanding of [contract interactions](/developers/docs/aztec-nr/framework-description/how_to_call_contracts)

## Sending a basic transactionLet's say you've connected to a contract, for example:

```
import { Contract } from "@aztec/aztec.js";  
  
const contract = await Contract.at(contractAddress, artifact, wallet);
```

or

```
import { MyContract } from "./artifacts/MyContract";  
  
const contract = await MyContract.at(contractAddress, wallet);
```

You should [choose your fee-paying method](/developers/docs/aztec-js/how_to_pay_fees) and just call a function on it:

```
const withFeeJuice = await contract.methods  
  .transfer(recipientAddress, amount)  
  .send({ from: fundedAccount.address }) // if this account has fee-juice  
  .wait();  
  
// or using the Sponsored FPC  
  
const sponsored = await contract.methods  
  .transfer(recipientAddress, amount)  
  .send({ fee: { paymentMethod: sponsoredPaymentMethod } })  
  .wait();
```

## Send without waiting```
// Send transaction and get a SentTx object  
const sentTx = contract.methods  
  .transfer(recipientAddress, amount)  
  .send({ from: fundedAccount.address });  
  
// Get transaction hash immediately  
const txHash = await sentTx.getTxHash();  
console.log(`Transaction sent with hash: ${txHash.toString()}`);  
  
// Wait for inclusion later  
const receipt = await sentTx.wait();  
console.log(`Transaction mined in block ${receipt.blockNumber}`);
```

## Send batch transactions## Execute multiple calls atomically```
import { BatchCall } from "@aztec/aztec.js";  
  
const batch = new BatchCall(wallet, [  
  token.methods.approve(spender, amount),  
  contract.methods.deposit(amount),  
  contract.methods.updateState(),  
]);  
  
const receipt = await batch.send({ from: fundedAccount.address }).wait();  
console.log(  
  `Batch executed in block ${receipt.blockNumber} with fee ${receipt.transactionFee}`  
);
```

warning

All calls in a batch must succeed or the entire batch reverts. Use batch transactions when you need atomic execution of multiple operations.

## Query transaction status## Get transaction receipt```
const txHash = await sentTx.getTxHash();  
const receipt = await wallet.getTxReceipt(txHash); // or node.getTxReceipt(txHash);
```

## Check transaction effects```
const txHash = await sentTx.getTxHash();  
const effect = await node.getTxEffect(txHash);  
  
// Access public data writes  
effect.data.publicDataWrites.forEach((write) => {  
  console.log(`Wrote ${write.value} to slot ${write.leafSlot}`);  
});  
  
// Check note hashes (private note commitments)  
effect.data.noteHashes.forEach((noteHash) => {  
  console.log(`Created note: ${noteHash.toString()}`);  
});  
  
// Check nullifiers (consumed notes)  
effect.data.nullifiers.forEach((nullifier) => {  
  console.log(`Nullified: ${nullifier.toString()}`);  
});
```

## Next steps* Learn to [simulate functions](/developers/docs/aztec-js/how_to_simulate_function) before sending
* Understand [authentication witnesses](/developers/docs/aztec-js/how_to_use_authwit) for delegated transactions
* Configure [gas and fees](/developers/docs/aztec-js/how_to_pay_fees) for optimal transaction costs
* Set up [transaction testing](/developers/docs/aztec-js/how_to_test) in your development workflow

---


# Simulating Functions

Source: https://docs.aztec.network/developers/docs/aztec-js/how_to_simulate_function

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to simulate function calls to read contract state.

## Prerequisites* Deployed contract address and ABI
* Wallet connection
* Understanding of [contract functions](/developers/docs/aztec-nr/framework-description/functions/how_to_define_functions)

## Connect to a contractLet's say you've connected to a contract, for example:

```
import { Contract } from "@aztec/aztec.js";  
  
const contract = await Contract.at(contractAddress, artifact, wallet);
```

or

```
import { MyContract } from "./artifacts/MyContract";  
  
const contract = await MyContract.at(contractAddress, wallet);
```

## Simulate public functions## Step 1: Call a public view function```
const result = await contract.methods  
  .get_public_value(param1)  
  .simulate({ from: callerAddress }); // assuming callerAddress is already registered on the wallet, i.e. wallet.createSchnorrAccount(caller.secret, caller.salt)  
  
console.log("Public value:", result);
```

## Step 2: Handle return values```
const result = await contract.methods  
  .get_multiple_values()  
  .simulate({ from: callerAddress });  
  
// Destructure if returning multiple values  
const [value1, value2] = result;
```

## Simulate private functions## Step 1: Call a private view function```
const privateResult = await contract.methods  
  .get_private_balance(ownerAddress)  
  .simulate({ from: ownerAddress });
```

## Step 2: Access private notes```
// Private functions can access the caller's private state  
const notes = await contract.methods  
  .get_my_notes()  
  .simulate({ from: ownerAddress });
```

warning

Private simulations only work if the caller has access to the private state being queried.

## Simulate utility functions## Step 1: Call utility function```
const result = await contract.methods  
  .compute_value(input1, input2)  
  .simulate({ from: account.address });  
  
console.log("Computed value:", result);
```

## Step 2: Use utility functions for complex queries```
const aggregatedData = await contract.methods  
  .get_aggregated_stats(startBlock, endBlock)  
  .simulate({ from: account.address });  
  
// Returns structured data based on function signature  
console.log("Stats:", aggregatedData);
```

## Simulate with different contexts## Simulate from different addresses```
// Simulate as different users to test access control  
const asOwner = await contract.methods  
  .admin_function()  
  .simulate({ from: ownerAddress });  
  
try {  
  const asUser = await contract.methods  
    .admin_function()  
    .simulate({ from: userAddress });  
} catch (error) {  
  console.log("User cannot access admin function");  
}
```

## Next steps* [Send transactions](/developers/docs/aztec-js/how_to_send_transaction) to modify contract state
* Learn about [private and public functions](/developers/docs/aztec-nr/framework-description/functions/how_to_define_functions)
* Explore [testing patterns](/developers/docs/aztec-js/how_to_test) for simulations
* Understand [state management](/developers/docs/aztec-nr/framework-description/how_to_define_storage)

---


# Using Authentication Witnesses

Source: https://docs.aztec.network/developers/docs/aztec-js/how_to_use_authwit

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide shows you how to create and use authentication witnesses (authwits) to authorize other accounts to perform actions on your behalf.

aztec-nr

Using AuthWitnesses is always a two-part process. This guide shows how to generate and use them, but you still need to set up your contract to accept and authenticate them.

Therefore it is recommended to read the `aztec-nr` [guide on authwitnesses](/developers/docs/aztec-nr/framework-description/how_to_use_authwit) before this one.

## Prerequisites* Deployed account wallets
* Contract with authwit validation (see [smart contract authwits](/developers/docs/aztec-nr/framework-description/how_to_use_authwit))
* Understanding of [authwit concepts](/developers/docs/foundational-topics/advanced/authwit)

## AuthWitsLet's also assume we have a contract with functions `some_public_function` and `some_private_function` with the macro `#[authorize_once("from", "authwit_nonce")]`, meaning it will check if:

* `from` is `msg_sender`, or
* there's an authwitness allowing `from` to call this function

Regardless of its type, you'll want to define what is being delegated (let's call it "action") and the intent ("who intends to act"). For example:

```
const nonce = Fr.random()  
  
// bob creates an authwit that authorizes alice to call the function on his behalf  
const action = contract.methods.some_private_function(bob, 10n, nonce)  
const intent = {  
    caller: alice.address, // alice "intends" to call the function on bob's behalf  
    action  
};
```

tip

The nonce is necessary to avoid replay attacks. However, the contract is smart enough to allow bob to call the function himself by setting the nonce to `0`.

## Create private authwitsPrivate AuthWits mean that some action is authorized in private. No specific transaction is made, the authorization is just sent as part of the actual transaction:

```
const authWit = await wallet.createAuthWit(bob.address, intent);
```

Now alice can call the function by providing the authwit:

```
await action.send({ from: alice.address, authWitnesses: [authWit] }).wait();
```

## Create public authwitsPublic authwits mean the authorization is public, so it requires a transaction. You create the authwit just as above, but the wallet needs to authorize it in the canonical `AuthRegistry` contract:

```
// "true" is specific here... because you may want to revoke it later!  
const authwit = await wallet.setPublicAuthWit(bob.address, intent, true);  
await authwit.send({ from: bob.address }).wait()
```

Now that everyone knows about the public authorization, alice can call the function normally:

```
await action.send({ from: alice.address }).wait()
```

## Create arbitrary message authwitsThis is useful when you need to authorize arbitrary data rather than a specific contract function call. For example, authorizing a signature over a message for offchain verification.

## Step 1: Create inner hashYou can use `computeInnerAuthWitHash` to get yourself a hash of arbitrary hash you can use in an authwit:

```
import { computeInnerAuthWitHash, computeAuthWitMessageHash } from "@aztec/aztec.js";  
  
// Create hash of arbitrary data  
const innerHash = computeInnerAuthWitHash([  
    field1,  
    field2,  
    field3  
]);  
  
// Create full authwit message hash  
const messageHash = computeAuthWitMessageHash(  
    executorAddress,  
    chainId,  
    version,  
    innerHash  
);
```

## Revoke public authwitsBecause public authwits are... well, public, that means you should be able to revoke them. Just set the last parameter to `false` and send the transaction:

```
// Set authorized to false to revoke  
const revoked = await authorizerWallet.setPublicAuthWit({  
    caller: executorAddress,  
    action: action  
}, false).send({ from: account.address });
```

## Next steps* Learn about [authwits in smart contracts](/developers/docs/aztec-nr/framework-description/how_to_use_authwit)
* Understand [authwit concepts](/developers/docs/foundational-topics/advanced/authwit)
* Explore [account abstraction](/developers/docs/foundational-topics/accounts)
* Implement [cross-chain messaging](/developers/docs/aztec-nr/framework-description/how_to_communicate_cross_chain)

---


# Paying Fees

Source: https://docs.aztec.network/developers/docs/aztec-js/how_to_pay_fees

Version: Devnet (v3.0.0-devnet.20251212)

On this page

This guide walks you through paying transaction fees on Aztec using various payment methods.

## Prerequisites* Running Aztec local network
* Deployed account wallet
* Understanding of [fee concepts](/developers/docs/foundational-topics/fees)

info

The fee asset is only transferrable within a block to the current sequencer, as it powers the fee abstraction mechanism on Aztec. The asset is not transferable beyond this to ensure credible neutrality between all third party developer made asset portals and to ensure local compliance rules can be followed.

## Pay with Fee JuiceFee Juice is the native fee token on Aztec.

If your account has Fee Juice (for example, from a faucet), is [deployed](/developers/docs/aztec-js/how_to_create_account), and is registered in your wallet, it will be used automatically to pay for the fee of the transaction:

```
const tx = await contract.methods  
  .myFunction(param1, param2)  
  .send({  
    from: fundedAccount.address,  
    // no fee payment method needed  
  })  
  .wait();  
  
console.log("Transaction fee:", tx.transactionFee);
```

## Use Fee Payment ContractsFee Payment Contracts (FPC) pay fees on your behalf, typically accepting a different token than Fee Juice. Since Fee Juice is non-transferable on L2, FPCs are the most common fee payment method.

## Sponsored Fee Payment ContractsThe Sponsored FPC pays for fees unconditionally without requiring payment in return. It is available on both the local network and the testnet (deployed by Aztec Labs).

You can derive the Sponsored FPC address from its deployment parameters and salt (which defaults to `0`):

```
import { SponsoredFPCContract } from "@aztec/noir-contracts.js/SponsoredFPC";  
import { getContractInstanceFromInstantiationParams } from "@aztec/stdlib/contracts";  
import { SponsoredFeePaymentMethod } from "@aztec/aztec.js/fee/testing";  
  
const sponsoredFPCInstance = await getContractInstanceFromInstantiationParams(  
  SponsoredFPCContract.artifact,  
  {  
    salt: new Fr(0),  
  }  
);
```

Register the contract with your wallet before deploying and using it:

```
await wallet.registerContract(  
  sponsoredFPCInstance,  
  SponsoredFPCContract.artifact  
);  
const sponsoredPaymentMethod = new SponsoredFeePaymentMethod(  
  sponsoredFPCInstance.address  
);  
  
// deploy account for free  
const deployMethod = await yourAccount.getDeployMethod();  
const txHash = await deployMethod  
  .send({  
    from: AztecAddress.ZERO,  
    fee: { paymentMethod: sponsoredPaymentMethod },  
  })  
  .wait();
```

## Use other Fee Paying ContractsThird-party FPCs can pay for your fees using custom logic, such as accepting different tokens instead of Fee Juice.

## Set gas settings```
import { GasSettings } from "@aztec/stdlib/gas";  
  
const maxFeesPerGas = (await node.getCurrentBaseFees()).mul(1.5); //adjust this to your needs  
const gasSettings = GasSettings.default({ maxFeesPerGas });
```

Private FPCs enable fee payments without revealing the payer's identity onchain:

```
import { PrivateFeePaymentMethod } from "@aztec/aztec.js/fee";  
  
const paymentMethod = new PrivateFeePaymentMethod(  
  fpcAddress,  
  senderAddress,  
  wallet,  
  gasSettings  
);  
  
const tx = await contract.methods  
  .myFunction(param1)  
  .send({  
    from: wallet.address,  
    fee: {  
      paymentMethod,  
    },  
  })  
  .wait();
```

Public FPCs can be used in the same way:

```
import { PublicFeePaymentMethod } from "@aztec/aztec.js/fee";  
  
const paymentMethod = new PublicFeePaymentMethod(  
  fpcAddress,  
  senderAddress,  
  wallet,  
  gasSettings  
);
```

## Bridge Fee Juice from L1Fee Juice is non-transferable on L2, but you can bridge it from L1, claim it on L2, and use it. This involves a few components that are part of a running network's infrastructure:

* An L1 fee juice contract
* An L1 fee juice portal
* An L2 fee juice portal
* An L2 fee juice contract

`aztec.js` provides helpers to simplify the process:

```
// essentially returns an extended wallet from Viem  
import { createExtendedL1Client } from "@aztec/ethereum";  
const walletClient = createExtendedL1Client(  
  ["https://your-ethereum-host"], // ex. http://localhost:8545 on the local network (yes it runs Anvil under the hood)  
  privateKey // the private key for some account, needs funds for gas!  
);  
  
// a helper to interact with the L1 fee juice portal  
import { L1FeeJuicePortalManager } from "@aztec/aztec.js/ethereum";  
const portalManager = await L1FeeJuicePortalManager.new(  
  node, // your Aztec node, ex. https://aztec-testnet-fullnode.zkv.xyz, or http://localhost:8080 for local network  
  walletClient,  
  logger // a logger, ex. import { createLogger } from "@aztec/aztec.js"  
);
```

Under the hood, `L1FeeJuicePortalManager` gets the L1 addresses from the node `node_getNodeInfo` endpoint. It then exposes an easy method `bridgeTokensPublic` which mints fee juice on L1 and sends it to an L2 address via the L1 portal:

```
const claim = await portalManager.bridgeTokensPublic(  
  acc.address, // the L2 address  
  1000000000000000000000n, // the amount to send to the L1 portal  
  true // whether to mint or not (set to false if your walletClient account already has fee juice!)  
);  
  
console.log("Claim secret:", claim.claimSecret);  
console.log("Claim amount:", claim.claimAmount);
```

After this transaction is minted on L1 and a few blocks pass, you can claim the message on L2 and use it directly to pay for fees:

```
import { FeeJuicePaymentMethodWithClaim } from "@aztec/aztec.js/fee";  
const feeJuiceWithClaim = new FeeJuicePaymentMethodWithClaim(  
  acc.address,  
  claim  
); // the l2 address and the claim  
  
yourContract.methods  
  .some_method(acc.address)  
  .send({ from: acc.address, fee: { paymentMethod: feeJuiceWithClaim } })  
  .wait();
```

Creating blocks

To advance time quickly, send a couple of dummy transactions and `.wait()` for them. For example:

```
// using the `sponsoredFeePaymentMethod` so the network has transactions to build blocks with!  
await contract.methods  
  .some_other_method(acc.address)  
  .send({  
    from: acc.address,  
    fee: { paymentMethod: sponsoredFeePaymentMethod },  
  })  
  .wait();  
await contract.methods  
  .some_other_method(acc.address)  
  .send({  
    from: acc.address,  
    fee: { paymentMethod: sponsoredFeePaymentMethod },  
  })  
  .wait();
```

This will add a transaction to each block!

## Configure gas settings## Set custom gas limitsSet custom gas limits by importing from `stdlib`:

```
import { GasSettings, Gas, GasFees } from "@aztec/stdlib/gas";  
  
const gasSettings = new GasSettings(  
  new Gas(100000, 100000), // gasLimits (DA, L2)  
  new Gas(10000, 10000), // teardownGasLimits  
  new GasFees(10, 10), // maxFeesPerGas  
  new GasFees(1, 1) // maxPriorityFeesPerGas  
);  
  
const tx = await contract.methods  
  .myFunction()  
  .send({  
    from: wallet.address,  
    fee: {  
      paymentMethod,  
      gasSettings,  
    },  
  })  
  .wait();
```

## Use automatic gas estimation```
const tx = await contract.methods  
  .myFunction()  
  .send({  
    from: wallet.address,  
    fee: {  
      paymentMethod,  
      estimateGas: true,  
      estimatedGasPadding: 0.2, // 20% padding  
    },  
  })  
  .wait();
```

tip

Gas estimation runs a simulation first to determine actual gas usage, then adds padding for safety.

## Next steps* Learn about [fee concepts](/developers/docs/foundational-topics/fees) in detail
* Explore [authentication witnesses](/developers/docs/aztec-js/how_to_use_authwit) for delegated payments
* See [testing guide](/developers/docs/aztec-js/how_to_test) for fee testing strategies

---


# Testing Aztec.nr contracts with TypeScript

Source: https://docs.aztec.network/developers/docs/aztec-js/how_to_test

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Note: This section became completely stale so it got removed and will be rewritten.

## Further reading* [How to simulate functions in Aztec.js](/developers/docs/aztec-js/how_to_simulate_function)
* [How to send transactions in Aztec.js](/developers/docs/aztec-js/how_to_send_transaction)
* [How to deploy a contract in Aztec.js](/developers/docs/aztec-js/how_to_deploy_contract)
* [How to create an account in Aztec.js](/developers/docs/aztec-js/how_to_create_account)
* [How to compile a contract](/developers/docs/aztec-nr/how_to_compile_contract).

---


# Reference

Source: https://docs.aztec.network/developers/docs/aztec-js/aztec_js_reference

Version: Devnet (v3.0.0-devnet.20251212)

On this page

*This documentation is auto-generated from the Aztec.js TypeScript source code.*

info

This is an auto-generated reference. For tutorials and guides, see the [Aztec.js Guide](/developers/docs/aztec-js).

*Package: @aztec/aztec.js*

*Generated: 2025-12-10T22:27:41.987Z*

This document provides a comprehensive reference for all public APIs in the Aztec.js library.

Each section is organized by module, with classes, interfaces, types, and functions documented with their full signatures, parameters, and return types.

## Table of Contents* [Account](#account)
  + [AccountContract](#accountcontract)
  + [getAccountContractAddress](#getaccountcontractaddress)
  + [AccountWithSecretKey](#accountwithsecretkey)
  + [Account](#account)
  + [BaseAccount](#baseaccount)
  + [AccountInterface](#accountinterface)
  + [SignerlessAccount](#signerlessaccount)
* [Authorization](#authorization)
  + [CallAuthorizationRequest](#callauthorizationrequest)
* [Contract](#contract)
  + [BaseContractInteraction](#basecontractinteraction)
  + [BatchCall](#batchcall)
  + [abiChecker](#abichecker)
  + [ContractMethod](#contractmethod)
  + [ContractStorageLayout](#contractstoragelayout)
  + [ContractBase](#contractbase)
  + [ContractFunctionInteraction](#contractfunctioninteraction)
  + [Contract](#contract)
  + [RequestDeployOptions](#requestdeployoptions)
  + [DeployOptions](#deployoptions)
  + [SimulateDeployOptions](#simulatedeployoptions)
  + [DeployMethod](#deploymethod)
  + [DeployedWaitOpts](#deployedwaitopts)
  + [DeployTxReceipt](#deploytxreceipt)
  + [DeploySentTx](#deploysenttx)
  + [getGasLimits](#getgaslimits)
  + [FeeEstimationOptions](#feeestimationoptions)
  + [FeePaymentMethodOption](#feepaymentmethodoption)
  + [GasSettingsOption](#gassettingsoption)
  + [InteractionFeeOptions](#interactionfeeoptions)
  + [SimulationInteractionFeeOptions](#simulationinteractionfeeoptions)
  + [RequestInteractionOptions](#requestinteractionoptions)
  + [SendInteractionOptions](#sendinteractionoptions)
  + [SimulateInteractionOptions](#simulateinteractionoptions)
  + [ProfileInteractionOptions](#profileinteractionoptions)
  + [SimulationReturn](#simulationreturn)
  + [toSendOptions](#tosendoptions)
  + [toSimulateOptions](#tosimulateoptions)
  + [toProfileOptions](#toprofileoptions)
  + [getClassRegistryContract](#getclassregistrycontract)
  + [getInstanceRegistryContract](#getinstanceregistrycontract)
  + [getFeeJuice](#getfeejuice)
  + [WaitOpts](#waitopts)
  + [DefaultWaitOpts](#defaultwaitopts)
  + [SentTx](#senttx)
  + [UnsafeContract](#unsafecontract)
  + [WaitForProvenOpts](#waitforprovenopts)
  + [DefaultWaitForProvenOpts](#defaultwaitforprovenopts)
  + [waitForProven](#waitforproven)
* [Deployment](#deployment)
  + [broadcastPrivateFunction](#broadcastprivatefunction)
  + [broadcastUtilityFunction](#broadcastutilityfunction)
  + [ContractDeployer](#contractdeployer)
  + [publishContractClass](#publishcontractclass)
  + [publishInstance](#publishinstance)
* [Ethereum](#ethereum)
  + [L2Claim](#l2claim)
  + [L2AmountClaim](#l2amountclaim)
  + [L2AmountClaimWithRecipient](#l2amountclaimwithrecipient)
  + [generateClaimSecret](#generateclaimsecret)
  + [L1TokenManager](#l1tokenmanager)
  + [L1FeeJuicePortalManager](#l1feejuiceportalmanager)
  + [L1ToL2TokenPortalManager](#l1tol2tokenportalmanager)
  + [L1TokenPortalManager](#l1tokenportalmanager)
* [Fee](#fee)
  + [FeeJuicePaymentMethodWithClaim](#feejuicepaymentmethodwithclaim)
  + [FeePaymentMethod](#feepaymentmethod)
  + [PrivateFeePaymentMethod](#privatefeepaymentmethod)
  + [PublicFeePaymentMethod](#publicfeepaymentmethod)
  + [SponsoredFeePaymentMethod](#sponsoredfeepaymentmethod)
* [Utils](#utils)
  + [FieldLike](#fieldlike)
  + [EthAddressLike](#ethaddresslike)
  + [AztecAddressLike](#aztecaddresslike)
  + [FunctionSelectorLike](#functionselectorlike)
  + [EventSelectorLike](#eventselectorlike)
  + [U128Like](#u128like)
  + [WrappedFieldLike](#wrappedfieldlike)
  + [IntentInnerHash](#intentinnerhash)
  + [CallIntent](#callintent)
  + [ContractFunctionInteractionCallIntent](#contractfunctioninteractioncallintent)
  + [computeAuthWitMessageHash](#computeauthwitmessagehash)
  + [getMessageHashFromIntent](#getmessagehashfromintent)
  + [computeInnerAuthWitHashFromAction](#computeinnerauthwithashfromaction)
  + [lookupValidity](#lookupvalidity)
  + [SetPublicAuthwitContractInteraction](#setpublicauthwitcontractinteraction)
  + [waitForL1ToL2MessageReady](#waitforl1tol2messageready)
  + [isL1ToL2MessageReady](#isl1tol2messageready)
  + [getFeeJuiceBalance](#getfeejuicebalance)
  + [readFieldCompressedString](#readfieldcompressedstring)
  + [waitForNode](#waitfornode)
  + [createAztecNodeClient](#createaztecnodeclient)
  + [AztecNode](#aztecnode)
  + [generatePublicKey](#generatepublickey)
* [Wallet](#wallet)
  + [AccountEntrypointMetaPaymentMethod](#accountentrypointmetapaymentmethod)
  + [AccountManager](#accountmanager)
  + [RequestDeployAccountOptions](#requestdeployaccountoptions)
  + [DeployAccountOptions](#deployaccountoptions)
  + [SimulateDeployAccountOptions](#simulatedeployaccountoptions)
  + [DeployAccountMethod](#deployaccountmethod)
  + [Aliased](#aliased)
  + [SimulateOptions](#simulateoptions)
  + [ProfileOptions](#profileoptions)
  + [SendOptions](#sendoptions)
  + [BatchableMethods](#batchablemethods)
  + [BatchedMethod](#batchedmethod)
  + [BatchedMethodResult](#batchedmethodresult)
  + [BatchedMethodResultWrapper](#batchedmethodresultwrapper)
  + [BatchResults](#batchresults)
  + [PrivateEventFilter](#privateeventfilter)
  + [PrivateEvent](#privateevent)
  + [Wallet](#wallet)
  + [FunctionCallSchema](#functioncallschema)
  + [ExecutionPayloadSchema](#executionpayloadschema)
  + [GasSettingsOptionSchema](#gassettingsoptionschema)
  + [WalletSimulationFeeOptionSchema](#walletsimulationfeeoptionschema)
  + [SendOptionsSchema](#sendoptionsschema)
  + [SimulateOptionsSchema](#simulateoptionsschema)
  + [ProfileOptionsSchema](#profileoptionsschema)
  + [MessageHashOrIntentSchema](#messagehashorintentschema)
  + [BatchedMethodSchema](#batchedmethodschema)
  + [ContractMetadataSchema](#contractmetadataschema)
  + [ContractClassMetadataSchema](#contractclassmetadataschema)
  + [EventMetadataDefinitionSchema](#eventmetadatadefinitionschema)
  + [PrivateEventSchema](#privateeventschema)
  + [PrivateEventFilterSchema](#privateeventfilterschema)
  + [WalletSchema](#walletschema)

---

## Account---

## `account/account_contract.ts`## AccountContract**Type:** Interface

An account contract instance. Knows its artifact, deployment arguments, how to create transaction execution requests out of function calls, and how to authorize actions.

## Methods## getContractArtifactReturns the artifact of this account contract.

**Signature:**

```
getContractArtifact(): Promise<ContractArtifact>
```

**Returns:**

`Promise<ContractArtifact>`

## getInitializationFunctionAndArgsReturns the initializer function name and arguments for this instance, or undefined if this contract does not require initialization.

**Signature:**

```
getInitializationFunctionAndArgs(): Promise<{  
      constructorName: string;  
      constructorArgs: any[];  
  } | undefined>
```

**Returns:**

```
Promise<  
    | {  
        /** The name of the function used to initialize the contract */  
        constructorName: string;  
        /** The args to the function used to initialize the contract */  
        constructorArgs: any[];  
      }  
    | undefined  
  >
```

## getInterfaceReturns the account interface for this account contract given an instance at the provided address. The account interface is responsible for assembling tx requests given requested function calls, and for creating signed auth witnesses given action identifiers (message hashes).

**Signature:**

```
getInterface(  
  address: CompleteAddress,  
  chainInfo: ChainInfo  
): AccountInterface
```

**Parameters:**

* `address`: `CompleteAddress`
  + Address of this account contract.
* `chainInfo`: `ChainInfo`
  + Chain id and version of the rollup where the account contract is initialized / published.

**Returns:**

`AccountInterface` - An account interface instance for creating tx requests and authorizing actions.

## getAuthWitnessProviderReturns the auth witness provider for the given address.

**Signature:**

```
getAuthWitnessProvider(address: CompleteAddress): AuthWitnessProvider
```

**Parameters:**

* `address`: `CompleteAddress`
  + Address for which to create auth witnesses.

**Returns:**

`AuthWitnessProvider`

## getAccountContractAddress**Type:** Function

Compute the address of an account contract from secret and salt.

**Signature:**

```
export async getAccountContractAddress(  
  accountContract: AccountContract,  
  secret: Fr,  
  salt: Fr  
)
```

**Parameters:**

* `accountContract`: `AccountContract`
* `secret`: `Fr`
* `salt`: `Fr`

**Returns:**

`Promise<any>`

---

## `account/account_with_secret_key.ts`## AccountWithSecretKey**Type:** Class

Extends Account with the encryption private key. Not required for implementing the wallet interface but useful for testing purposes or exporting an account to another pxe.

**Extends:** `BaseAccount`

## Constructor**Signature:**

```
constructor(  
  account: AccountInterface,  
  private secretKey: Fr,  
  public readonly salt: Salt  
)
```

**Parameters:**

* `account`: `AccountInterface`
* `secretKey`: `Fr`
* `salt`: `Salt`
  + Deployment salt for this account contract.

## Methods## getSecretKeyReturns the encryption private key associated with this account.

**Signature:**

```
public getSecretKey()
```

**Returns:**

`Fr`

## getEncryptionSecretReturns the encryption secret, the secret of the encryption point—the point that others use to encrypt messages to this account note - this ensures that the address secret always corresponds to an address point with y being positive dev - this is also referred to as the address secret, which decrypts payloads encrypted to an address point

**Signature:**

```
public async getEncryptionSecret()
```

**Returns:**

`Promise<any>`

---

## `account/account.ts`## Account**Type:** Type Alias

A type defining an account, capable of both creating authwits and using them to authenticate transaction execution requests.

**Signature:**

```
export type Account = AccountInterface & AuthwitnessIntentProvider;
```

## BaseAccount**Type:** Class

An account implementation that uses authwits as an authentication mechanism and can assemble transaction execution requests for an entrypoint.

**Implements:** `Account`

## Constructor**Signature:**

```
constructor(protected account: AccountInterface)
```

**Parameters:**

* `account`: `AccountInterface`

## Methods## createTxExecutionRequest**Signature:**

```
createTxExecutionRequest(  
  exec: ExecutionPayload,  
  gasSettings: GasSettings,  
  options: DefaultAccountEntrypointOptions  
): Promise<TxExecutionRequest>
```

**Parameters:**

* `exec`: `ExecutionPayload`
* `gasSettings`: `GasSettings`
* `options`: `DefaultAccountEntrypointOptions`

**Returns:**

`Promise<TxExecutionRequest>`

## getChainId**Signature:**

```
getChainId(): Fr
```

**Returns:**

`Fr`

## getVersion**Signature:**

```
getVersion(): Fr
```

**Returns:**

`Fr`

## getCompleteAddressReturns the complete address of the account that implements this wallet.

**Signature:**

```
public getCompleteAddress()
```

**Returns:**

`CompleteAddress`

## getAddressReturns the address of the account that implements this wallet.

**Signature:**

```
public getAddress()
```

**Returns:**

`any`

## createAuthWitComputes an authentication witness from either a message hash or an intent. If a message hash is provided, it will create a witness for the hash directly. Otherwise, it will compute the message hash using the intent, along with the chain id and the version values provided by the wallet.

**Signature:**

```
async createAuthWit(messageHashOrIntent: Fr | Buffer | CallIntent | IntentInnerHash): Promise<AuthWitness>
```

**Parameters:**

* `messageHashOrIntent`: `Fr | Buffer | CallIntent | IntentInnerHash`
  + The message hash of the intent to approve

**Returns:**

`Promise<AuthWitness>` - The authentication witness

---

## `account/interface.ts`## AccountInterface**Type:** Interface

Handler for interfacing with an account. Knows how to create transaction execution requests and authorize actions for its corresponding account.

**Extends:** `EntrypointInterface`, `AuthWitnessProvider`

## Methods## getCompleteAddressReturns the complete address for this account.

**Signature:**

```
getCompleteAddress(): CompleteAddress
```

**Returns:**

`CompleteAddress`

## getAddressReturns the address for this account.

**Signature:**

```
getAddress(): AztecAddress
```

**Returns:**

`AztecAddress`

## getChainIdReturns the chain id for this account

**Signature:**

```
getChainId(): Fr
```

**Returns:**

`Fr`

## getVersionReturns the rollup version for this account

**Signature:**

```
getVersion(): Fr
```

**Returns:**

`Fr`

---

## `account/signerless_account.ts`## SignerlessAccount**Type:** Class

Account implementation which creates a transaction using the multicall protocol contract as entrypoint.

**Implements:** `Account`

## Constructor**Signature:**

```
constructor(chainInfo: ChainInfo)
```

**Parameters:**

* `chainInfo`: `ChainInfo`

## Methods## createTxExecutionRequest**Signature:**

```
createTxExecutionRequest(  
  exec: ExecutionPayload,  
  gasSettings: GasSettings  
): Promise<TxExecutionRequest>
```

**Parameters:**

* `exec`: `ExecutionPayload`
* `gasSettings`: `GasSettings`

**Returns:**

`Promise<TxExecutionRequest>`

## getChainId**Signature:**

```
getChainId(): Fr
```

**Returns:**

`Fr`

## getVersion**Signature:**

```
getVersion(): Fr
```

**Returns:**

`Fr`

## getCompleteAddress**Signature:**

```
getCompleteAddress(): CompleteAddress
```

**Returns:**

`CompleteAddress`

## getAddress**Signature:**

```
getAddress(): AztecAddress
```

**Returns:**

`AztecAddress`

## createAuthWit**Signature:**

```
createAuthWit(_intent: Fr | Buffer | IntentInnerHash | CallIntent): Promise<AuthWitness>
```

**Parameters:**

* `_intent`: `Fr | Buffer | IntentInnerHash | CallIntent`

**Returns:**

`Promise<AuthWitness>`

## Authorization---

## `authorization/call_authorization_request.ts`## CallAuthorizationRequest**Type:** Class

An authwit request for a function call. Includes the preimage of the data to be signed, as opposed of just the inner hash.

## Constructor**Signature:**

```
constructor(  
  public selector: AuthorizationSelector,  
  public innerHash: Fr,  
  public msgSender: AztecAddress,  
  public functionSelector: FunctionSelector,  
  public argsHash: Fr,  
  public args: Fr[]  
)
```

**Parameters:**

* `selector`: `AuthorizationSelector`
  + The selector of the authwit type, used to identify it when emitted from `emit_offchain_effect`oracle. Computed as poseidon2("CallAuthwit((Field),(u32),Field)".to\_bytes())
* `innerHash`: `Fr`
  + The inner hash of the authwit, computed as poseidon2([msg\_sender, selector, args\_hash])
* `msgSender`: `AztecAddress`
  + The address performing the call
* `functionSelector`: `FunctionSelector`
  + The selector of the function that is to be authorized
* `argsHash`: `Fr`
  + The hash of the arguments to the function call,
* `args`: `Fr[]`
  + The arguments to the function call.

## Methods## getSelector**Signature:**

```
static getSelector(): Promise<AuthorizationSelector>
```

**Returns:**

`Promise<AuthorizationSelector>`

## fromFields**Signature:**

```
static async fromFields(fields: Fr[]): Promise<CallAuthorizationRequest>
```

**Parameters:**

* `fields`: `Fr[]`

**Returns:**

`Promise<CallAuthorizationRequest>`

## Contract---

## `contract/base_contract_interaction.ts`## BaseContractInteraction**Type:** Class

Base class for an interaction with a contract, be it a deployment, a function call, or a batch. Implements the sequence create/simulate/send.

## Constructor**Signature:**

```
constructor(  
  protected wallet: Wallet,  
  protected authWitnesses: AuthWitness[] = [],  
  protected capsules: Capsule[] = []  
)
```

**Parameters:**

* `wallet`: `Wallet`
* `authWitnesses` (optional): `AuthWitness[]`
* `capsules` (optional): `Capsule[]`

## Properties## log**Type:** `any`

## Methods## requestReturns an execution request that represents this operation. Can be used as a building block for constructing batch requests.

**Signature:**

```
public abstract request(options?: RequestInteractionOptions): Promise<ExecutionPayload>
```

**Parameters:**

* `options` (optional): `RequestInteractionOptions`
  + An optional object containing additional configuration for the transaction.

**Returns:**

`Promise<ExecutionPayload>` - An execution request wrapped in promise.

## sendSends a transaction to the contract function with the specified options. This function throws an error if called on a utility function. It creates and signs the transaction if necessary, and returns a SentTx instance, which can be used to track the transaction status, receipt, and events.

**Signature:**

```
public send(options: SendInteractionOptions): SentTx
```

**Parameters:**

* `options`: `SendInteractionOptions`
  + An object containing 'from' property representing the AztecAddress of the sender and optional fee configuration

**Returns:**

`SentTx` - A SentTx instance for tracking the transaction status and information.

---

## `contract/batch_call.ts`## BatchCall**Type:** Class

A batch of function calls to be sent as a single transaction through a wallet.

**Extends:** `BaseContractInteraction`

## Constructor**Signature:**

```
constructor(  
  wallet: Wallet,  
  protected interactions: (BaseContractInteraction | ExecutionPayload)[]  
)
```

**Parameters:**

* `wallet`: `Wallet`
* `interactions`: `(BaseContractInteraction | ExecutionPayload)[]`

## Methods## requestReturns an execution request that represents this operation.

**Signature:**

```
public async request(options: RequestInteractionOptions = {}): Promise<ExecutionPayload>
```

**Parameters:**

* `options` (optional): `RequestInteractionOptions`
  + An optional object containing additional configuration for the request generation.

**Returns:**

`Promise<ExecutionPayload>` - An execution payload wrapped in promise.

## simulateSimulates the batch, supporting private, public and utility functions. Although this is a single interaction with the wallet, private and public functions will be grouped into a single ExecutionPayload that the wallet will simulate as a single transaction. Utility function calls will simply be executed one by one.

**Signature:**

```
public async simulate(options: SimulateInteractionOptions): Promise<any>
```

**Parameters:**

* `options`: `SimulateInteractionOptions`
  + An optional object containing additional configuration for the interaction.

**Returns:**

`Promise<any>` - The results of all the interactions that make up the batch

## getExecutionPayloads**Signature:**

```
protected async getExecutionPayloads(): Promise<ExecutionPayload[]>
```

**Returns:**

`Promise<ExecutionPayload[]>`

---

## `contract/checker.ts`## abiChecker**Type:** Function

Validates the given ContractArtifact object by checking its functions and their parameters. Ensures that the ABI has at least one function, a constructor, valid bytecode, and correct parameter types. Throws an error if any inconsistency is detected during the validation process.

**Signature:**

```
export abiChecker(artifact: ContractArtifact)
```

**Parameters:**

* `artifact`: `ContractArtifact`
  + The ContractArtifact object to be validated.

**Returns:**

`boolean` - A boolean value indicating whether the artifact is valid or not.

---

## `contract/contract_base.ts`## ContractMethod**Type:** Type Alias

Type representing a contract method that returns a ContractFunctionInteraction instance and has a readonly 'selector' property of type Buffer. Takes any number of arguments.

**Signature:**

```
export type ContractMethod = ((...args: any[]) => ContractFunctionInteraction) & {  
 selector: () => Promise<FunctionSelector>;  
};
```

**Type Members:**

## selectorThe unique identifier for a contract function in bytecode.

**Type:** `() => Promise<FunctionSelector>`

## ContractStorageLayout**Type:** Type Alias

Type representing the storage layout of a contract.

**Signature:**

```
export type ContractStorageLayout<T extends string> = {  
 [K in T]: FieldLayout;  
};
```

**Type Members:**

## [K in T]**Signature:** `[K in T]: FieldLayout`

**Key Type:** `T`

**Value Type:** `FieldLayout`

## ContractBase**Type:** Class

Abstract implementation of a contract extended by the Contract class and generated contract types.

## Constructor**Signature:**

```
protected constructor(  
  public readonly address: AztecAddress,  
  public readonly artifact: ContractArtifact,  
  public wallet: Wallet  
)
```

**Parameters:**

* `address`: `AztecAddress`
  + The contract's address.
* `artifact`: `ContractArtifact`
  + The Application Binary Interface for the contract.
* `wallet`: `Wallet`
  + The wallet used for interacting with this contract.

## Properties## methodsAn object containing contract methods mapped to their respective names.

**Type:** `{ [name: string]: ContractMethod }`

## Methods## withWalletCreates a new instance of the contract wrapper attached to a different wallet.

**Signature:**

```
public withWallet(wallet: Wallet): this
```

**Parameters:**

* `wallet`: `Wallet`
  + Wallet to use for sending txs.

**Returns:**

`this` - A new contract instance.

---

## `contract/contract_function_interaction.ts`## ContractFunctionInteraction**Type:** Class

This is the class that is returned when calling e.g. `contract.methods.myMethod(arg0, arg1)`. It contains available interactions one can call on a method, including view.

**Extends:** `BaseContractInteraction`

## Constructor**Signature:**

```
constructor(  
  wallet: Wallet,  
  protected contractAddress: AztecAddress,  
  protected functionDao: FunctionAbi,  
  protected args: any[],  
  authWitnesses: AuthWitness[] = [],  
  capsules: Capsule[] = [],  
  private extraHashedArgs: HashedValues[] = []  
)
```

**Parameters:**

* `wallet`: `Wallet`
* `contractAddress`: `AztecAddress`
* `functionDao`: `FunctionAbi`
* `args`: `any[]`
* `authWitnesses` (optional): `AuthWitness[]`
* `capsules` (optional): `Capsule[]`
* `extraHashedArgs` (optional): `HashedValues[]`

## Methods## getFunctionCallReturns the encoded function call wrapped by this interaction Useful when generating authwits

**Signature:**

```
public async getFunctionCall()
```

**Returns:**

`Promise<{ name: any; args: any; selector: any; type: any; to: AztecAddress; isStatic: any; hideMsgSender: boolean; returnTypes: any; }>` - An encoded function call

## requestReturns the execution payload that allows this operation to happen on chain.

**Signature:**

```
public override async request(options: RequestInteractionOptions = {}): Promise<ExecutionPayload>
```

**Parameters:**

* `options` (optional): `RequestInteractionOptions`
  + Configuration options.

**Returns:**

`Promise<ExecutionPayload>` - The execution payload for this operation

## simulateSimulate a transaction and get information from its execution. Differs from prove in a few important ways: 1. It returns the values of the function execution, plus additional metadata if requested 2. It supports `utility`, `private` and `public` functions

**Signature:**

```
public async simulate<T extends SimulateInteractionOptions>(options: T): Promise<SimulationReturn<Exclude<T['fee'], undefined>['estimateGas']>>
```

**Parameters:**

* `options`: `T`
  + An optional object containing additional configuration for the simulation.

**Returns:**

`Promise<SimulationReturn<Exclude<T['fee'], undefined>['estimateGas']>>` - Depending on the simulation options, this method directly returns the result value of the executed function or a rich object containing extra metadata, such as estimated gas costs (if requested via options), execution statistics and emitted offchain effects

## simulate**Signature:**

```
public async simulate<T extends SimulateInteractionOptions>(options: T): Promise<SimulationReturn<T['includeMetadata']>>
```

**Parameters:**

* `options`: `T`

**Returns:**

`Promise<SimulationReturn<T['includeMetadata']>>`

## simulate**Signature:**

```
public async simulate(options: SimulateInteractionOptions): Promise<SimulationReturn<typeof options.includeMetadata>>
```

**Parameters:**

* `options`: `SimulateInteractionOptions`

**Returns:**

`Promise<SimulationReturn<typeof options.includeMetadata>>`

## profileSimulate a transaction and profile the gate count for each function in the transaction.

**Signature:**

```
public async profile(options: ProfileInteractionOptions): Promise<TxProfileResult>
```

**Parameters:**

* `options`: `ProfileInteractionOptions`
  + Same options as `simulate`, plus profiling method

**Returns:**

`Promise<TxProfileResult>` - An object containing the function return value and profile result.

## withAugments this ContractFunctionInteraction with additional metadata, such as authWitnesses, capsules, and extraHashedArgs. This is useful when creating a "batteries included" interaction, such as registering a contract class with its associated capsule instead of having the user provide them externally.

**Signature:**

```
public with({ authWitnesses = [], capsules = [], extraHashedArgs = [], }: {  
    authWitnesses?: AuthWitness[];  
    capsules?: Capsule[];  
    extraHashedArgs?: HashedValues[];  
}): ContractFunctionInteraction
```

**Parameters:**

* `{ authWitnesses = [], capsules = [], extraHashedArgs = [], }`: `{ /** The authWitnesses to add to the interaction */ authWitnesses?: AuthWitness[]; /** The capsules to add to the interaction */ capsules?: Capsule[]; /** The extra hashed args to add to the interaction */ extraHashedArgs?: HashedValues[]; }`

**Returns:**

`ContractFunctionInteraction` - A new ContractFunctionInteraction with the added metadata, but calling the same original function in the same manner

---

## `contract/contract.ts`## Contract**Type:** Class

The Contract class represents a contract and provides utility methods for interacting with it. It enables the creation of ContractFunctionInteraction instances for each function in the contract's ABI, allowing users to call or send transactions to these functions. Additionally, the Contract class can be used to attach the contract instance to a deployed contract onchain through the PXE, which facilitates interaction with Aztec's privacy protocol.

**Extends:** `ContractBase`

## Methods## atGets a contract instance.

**Signature:**

```
public static at(  
  address: AztecAddress,  
  artifact: ContractArtifact,  
  wallet: Wallet  
): Contract
```

**Parameters:**

* `address`: `AztecAddress`
  + The address of the contract instance.
* `artifact`: `ContractArtifact`
  + Build artifact of the contract.
* `wallet`: `Wallet`
  + The wallet to use when interacting with the contract.

**Returns:**

`Contract` - A promise that resolves to a new Contract instance.

## deployCreates a tx to deploy (initialize and/or publish) a new instance of a contract.

**Signature:**

```
public static deploy(  
  wallet: Wallet,  
  artifact: ContractArtifact,  
  args: any[],  
  constructorName?: string  
)
```

**Parameters:**

* `wallet`: `Wallet`
  + The wallet for executing the deployment.
* `artifact`: `ContractArtifact`
  + Build artifact of the contract to deploy
* `args`: `any[]`
  + Arguments for the constructor.
* `constructorName` (optional): `string`
  + The name of the constructor function to call.

**Returns:**

`DeployMethod<Contract>`

## deployWithPublicKeysCreates a tx to deploy (initialize and/or publish) a new instance of a contract using the specified public keys hash to derive the address.

**Signature:**

```
public static deployWithPublicKeys(  
  publicKeys: PublicKeys,  
  wallet: Wallet,  
  artifact: ContractArtifact,  
  args: any[],  
  constructorName?: string  
)
```

**Parameters:**

* `publicKeys`: `PublicKeys`
  + Hash of public keys to use for deriving the address.
* `wallet`: `Wallet`
  + The wallet for executing the deployment.
* `artifact`: `ContractArtifact`
  + Build artifact of the contract.
* `args`: `any[]`
  + Arguments for the constructor.
* `constructorName` (optional): `string`
  + The name of the constructor function to call.

**Returns:**

`DeployMethod<Contract>`

---

## `contract/deploy_method.ts`## RequestDeployOptions**Type:** Type Alias

Options for deploying a contract on the Aztec network. Allows specifying a contract address salt and different options to tweak contract publication and initialization

**Signature:**

```
export type RequestDeployOptions = RequestInteractionOptions & {  
 contractAddressSalt?: Fr;  
 deployer?: AztecAddress;  
 skipClassPublication?: boolean;  
 skipInstancePublication?: boolean;  
 skipInitialization?: boolean;  
 skipRegistration?: boolean;  
};
```

**Type Members:**

## contractAddressSaltAn optional salt value used to deterministically calculate the contract address.

**Type:** `Fr`

## deployerDeployer address that will be used for the deployed contract's address computation. If set to 0, the sender's address won't be mixed in

**Type:** `AztecAddress`

## skipClassPublicationSkip contract class publication.

**Type:** `boolean`

## skipInstancePublicationSkip publication, instead just privately initialize the contract.

**Type:** `boolean`

## skipInitializationSkip contract initialization.

**Type:** `boolean`

## skipRegistrationSkip contract registration in the wallet

**Type:** `boolean`

## DeployOptions**Type:** Type Alias

Extends the deployment options with the required parameters to send the transaction

**Signature:**

```
export type DeployOptions = Omit<RequestDeployOptions, 'deployer'> & {  
 universalDeploy?: boolean;  
} & Pick<SendInteractionOptions, 'from' | 'fee'>;
```

**Type Members:**

## universalDeploySet to true to *not* include the sender in the address computation. This option is mutually exclusive with "deployer"

**Type:** `boolean`

## SimulateDeployOptions**Type:** Type Alias

Options for simulating the deployment of a contract Allows skipping certain validations and computing gas estimations

**Signature:**

```
export type SimulateDeployOptions = Omit<DeployOptions, 'fee'> & {  
 fee?: SimulationInteractionFeeOptions;  
 skipTxValidation?: boolean;  
 skipFeeEnforcement?: boolean;  
 includeMetadata?: boolean;  
};
```

**Type Members:**

## feeThe fee options for the transaction.

**Type:** `SimulationInteractionFeeOptions`

## skipTxValidationSimulate without checking for the validity of the resulting transaction, e.g. whether it emits any existing nullifiers.

**Type:** `boolean`

## skipFeeEnforcementWhether to ensure the fee payer is not empty and has enough balance to pay for the fee.

**Type:** `boolean`

## includeMetadataWhether to include metadata such as offchain effects and performance statistics (e.g. timing information of the different circuits and oracles) in the simulation result, instead of just the return value of the function

**Type:** `boolean`

## DeployMethod**Type:** Class

Contract interaction for deployment. Handles class publication, instance publication, and initialization of the contract. Note that for some contracts, a tx is not required as part of its "creation": If there are no public functions, and if there are no initialization functions, then technically the contract has already been "created", and all of the contract's functions (private and utility) can be interacted-with immediately, without any "deployment tx". Extends the BaseContractInteraction class.

**Extends:** `BaseContractInteraction`

## Constructor**Signature:**

```
constructor(  
  private publicKeys: PublicKeys,  
  wallet: Wallet,  
  protected artifact: ContractArtifact,  
  protected postDeployCtor: (instance: ContractInstanceWithAddress, wallet: Wallet) => TContract,  
  private args: any[] = [],  
  constructorNameOrArtifact?: string | FunctionArtifact,  
  authWitnesses: AuthWitness[] = [],  
  capsules: Capsule[] = []  
)
```

**Parameters:**

* `publicKeys`: `PublicKeys`
* `wallet`: `Wallet`
* `artifact`: `ContractArtifact`
* `postDeployCtor`: `(instance: ContractInstanceWithAddress, wallet: Wallet) => TContract`
* `args` (optional): `any[]`
* `constructorNameOrArtifact` (optional): `string | FunctionArtifact`
* `authWitnesses` (optional): `AuthWitness[]`
* `capsules` (optional): `Capsule[]`

## Methods## requestReturns the execution payload that allows this operation to happen on chain.

**Signature:**

```
public async request(options?: RequestDeployOptions): Promise<ExecutionPayload>
```

**Parameters:**

* `options` (optional): `RequestDeployOptions`
  + Configuration options.

**Returns:**

`Promise<ExecutionPayload>` - The execution payload for this operation

## convertDeployOptionsToRequestOptions**Signature:**

```
convertDeployOptionsToRequestOptions(options: DeployOptions): RequestDeployOptions
```

**Parameters:**

* `options`: `DeployOptions`

**Returns:**

`RequestDeployOptions`

## registerAdds this contract to the wallet and returns the Contract object.

**Signature:**

```
public async register(options?: RequestDeployOptions): Promise<TContract>
```

**Parameters:**

* `options` (optional): `RequestDeployOptions`
  + Deployment options.

**Returns:**

`Promise<TContract>`

## getPublicationExecutionPayloadReturns an execution payload for: - publication of the contract class and - publication of the contract instance to enable public execution depending on the provided options.

**Signature:**

```
protected async getPublicationExecutionPayload(options?: RequestDeployOptions): Promise<ExecutionPayload>
```

**Parameters:**

* `options` (optional): `RequestDeployOptions`
  + Contract creation options.

**Returns:**

`Promise<ExecutionPayload>` - An execution payload with potentially calls (and bytecode capsule) to the class registry and instance registry.

## getInitializationExecutionPayloadReturns the calls necessary to initialize the contract.

**Signature:**

```
protected async getInitializationExecutionPayload(options?: RequestDeployOptions): Promise<ExecutionPayload>
```

**Parameters:**

* `options` (optional): `RequestDeployOptions`
  + Deployment options.

**Returns:**

`Promise<ExecutionPayload>` - An array of function calls.

## sendSend a contract deployment transaction (initialize and/or publish) using the provided options. This function extends the 'send' method from the ContractFunctionInteraction class, allowing us to send a transaction specifically for contract deployment.

**Signature:**

```
public override send(options: DeployOptions): DeploySentTx<TContract>
```

**Parameters:**

* `options`: `DeployOptions`
  + An object containing various deployment options such as contractAddressSalt and from.

**Returns:**

`DeploySentTx<TContract>` - A SentTx object that returns the receipt and the deployed contract instance.

## getInstanceBuilds the contract instance and returns it.

**Signature:**

```
public async getInstance(options?: RequestDeployOptions): Promise<ContractInstanceWithAddress>
```

**Parameters:**

* `options` (optional): `RequestDeployOptions`
  + An object containing various initialization and publication options.

**Returns:**

`Promise<ContractInstanceWithAddress>` - An instance object.

## simulateSimulate the deployment

**Signature:**

```
public async simulate(options: SimulateDeployOptions): Promise<SimulationReturn<true>>
```

**Parameters:**

* `options`: `SimulateDeployOptions`
  + An optional object containing additional configuration for the simulation.

**Returns:**

`Promise<SimulationReturn<true>>` - A simulation result object containing metadata of the execution, including gas estimations (if requested via options), execution statistics and emitted offchain effects

## profileSimulate a deployment and profile the gate count for each function in the transaction.

**Signature:**

```
public async profile(options: DeployOptions & ProfileInteractionOptions): Promise<TxProfileResult>
```

**Parameters:**

* `options`: `DeployOptions & ProfileInteractionOptions`
  + Same options as `send`, plus extra profiling options.

**Returns:**

`Promise<TxProfileResult>` - An object containing the function return value and profile result.

## withAugments this DeployMethod with additional metadata, such as authWitnesses and capsules.

**Signature:**

```
public with({ authWitnesses = [], capsules = [], }: {  
    authWitnesses?: AuthWitness[];  
    capsules?: Capsule[];  
}): DeployMethod
```

**Parameters:**

* `{ authWitnesses = [], capsules = [], }`: `{ /** The authWitnesses to add to the deployment */ authWitnesses?: AuthWitness[]; /** The capsules to add to the deployment */ capsules?: Capsule[]; }`

**Returns:**

`DeployMethod` - A new DeployMethod with the added metadata, but calling the same original function in the same manner

## Getters## address (getter)")

Return this deployment address.

**Signature:**

```
public get address() {
```

**Returns:** `any`

## partialAddress (getter)")

Returns the partial address for this deployment.

**Signature:**

```
public get partialAddress() {
```

**Returns:** `any`

---

## `contract/deploy_sent_tx.ts`## DeployedWaitOpts**Type:** Type Alias

Options related to waiting for a deployment tx.

**Signature:**

```
export type DeployedWaitOpts = WaitOpts & {  
 wallet?: Wallet;  
};
```

**Type Members:**

## walletWallet to use for creating a contract instance. Uses the one set in the deployer constructor if not set.

**Type:** `Wallet`

## DeployTxReceipt**Type:** Type Alias

Extends a transaction receipt with a contract instance that represents the newly deployed contract.

**Signature:**

```
export type DeployTxReceipt<TContract extends ContractBase = ContractBase> = FieldsOf<TxReceipt> & {  
 contract: TContract;  
 instance: ContractInstanceWithAddress;  
};
```

**Type Members:**

## contractInstance of the newly deployed contract.

**Type:** `TContract`

## instanceThe deployed contract instance with address and metadata.

**Type:** `ContractInstanceWithAddress`

## DeploySentTx**Type:** Class

A contract deployment transaction sent to the network, extending SentTx with methods to publish a contract instance.

**Extends:** `SentTx`

## Constructor**Signature:**

```
constructor(  
  wallet: Wallet,  
  sendTx: () => Promise<TxHash>,  
  private postDeployCtor: (instance: ContractInstanceWithAddress, wallet: Wallet) => TContract,  
  private instanceGetter: () => Promise<ContractInstanceWithAddress>  
)
```

**Parameters:**

* `wallet`: `Wallet`
* `sendTx`: `() => Promise<TxHash>`
* `postDeployCtor`: `(instance: ContractInstanceWithAddress, wallet: Wallet) => TContract`
* `instanceGetter`: `() => Promise<ContractInstanceWithAddress>`
  + A getter for the deployed contract instance

## Methods## getInstanceReturns the contract instance for this deployment.

**Signature:**

```
public async getInstance(): Promise<ContractInstanceWithAddress>
```

**Returns:**

`Promise<ContractInstanceWithAddress>` - The deployed contract instance with address and metadata.

## deployedAwaits for the tx to be mined and returns the contract instance. Throws if tx is not mined.

**Signature:**

```
public async deployed(opts?: DeployedWaitOpts): Promise<TContract>
```

**Parameters:**

* `opts` (optional): `DeployedWaitOpts`
  + Options for configuring the waiting for the tx to be mined.

**Returns:**

`Promise<TContract>` - The deployed contract instance.

## waitAwaits for the tx to be mined and returns the receipt along with a contract instance. Throws if tx is not mined.

**Signature:**

```
public override async wait(opts?: DeployedWaitOpts): Promise<DeployTxReceipt<TContract>>
```

**Parameters:**

* `opts` (optional): `DeployedWaitOpts`
  + Options for configuring the waiting for the tx to be mined.

**Returns:**

`Promise<DeployTxReceipt<TContract>>` - The transaction receipt with the deployed contract instance.

---

## `contract/get_gas_limits.ts`## getGasLimits**Type:** Function

Returns suggested total and teardown gas limits for a simulated tx.

**Signature:**

```
export getGasLimits(  
  simulationResult: TxSimulationResult,  
  pad = 0.1  
): {  
      gasLimits: Gas;  
      teardownGasLimits: Gas;  
}
```

**Parameters:**

* `simulationResult`: `TxSimulationResult`
* `pad` (optional): `any`
  + Percentage to pad the suggested gas limits by, (as decimal, e.g., 0.10 for 10%).

**Returns:**

```
{  
  /**  
   * Gas limit for the tx, excluding teardown gas  
   */  
  gasLimits: Gas;  
  /**  
   * Gas limit for the teardown phase  
   */  
  teardownGasLimits: Gas;  
}
```

---

## `contract/interaction_options.ts`## FeeEstimationOptions**Type:** Type Alias

Options used to tweak the simulation and add gas estimation capabilities

**Signature:**

```
export type FeeEstimationOptions = {  
 estimateGas?: boolean;  
 estimatedGasPadding?: number;  
};
```

**Type Members:**

## estimateGasWhether to modify the fee settings of the simulation with high gas limit to figure out actual gas settings.

**Type:** `boolean`

## estimatedGasPaddingPercentage to pad the estimated gas limits by, if empty, defaults to 0.1. Only relevant if estimateGas is set.

**Type:** `number`

## FeePaymentMethodOption**Type:** Type Alias

Interactions allow configuring a custom fee payment method that gets bundled with the transaction before sending it to the wallet

**Signature:**

```
export type FeePaymentMethodOption = {  
 paymentMethod?: FeePaymentMethod;  
};
```

**Type Members:**

## paymentMethodFee payment method to embed in the interaction

**Type:** `FeePaymentMethod`

## GasSettingsOption**Type:** Type Alias

User-defined partial gas settings for the interaction. This type is completely optional since the wallet will fill in the missing options

**Signature:**

```
export type GasSettingsOption = {  
 gasSettings?: Partial<FieldsOf<GasSettings>>;  
};
```

**Type Members:**

## gasSettingsThe gas settings

**Type:** `Partial<FieldsOf<GasSettings>>`

## InteractionFeeOptions**Type:** Type Alias

Fee options as set by a user.

**Signature:**

```
export type InteractionFeeOptions = GasSettingsOption & FeePaymentMethodOption;
```

## SimulationInteractionFeeOptions**Type:** Type Alias

Fee options that can be set for simulation *only*

**Signature:**

```
export type SimulationInteractionFeeOptions = InteractionFeeOptions & FeeEstimationOptions;
```

## RequestInteractionOptions**Type:** Type Alias

Represents the options to configure a request from a contract interaction. Allows specifying additional auth witnesses and capsules to use during execution

**Signature:**

```
export type RequestInteractionOptions = {  
 authWitnesses?: AuthWitness[];  
 capsules?: Capsule[];  
 fee?: FeePaymentMethodOption;  
};
```

**Type Members:**

## authWitnessesExtra authwits to use during execution

**Type:** `AuthWitness[]`

## capsulesExtra capsules to use during execution

**Type:** `Capsule[]`

## feeFee payment method to embed in the interaction request

**Type:** `FeePaymentMethodOption`

## SendInteractionOptions**Type:** Type Alias

Represents options for calling a (constrained) function in a contract.

**Signature:**

```
export type SendInteractionOptions = RequestInteractionOptions & {  
 from: AztecAddress;  
 fee?: InteractionFeeOptions;  
};
```

**Type Members:**

## fromThe sender's Aztec address.

**Type:** `AztecAddress`

## feeThe fee options for the transaction.

**Type:** `InteractionFeeOptions`

## SimulateInteractionOptions**Type:** Type Alias

Represents the options for simulating a contract function interaction. Allows specifying the address from which the method should be called. Disregarded for simulation of public functions

**Signature:**

```
export type SimulateInteractionOptions = Omit<SendInteractionOptions, 'fee'> & {  
 fee?: SimulationInteractionFeeOptions;  
 skipTxValidation?: boolean;  
 skipFeeEnforcement?: boolean;  
 includeMetadata?: boolean;  
};
```

**Type Members:**

## feeThe fee options for the transaction.

**Type:** `SimulationInteractionFeeOptions`

## skipTxValidationSimulate without checking for the validity of the resulting transaction, e.g. whether it emits any existing nullifiers.

**Type:** `boolean`

## skipFeeEnforcementWhether to ensure the fee payer is not empty and has enough balance to pay for the fee.

**Type:** `boolean`

## includeMetadataWhether to include metadata such as offchain effects and performance statistics (e.g. timing information of the different circuits and oracles) in the simulation result, instead of just the return value of the function

**Type:** `boolean`

## ProfileInteractionOptions**Type:** Type Alias

Represents the options for profiling an interaction.

**Signature:**

```
export type ProfileInteractionOptions = SimulateInteractionOptions & {  
 profileMode: 'gates' | 'execution-steps' | 'full';  
 skipProofGeneration?: boolean;  
};
```

**Type Members:**

## profileModeWhether to return gates information or the bytecode/witnesses.

**Type:** `'gates' | 'execution-steps' | 'full'`

## skipProofGenerationWhether to generate a Chonk proof or not

**Type:** `boolean`

## SimulationReturn**Type:** Type Alias

Represents the result type of a simulation. By default, it will just be the return value of the simulated function If `includeMetadata` is set to true in `SimulateInteractionOptions` on the input of `simulate(...)`, it will provide extra information.

**Signature:**

```
export type SimulationReturn<T extends boolean | undefined> = T extends true  
 ? {  
 stats: SimulationStats;  
 offchainEffects: OffchainEffect[];  
 result: any;  
 estimatedGas: Pick<GasSettings, 'gasLimits' | 'teardownGasLimits'>;  
 }  
 : any;
```

## toSendOptions**Type:** Function

Transforms and cleans up the higher level SendInteractionOptions defined by the interaction into SendOptions, which are the ones that can be serialized and forwarded to the wallet

**Signature:**

```
export toSendOptions(options: SendInteractionOptions): SendOptions
```

**Parameters:**

* `options`: `SendInteractionOptions`

**Returns:**

`SendOptions`

## toSimulateOptions**Type:** Function

Transforms and cleans up the higher level SimulateInteractionOptions defined by the interaction into SimulateOptions, which are the ones that can be serialized and forwarded to the wallet

**Signature:**

```
export toSimulateOptions(options: SimulateInteractionOptions): SimulateOptions
```

**Parameters:**

* `options`: `SimulateInteractionOptions`

**Returns:**

`SimulateOptions`

## toProfileOptions**Type:** Function

Transforms and cleans up the higher level ProfileInteractionOptions defined by the interaction into ProfileOptions, which are the ones that can be serialized and forwarded to the wallet

**Signature:**

```
export toProfileOptions(options: ProfileInteractionOptions): ProfileOptions
```

**Parameters:**

* `options`: `ProfileInteractionOptions`

**Returns:**

`ProfileOptions`

---

## `contract/protocol_contracts.ts`## getClassRegistryContract**Type:** Function

Returns a Contract wrapper for the contract class registry.

**Signature:**

```
export async getClassRegistryContract(wallet: Wallet)
```

**Parameters:**

* `wallet`: `Wallet`

**Returns:**

`Promise<UnsafeContract>`

## getInstanceRegistryContract**Type:** Function

Returns a Contract wrapper for the contract instance registry.

**Signature:**

```
export async getInstanceRegistryContract(wallet: Wallet)
```

**Parameters:**

* `wallet`: `Wallet`

**Returns:**

`Promise<UnsafeContract>`

## getFeeJuice**Type:** Function

Returns a Contract wrapper for the fee juice contract

**Signature:**

```
export async getFeeJuice(wallet: Wallet)
```

**Parameters:**

* `wallet`: `Wallet`

**Returns:**

`Promise<UnsafeContract>`

---

## `contract/sent_tx.ts`## WaitOpts**Type:** Type Alias

Options related to waiting for a tx.

**Signature:**

```
export type WaitOpts = {  
 ignoreDroppedReceiptsFor?: number;  
 timeout?: number;  
 interval?: number;  
 dontThrowOnRevert?: boolean;  
};
```

**Type Members:**

## ignoreDroppedReceiptsForThe amount of time to ignore TxStatus.DROPPED receipts (in seconds) due to the presumption that it is being propagated by the p2p network. Defaults to 5.

**Type:** `number`

## timeoutThe maximum time (in seconds) to wait for the transaction to be mined. Defaults to 60.

**Type:** `number`

## intervalThe time interval (in seconds) between retries to fetch the transaction receipt. Defaults to 1.

**Type:** `number`

## dontThrowOnRevertWhether to accept a revert as a status code for the tx when waiting for it. If false, will throw if the tx reverts.

**Type:** `boolean`

## DefaultWaitOpts**Type:** Constant

**Value Type:** `WaitOpts`

## SentTx**Type:** Class

The SentTx class represents a sent transaction through the PXE (or directly to a node) providing methods to fetch its hash, receipt, and mining status.

## Constructor**Signature:**

```
constructor(  
  protected walletOrNode: Wallet | AztecNode,  
  sendTx: () => Promise<TxHash>  
)
```

**Parameters:**

* `walletOrNode`: `Wallet | AztecNode`
* `sendTx`: `() => Promise<TxHash>`

## Properties## sendTxPromise**Type:** `Promise<void>`

## sendTxError**Type:** `Error`

## txHash**Type:** `TxHash`

## Methods## getTxHashRetrieves the transaction hash of the SentTx instance. The function internally awaits for the 'txHashPromise' to resolve, and then returns the resolved transaction hash.

**Signature:**

```
public async getTxHash(): Promise<TxHash>
```

**Returns:**

`Promise<TxHash>` - A promise that resolves to the transaction hash of the SentTx instance. TODO(#7717): Don't throw here.

## getReceiptRetrieve the transaction receipt associated with the current SentTx instance. The function fetches the transaction hash using 'getTxHash' and then queries the PXE to get the corresponding transaction receipt.

**Signature:**

```
public async getReceipt(): Promise<TxReceipt>
```

**Returns:**

`Promise<TxReceipt>` - A promise that resolves to a TxReceipt object representing the fetched transaction receipt.

## waitAwaits for a tx to be mined and returns the receipt. Throws if tx is not mined.

**Signature:**

```
public async wait(opts?: WaitOpts): Promise<FieldsOf<TxReceipt>>
```

**Parameters:**

* `opts` (optional): `WaitOpts`
  + Options for configuring the waiting for the tx to be mined.

**Returns:**

`Promise<FieldsOf<TxReceipt>>` - The transaction receipt.

## waitForReceipt**Signature:**

```
protected async waitForReceipt(opts?: WaitOpts): Promise<TxReceipt>
```

**Parameters:**

* `opts` (optional): `WaitOpts`

**Returns:**

`Promise<TxReceipt>`

---

## `contract/unsafe_contract.ts`## UnsafeContract**Type:** Class

Unsafe constructor for ContractBase that bypasses the check that the instance is registered in the wallet.

**Extends:** `ContractBase`

## Constructor**Signature:**

```
constructor(  
  instance: ContractInstanceWithAddress,  
  artifact: ContractArtifact,  
  wallet: Wallet  
)
```

**Parameters:**

* `instance`: `ContractInstanceWithAddress`
  + The deployed contract instance definition.
* `artifact`: `ContractArtifact`
  + The Application Binary Interface for the contract.
* `wallet`: `Wallet`
  + The wallet used for interacting with this contract.

---

## `contract/wait_for_proven.ts`## WaitForProvenOpts**Type:** Type Alias

Options for waiting for a transaction to be proven.

**Signature:**

```
export type WaitForProvenOpts = {  
 provenTimeout?: number;  
 interval?: number;  
};
```

**Type Members:**

## provenTimeoutTime to wait for the tx to be proven before timing out

**Type:** `number`

## intervalElapsed time between polls to the node

**Type:** `number`

## DefaultWaitForProvenOpts**Type:** Constant

**Value Type:** `WaitForProvenOpts`

## waitForProven**Type:** Function

Wait for a transaction to be proven by polling the node

**Signature:**

```
export async waitForProven(  
  node: AztecNode,  
  receipt: TxReceipt,  
  opts?: WaitForProvenOpts  
)
```

**Parameters:**

* `node`: `AztecNode`
* `receipt`: `TxReceipt`
* `opts` (optional): `WaitForProvenOpts`

**Returns:**

`Promise<any>`

## Deployment---

## `deployment/broadcast_function.ts`## broadcastPrivateFunction**Type:** Function

Sets up a call to broadcast a private function's bytecode via the ClassRegistry contract. Note that this is not required for users to call the function, but is rather a convenience to make this code publicly available so dapps or wallets do not need to redistribute it.

**Signature:**

```
export async broadcastPrivateFunction(  
  wallet: Wallet,  
  artifact: ContractArtifact,  
  selector: FunctionSelector  
): Promise<ContractFunctionInteraction>
```

**Parameters:**

* `wallet`: `Wallet`
  + Wallet to send the transaction.
* `artifact`: `ContractArtifact`
  + Contract artifact that contains the function to be broadcast.
* `selector`: `FunctionSelector`
  + Selector of the function to be broadcast.

**Returns:**

`Promise<ContractFunctionInteraction>` - A ContractFunctionInteraction object that can be used to send the transaction.

## broadcastUtilityFunction**Type:** Function

Sets up a call to broadcast a utility function's bytecode via the ClassRegistry contract. Note that this is not required for users to call the function, but is rather a convenience to make this code publicly available so dapps or wallets do not need to redistribute it.

**Signature:**

```
export async broadcastUtilityFunction(  
  wallet: Wallet,  
  artifact: ContractArtifact,  
  selector: FunctionSelector  
): Promise<ContractFunctionInteraction>
```

**Parameters:**

* `wallet`: `Wallet`
  + Wallet to send the transaction.
* `artifact`: `ContractArtifact`
  + Contract artifact that contains the function to be broadcast.
* `selector`: `FunctionSelector`
  + Selector of the function to be broadcast.

**Returns:**

`Promise<ContractFunctionInteraction>` - A ContractFunctionInteraction object that can be used to send the transaction.

---

## `deployment/contract_deployer.ts`## ContractDeployer**Type:** Class

A class for deploying contract.

## Constructor**Signature:**

```
constructor(  
  private artifact: ContractArtifact,  
  private wallet: Wallet,  
  private publicKeys?: PublicKeys,  
  private constructorName?: string  
)
```

**Parameters:**

* `artifact`: `ContractArtifact`
* `wallet`: `Wallet`
* `publicKeys` (optional): `PublicKeys`
* `constructorName` (optional): `string`

## Methods## deployDeploy a contract using the provided ABI and constructor arguments. This function creates a new DeployMethod instance that can be used to send deployment transactions and query deployment status. The method accepts any number of constructor arguments, which will be passed to the contract's constructor during deployment.

**Signature:**

```
public deploy(...args: any[])
```

**Parameters:**

* `args`: `any[]`
  + The constructor arguments for the contract being deployed.

**Returns:**

`DeployMethod<Contract>` - A DeployMethod instance configured with the ABI, PXE, and constructor arguments.

---

## `deployment/publish_class.ts`## publishContractClass**Type:** Function

Sets up a call to publish a contract class given its artifact.

**Signature:**

```
export async publishContractClass(  
  wallet: Wallet,  
  artifact: ContractArtifact  
): Promise<ContractFunctionInteraction>
```

**Parameters:**

* `wallet`: `Wallet`
* `artifact`: `ContractArtifact`

**Returns:**

`Promise<ContractFunctionInteraction>`

---

## `deployment/publish_instance.ts`## publishInstance**Type:** Function

Sets up a call to the canonical contract instance registry to publish a contract instance.

**Signature:**

```
export async publishInstance(  
  wallet: Wallet,  
  instance: ContractInstanceWithAddress  
): Promise<ContractFunctionInteraction>
```

**Parameters:**

* `wallet`: `Wallet`
  + The wallet to use for the publication (setup) tx.
* `instance`: `ContractInstanceWithAddress`
  + The instance to publish.

**Returns:**

`Promise<ContractFunctionInteraction>`

## Ethereum---

## `ethereum/portal_manager.ts`## L2Claim**Type:** Type Alias

L1 to L2 message info to claim it on L2.

**Signature:**

```
export type L2Claim = {  
 claimSecret: Fr;  
 claimSecretHash: Fr;  
 messageHash: Hex;  
 messageLeafIndex: bigint;  
};
```

**Type Members:**

## claimSecretSecret for claiming.

**Type:** `Fr`

## claimSecretHashHash of the secret for claiming.

**Type:** `Fr`

## messageHashHash of the message.

**Type:** `Hex`

## messageLeafIndexLeaf index in the L1 to L2 message tree.

**Type:** `bigint`

## L2AmountClaim**Type:** Type Alias

L1 to L2 message info that corresponds to an amount to claim.

**Signature:**

```
export type L2AmountClaim = L2Claim & { claimAmount: bigint };
```

**Type Members:**

## claimAmount**Type:** `bigint`

## L2AmountClaimWithRecipient**Type:** Type Alias

L1 to L2 message info that corresponds to an amount to claim with associated recipient.

**Signature:**

```
export type L2AmountClaimWithRecipient = L2AmountClaim & {  
 recipient: AztecAddress;  
};
```

**Type Members:**

## recipientAddress that will receive the newly minted notes.

**Type:** `AztecAddress`

## generateClaimSecret**Type:** Function

Generates a pair secret and secret hash

**Signature:**

```
export async generateClaimSecret(logger?: Logger): Promise<[  
      Fr,  
      Fr  
  ]>
```

**Parameters:**

* `logger` (optional): `Logger`

**Returns:**

`Promise<[Fr, Fr]>`

## L1TokenManager**Type:** Class

Helper for managing an ERC20 on L1.

## Constructor**Signature:**

```
public constructor(  
  public readonly tokenAddress: EthAddress,  
  public readonly handlerAddress: EthAddress | undefined,  
  private readonly extendedClient: ExtendedViemWalletClient,  
  private logger: Logger  
)
```

**Parameters:**

* `tokenAddress`: `EthAddress`
  + Address of the ERC20 contract.
* `handlerAddress`: `EthAddress | undefined`
  + Address of the handler/faucet contract.
* `extendedClient`: `ExtendedViemWalletClient`
* `logger`: `Logger`

## Methods## getMintAmountReturns the amount of tokens available to mint via the handler.

**Signature:**

```
public async getMintAmount()
```

**Returns:**

`Promise<any>`

## getL1TokenBalanceReturns the balance of the given address.

**Signature:**

```
public async getL1TokenBalance(address: Hex)
```

**Parameters:**

* `address`: `Hex`
  + Address to get the balance of.

**Returns:**

`Promise<any>`

## mintMints a fixed amount of tokens for the given address. Returns once the tx has been mined.

**Signature:**

```
public async mint(  
  address: Hex,  
  addressName?: string  
)
```

**Parameters:**

* `address`: `Hex`
  + Address to mint the tokens for.
* `addressName` (optional): `string`
  + Optional name of the address for logging.

**Returns:**

`Promise<void>`

## approveApproves tokens for the given address. Returns once the tx has been mined.

**Signature:**

```
public async approve(  
  amount: bigint,  
  address: Hex,  
  addressName = ''  
)
```

**Parameters:**

* `amount`: `bigint`
  + Amount to approve.
* `address`: `Hex`
  + Address to approve the tokens for.
* `addressName` (optional): `any`
  + Optional name of the address for logging.

**Returns:**

`Promise<void>`

## L1FeeJuicePortalManager**Type:** Class

Helper for interacting with the FeeJuicePortal on L1.

## Constructor**Signature:**

```
constructor(  
  portalAddress: EthAddress,  
  tokenAddress: EthAddress,  
  handlerAddress: EthAddress,  
  private readonly extendedClient: ExtendedViemWalletClient,  
  private readonly logger: Logger  
)
```

**Parameters:**

* `portalAddress`: `EthAddress`
* `tokenAddress`: `EthAddress`
* `handlerAddress`: `EthAddress`
* `extendedClient`: `ExtendedViemWalletClient`
* `logger`: `Logger`

## Methods## getTokenManagerReturns the associated token manager for the L1 ERC20.

**Signature:**

```
public getTokenManager()
```

**Returns:**

`L1TokenManager`

## bridgeTokensPublicBridges fee juice from L1 to L2 publicly. Handles L1 ERC20 approvals. Returns once the tx has been mined.

**Signature:**

```
public async bridgeTokensPublic(  
  to: AztecAddress,  
  amount: bigint | undefined,  
  mint = false  
): Promise<L2AmountClaim>
```

**Parameters:**

* `to`: `AztecAddress`
  + Address to send the tokens to on L2.
* `amount`: `bigint | undefined`
  + Amount of tokens to send.
* `mint` (optional): `any`
  + Whether to mint the tokens before sending (only during testing).

**Returns:**

`Promise<L2AmountClaim>`

## newCreates a new instance

**Signature:**

```
public static async new(  
  node: AztecNode,  
  extendedClient: ExtendedViemWalletClient,  
  logger: Logger  
): Promise<L1FeeJuicePortalManager>
```

**Parameters:**

* `node`: `AztecNode`
  + Aztec node client used for retrieving the L1 contract addresses.
* `extendedClient`: `ExtendedViemWalletClient`
  + Wallet client, extended with public actions.
* `logger`: `Logger`
  + Logger.

**Returns:**

`Promise<L1FeeJuicePortalManager>`

## L1ToL2TokenPortalManager**Type:** Class

Helper for interacting with a test TokenPortal on L1 for sending tokens to L2.

## Constructor**Signature:**

```
constructor(  
  portalAddress: EthAddress,  
  tokenAddress: EthAddress,  
  handlerAddress: EthAddress | undefined,  
  protected extendedClient: ExtendedViemWalletClient,  
  protected logger: Logger  
)
```

**Parameters:**

* `portalAddress`: `EthAddress`
* `tokenAddress`: `EthAddress`
* `handlerAddress`: `EthAddress | undefined`
* `extendedClient`: `ExtendedViemWalletClient`
* `logger`: `Logger`

## Properties## portal**Type:** `ViemContract<typeof TokenPortalAbi>`

## tokenManager**Type:** `L1TokenManager`

## Methods## getTokenManagerReturns the token manager for the underlying L1 token.

**Signature:**

```
public getTokenManager()
```

**Returns:**

`L1TokenManager`

## bridgeTokensPublicBridges tokens from L1 to L2. Handles token approvals. Returns once the tx has been mined.

**Signature:**

```
public async bridgeTokensPublic(  
  to: AztecAddress,  
  amount: bigint,  
  mint = false  
): Promise<L2AmountClaim>
```

**Parameters:**

* `to`: `AztecAddress`
  + Address to send the tokens to on L2.
* `amount`: `bigint`
  + Amount of tokens to send.
* `mint` (optional): `any`
  + Whether to mint the tokens before sending (only during testing).

**Returns:**

`Promise<L2AmountClaim>`

## bridgeTokensPrivateBridges tokens from L1 to L2 privately. Handles token approvals. Returns once the tx has been mined.

**Signature:**

```
public async bridgeTokensPrivate(  
  to: AztecAddress,  
  amount: bigint,  
  mint = false  
): Promise<L2AmountClaimWithRecipient>
```

**Parameters:**

* `to`: `AztecAddress`
  + Address to send the tokens to on L2.
* `amount`: `bigint`
  + Amount of tokens to send.
* `mint` (optional): `any`
  + Whether to mint the tokens before sending (only during testing).

**Returns:**

`Promise<L2AmountClaimWithRecipient>`

## L1TokenPortalManager**Type:** Class

Helper for interacting with a test TokenPortal on L1 for both withdrawing from and bridging to L2.

**Extends:** `L1ToL2TokenPortalManager`

## Constructor**Signature:**

```
constructor(  
  portalAddress: EthAddress,  
  tokenAddress: EthAddress,  
  handlerAddress: EthAddress | undefined,  
  outboxAddress: EthAddress,  
  extendedClient: ExtendedViemWalletClient,  
  logger: Logger  
)
```

**Parameters:**

* `portalAddress`: `EthAddress`
* `tokenAddress`: `EthAddress`
* `handlerAddress`: `EthAddress | undefined`
* `outboxAddress`: `EthAddress`
* `extendedClient`: `ExtendedViemWalletClient`
* `logger`: `Logger`

## Methods## withdrawFundsWithdraws funds from the portal by consuming an L2 to L1 message. Returns once the tx is mined on L1.

**Signature:**

```
public async withdrawFunds(  
  amount: bigint,  
  recipient: EthAddress,  
  blockNumber: bigint,  
  messageIndex: bigint,  
  siblingPath: SiblingPath<number>  
)
```

**Parameters:**

* `amount`: `bigint`
  + Amount to withdraw.
* `recipient`: `EthAddress`
  + Who will receive the funds.
* `blockNumber`: `bigint`
  + L2 block number of the message.
* `messageIndex`: `bigint`
  + Index of the message.
* `siblingPath`: `SiblingPath<number>`
  + Sibling path of the message.

**Returns:**

`Promise<void>`

## getL2ToL1MessageLeafComputes the L2 to L1 message leaf for the given parameters.

**Signature:**

```
public async getL2ToL1MessageLeaf(  
  amount: bigint,  
  recipient: EthAddress,  
  l2Bridge: AztecAddress,  
  callerOnL1: EthAddress = EthAddress.ZERO  
): Promise<Fr>
```

**Parameters:**

* `amount`: `bigint`
  + Amount to bridge.
* `recipient`: `EthAddress`
  + Recipient on L1.
* `l2Bridge`: `AztecAddress`
  + Address of the L2 bridge.
* `callerOnL1` (optional): `EthAddress`
  + Caller address on L1.

**Returns:**

`Promise<Fr>`

## Fee---

## `fee/fee_juice_payment_method_with_claim.ts`## FeeJuicePaymentMethodWithClaim**Type:** Class

Pay fee directly with Fee Juice claimed in the same tx. Claiming consumes an L1 to L2 message that "contains" the fee juice bridged from L1.

**Implements:** `FeePaymentMethod`

## Constructor**Signature:**

```
constructor(  
  private sender: AztecAddress,  
  private claim: Pick<L2AmountClaim, 'claimAmount' | 'claimSecret' | 'messageLeafIndex'>  
)
```

**Parameters:**

* `sender`: `AztecAddress`
* `claim`: `Pick<L2AmountClaim, 'claimAmount' | 'claimSecret' | 'messageLeafIndex'>`

## Methods## getExecutionPayloadCreates an execution payload to pay the fee in Fee Juice.

**Signature:**

```
async getExecutionPayload(): Promise<ExecutionPayload>
```

**Returns:**

`Promise<ExecutionPayload>` - An execution payload that just contains the `claim_and_end_setup` function call.

## getAsset**Signature:**

```
getAsset()
```

**Returns:**

`Promise<any>`

## getFeePayer**Signature:**

```
getFeePayer(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>`

## getGasSettings**Signature:**

```
getGasSettings(): GasSettings | undefined
```

**Returns:**

`GasSettings | undefined`

---

## `fee/fee_payment_method.ts`## FeePaymentMethod**Type:** Interface

Holds information about how the fee for a transaction is to be paid.

## Methods## getAssetThe asset used to pay the fee.

**Signature:**

```
getAsset(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>`

## getExecutionPayloadReturns the data to be added to the final execution request to pay the fee in the given asset

**Signature:**

```
getExecutionPayload(): Promise<ExecutionPayload>
```

**Returns:**

`Promise<ExecutionPayload>` - The function calls to pay the fee.

## getFeePayerThe expected fee payer for this tx.

**Signature:**

```
getFeePayer(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>`

## getGasSettingsThe gas settings (if any) used to compute the execution payload of the payment method

**Signature:**

```
getGasSettings(): GasSettings | undefined
```

**Returns:**

`GasSettings | undefined`

---

## `fee/private_fee_payment_method.ts`## PrivateFeePaymentMethod**Type:** Class

Holds information about how the fee for a transaction is to be paid.

**Implements:** `FeePaymentMethod`

## Constructor**Signature:**

```
constructor(  
  private paymentContract: AztecAddress,  
  private sender: AztecAddress,  
  private wallet: Wallet,  
  protected gasSettings: GasSettings,  
  private setMaxFeeToOne = false  
)
```

**Parameters:**

* `paymentContract`: `AztecAddress`
  + Address which will hold the fee payment.
* `sender`: `AztecAddress`
  + Address of the account that will pay the fee
* `wallet`: `Wallet`
  + A wallet to perform the simulation to get the accepted asset
* `gasSettings`: `GasSettings`
  + Gas settings used to compute the maximum fee the user is willing to pay
* `setMaxFeeToOne` (optional): `any`
  + If true, the max fee will be set to 1. TODO(#7694): Remove this param once the lacking feature in TXE is implemented.

## Methods## getAssetThe asset used to pay the fee.

**Signature:**

```
async getAsset(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>` - The asset used to pay the fee.

## getFeePayer**Signature:**

```
getFeePayer(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>`

## getExecutionPayloadCreates an execution payload to pay the fee using a private function through an FPC in the desired asset

**Signature:**

```
async getExecutionPayload(): Promise<ExecutionPayload>
```

**Returns:**

`Promise<ExecutionPayload>` - An execution payload that contains the required function calls and auth witnesses.

## getGasSettings**Signature:**

```
getGasSettings(): GasSettings | undefined
```

**Returns:**

`GasSettings | undefined`

---

## `fee/public_fee_payment_method.ts`## PublicFeePaymentMethod**Type:** Class

Holds information about how the fee for a transaction is to be paid.

**Implements:** `FeePaymentMethod`

## Constructor**Signature:**

```
constructor(  
  protected paymentContract: AztecAddress,  
  protected sender: AztecAddress,  
  protected wallet: Wallet,  
  protected gasSettings: GasSettings  
)
```

**Parameters:**

* `paymentContract`: `AztecAddress`
  + Address which will hold the fee payment.
* `sender`: `AztecAddress`
  + An auth witness provider to authorize fee payments
* `wallet`: `Wallet`
  + A wallet to perform the simulation to get the accepted asset
* `gasSettings`: `GasSettings`
  + Gas settings used to compute the maximum fee the user is willing to pay

## Methods## getAssetThe asset used to pay the fee.

**Signature:**

```
async getAsset(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>` - The asset used to pay the fee.

## getFeePayer**Signature:**

```
getFeePayer(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>`

## getExecutionPayloadCreates an execution payload to pay the fee using a public function through an FPC in the desired asset

**Signature:**

```
async getExecutionPayload(): Promise<ExecutionPayload>
```

**Returns:**

`Promise<ExecutionPayload>` - An execution payload that contains the required function calls.

## getGasSettings**Signature:**

```
getGasSettings(): GasSettings | undefined
```

**Returns:**

`GasSettings | undefined`

---

## `fee/sponsored_fee_payment.ts`## SponsoredFeePaymentMethod**Type:** Class

A fee payment method that uses a contract that blindly sponsors transactions. This contract is expected to be prefunded in testing environments.

**Implements:** `FeePaymentMethod`

## Constructor**Signature:**

```
constructor(private paymentContract: AztecAddress)
```

**Parameters:**

* `paymentContract`: `AztecAddress`

## Methods## getAsset**Signature:**

```
getAsset(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>`

## getFeePayer**Signature:**

```
getFeePayer()
```

**Returns:**

`Promise<any>`

## getExecutionPayload**Signature:**

```
async getExecutionPayload(): Promise<ExecutionPayload>
```

**Returns:**

`Promise<ExecutionPayload>`

## getGasSettings**Signature:**

```
getGasSettings(): GasSettings | undefined
```

**Returns:**

`GasSettings | undefined`

## Utils---

## `utils/abi_types.ts`## FieldLike**Type:** Type Alias

Any type that can be converted into a field for a contract call.

**Signature:**

```
export type FieldLike = Fr | Buffer | bigint | number | { toField: () => Fr };
```

## EthAddressLike**Type:** Type Alias

Any type that can be converted into an EthAddress Aztec.nr struct.

**Signature:**

```
export type EthAddressLike = { address: FieldLike } | EthAddress;
```

## AztecAddressLike**Type:** Type Alias

Any type that can be converted into an AztecAddress Aztec.nr struct.

**Signature:**

```
export type AztecAddressLike = { address: FieldLike } | AztecAddress;
```

## FunctionSelectorLike**Type:** Type Alias

Any type that can be converted into a FunctionSelector Aztec.nr struct.

**Signature:**

```
export type FunctionSelectorLike = FieldLike | FunctionSelector;
```

## EventSelectorLike**Type:** Type Alias

Any type that can be converted into an EventSelector Aztec.nr struct.

**Signature:**

```
export type EventSelectorLike = FieldLike | EventSelector;
```

## U128Like**Type:** Type Alias

Any type that can be converted into a U128.

**Signature:**

```
export type U128Like = bigint | number;
```

## WrappedFieldLike**Type:** Type Alias

Any type that can be converted into a struct with a single `inner` field.

**Signature:**

```
export type WrappedFieldLike = { inner: FieldLike } | FieldLike;
```

---

## `utils/authwit.ts`## IntentInnerHash**Type:** Type Alias

Intent with an inner hash

**Signature:**

```
export type IntentInnerHash = {  
 consumer: AztecAddress;  
 innerHash: Fr;  
};
```

**Type Members:**

## consumerThe consumer

**Type:** `AztecAddress`

## innerHashThe action to approve

**Type:** `Fr`

## CallIntent**Type:** Type Alias

Intent with a call

**Signature:**

```
export type CallIntent = {  
 caller: AztecAddress;  
 call: FunctionCall;  
};
```

**Type Members:**

## callerThe caller to approve

**Type:** `AztecAddress`

## callThe call to approve

**Type:** `FunctionCall`

## ContractFunctionInteractionCallIntent**Type:** Type Alias

Intent with a ContractFunctionInteraction

**Signature:**

```
export type ContractFunctionInteractionCallIntent = {  
 caller: AztecAddress;  
 action: ContractFunctionInteraction;  
};
```

**Type Members:**

## callerThe caller to approve

**Type:** `AztecAddress`

## actionThe action to approve

**Type:** `ContractFunctionInteraction`

## computeAuthWitMessageHash**Type:** Constant

Compute an authentication witness message hash from an intent and metadata If using the `IntentInnerHash`, the consumer is the address that can "consume" the authwit, for token approvals it is the token contract itself. The `innerHash` itself will be the message that a contract is allowed to execute. At the point of "approval checking", the validating contract (account for private and registry for public) will be computing the message hash (`H(consumer, chainid, version, inner_hash)`) where the all but the `inner_hash` is injected from the context (consumer = msg\_sender), and use it for the authentication check. Therefore, any allowed `innerHash` will therefore also have information around where it can be spent (version, chainId) and who can spend it (consumer). If using the `CallIntent`, the caller is the address that is making the call, for a token approval from Alice to Bob, this would be Bob. The action is then used along with the `caller` to compute the `innerHash` and the consumer.

**Value Type:** `any`

## getMessageHashFromIntent**Type:** Function

Compute an authentication witness message hash from an intent and metadata. This is just a wrapper around computeAuthwitMessageHash that allows receiving an already computed messageHash as input

**Signature:**

```
export async getMessageHashFromIntent(  
  messageHashOrIntent: Fr | IntentInnerHash | CallIntent | ContractFunctionInteractionCallIntent,  
  chainInfo: ChainInfo  
)
```

**Parameters:**

* `messageHashOrIntent`: `Fr | IntentInnerHash | CallIntent | ContractFunctionInteractionCallIntent`
  + The precomputed messageHash or intent to approve (consumer and innerHash or caller and call/action)
* `chainInfo`: `ChainInfo`

**Returns:**

`Promise<Fr>` - The message hash for the intent

## computeInnerAuthWitHashFromAction**Type:** Constant

Computes the inner authwitness hash for either a function call or an action, for it to later be combined with the metadata required for the outer hash and eventually the full AuthWitness.

**Value Type:** `any`

## lookupValidity**Type:** Function

Lookup the validity of an authwit in private and public contexts. Uses the chain id and version of the wallet.

**Signature:**

```
export async lookupValidity(  
  wallet: Wallet,  
  onBehalfOf: AztecAddress,  
  intent: IntentInnerHash | CallIntent | ContractFunctionInteractionCallIntent,  
  witness: AuthWitness  
): Promise<{  
      isValidInPrivate: boolean;  
      isValidInPublic: boolean;  
  }>
```

**Parameters:**

* `wallet`: `Wallet`
  + The wallet use to simulate and read the public data
* `onBehalfOf`: `AztecAddress`
  + The address of the "approver"
* `intent`: `IntentInnerHash | CallIntent | ContractFunctionInteractionCallIntent`
  + The consumer and inner hash or the caller and action to lookup
* `witness`: `AuthWitness`
  + The computed authentication witness to check

**Returns:**

```
Promise<{  
  /** boolean flag indicating if the authwit is valid in private context */  
  isValidInPrivate: boolean;  
  /** boolean flag indicating if the authwit is valid in public context */  
  isValidInPublic: boolean;  
}>
```

A struct containing the validity of the authwit in private and public contexts.

## SetPublicAuthwitContractInteraction**Type:** Class

Convenience class designed to wrap the very common interaction of setting a public authwit in the AuthRegistry contract

**Extends:** `ContractFunctionInteraction`

## Constructor**Signature:**

```
private constructor(  
  wallet: Wallet,  
  private from: AztecAddress,  
  messageHash: Fr,  
  authorized: boolean  
)
```

**Parameters:**

* `wallet`: `Wallet`
* `from`: `AztecAddress`
* `messageHash`: `Fr`
* `authorized`: `boolean`

## Methods## create**Signature:**

```
static async create(  
  wallet: Wallet,  
  from: AztecAddress,  
  messageHashOrIntent: Fr | IntentInnerHash | CallIntent | ContractFunctionInteractionCallIntent,  
  authorized: boolean  
)
```

**Parameters:**

* `wallet`: `Wallet`
* `from`: `AztecAddress`
* `messageHashOrIntent`: `Fr | IntentInnerHash | CallIntent | ContractFunctionInteractionCallIntent`
* `authorized`: `boolean`

**Returns:**

`Promise<SetPublicAuthwitContractInteraction>`

## simulateOverrides the simulate method, adding the sender of the authwit (authorizer) as from and preventing misuse

**Signature:**

```
public override simulate<T extends SimulateInteractionOptions>(options: Omit<T, 'from'>): Promise<SimulationReturn<T['includeMetadata']>>
```

**Parameters:**

* `options`: `Omit<T, 'from'>`
  + An optional object containing additional configuration for the transaction.

**Returns:**

`Promise<SimulationReturn<T['includeMetadata']>>` - The result of the transaction as returned by the contract function.

## simulate**Signature:**

```
public override simulate(options: Omit<SimulateInteractionOptions, 'from'> = {}): Promise<SimulationReturn<typeof options.includeMetadata>>
```

**Parameters:**

* `options` (optional): `Omit<SimulateInteractionOptions, 'from'>`

**Returns:**

`Promise<SimulationReturn<typeof options.includeMetadata>>`

## profileOverrides the profile method, adding the sender of the authwit (authorizer) as from and preventing misuse

**Signature:**

```
public override profile(options: Omit<ProfileInteractionOptions, 'from'> = { profileMode: 'gates' }): Promise<TxProfileResult>
```

**Parameters:**

* `options` (optional): `Omit<ProfileInteractionOptions, 'from'>`
  + Same options as `simulate`, plus profiling method

**Returns:**

`Promise<TxProfileResult>` - An object containing the function return value and profile result.

## sendOverrides the send method, adding the sender of the authwit (authorizer) as from and preventing misuse

**Signature:**

```
public override send(options: Omit<SendInteractionOptions, 'from'> = {}): SentTx
```

**Parameters:**

* `options` (optional): `Omit<SendInteractionOptions, 'from'>`
  + An optional object containing 'fee' options information

**Returns:**

`SentTx` - A SentTx instance for tracking the transaction status and information.

---

## `utils/cross_chain.ts`## waitForL1ToL2MessageReady**Type:** Function

Waits for the L1 to L2 message to be ready to be consumed.

**Signature:**

```
export async waitForL1ToL2MessageReady(  
  node: Pick<AztecNode, 'getBlockNumber' | 'getL1ToL2MessageBlock'>,  
  l1ToL2MessageHash: Fr,  
  opts: {  
    timeoutSeconds: number;  
    forPublicConsumption: boolean;  
}  
)
```

**Parameters:**

* `node`: `Pick<AztecNode, 'getBlockNumber' | 'getL1ToL2MessageBlock'>`
  + Aztec node instance used to obtain the information about the message
* `l1ToL2MessageHash`: `Fr`
  + Hash of the L1 to L2 message
* `opts`: `{ /** Timeout for the operation in seconds */ timeoutSeconds: number; /** True if the message is meant to be consumed from a public function */ forPublicConsumption: boolean; }`
  + Options

**Returns:**

`Promise<any>`

## isL1ToL2MessageReady**Type:** Function

Returns whether the L1 to L2 message is ready to be consumed.

**Signature:**

```
export async isL1ToL2MessageReady(  
  node: Pick<AztecNode, 'getBlockNumber' | 'getL1ToL2MessageBlock'>,  
  l1ToL2MessageHash: Fr,  
  opts: {  
    forPublicConsumption: boolean;  
    messageBlockNumber?: number;  
}  
): Promise<boolean>
```

**Parameters:**

* `node`: `Pick<AztecNode, 'getBlockNumber' | 'getL1ToL2MessageBlock'>`
  + Aztec node instance used to obtain the information about the message
* `l1ToL2MessageHash`: `Fr`
  + Hash of the L1 to L2 message
* `opts`: `{ /** True if the message is meant to be consumed from a public function */ forPublicConsumption: boolean; /** Cached synced block number for the message (will be fetched from PXE otherwise) */ messageBlockNumber?: number; }`
  + Options

**Returns:**

`Promise<boolean>` - True if the message is ready to be consumed, false otherwise

---

## `utils/fee_juice.ts`## getFeeJuiceBalance**Type:** Function

Returns the owner's fee juice balance. Note: This is used only e2e\_local\_network\_example test. TODO: Consider nuking.

**Signature:**

```
export async getFeeJuiceBalance(  
  owner: AztecAddress,  
  node: AztecNode  
): Promise<bigint>
```

**Parameters:**

* `owner`: `AztecAddress`
* `node`: `AztecNode`

**Returns:**

`Promise<bigint>`

---

## `utils/field_compressed_string.ts`## readFieldCompressedString**Type:** Constant

This turns

**Value Type:** `any`

---

## `utils/node.ts`## waitForNode**Type:** Constant

**Value Type:** `any`

## createAztecNodeClient**Type:** Constant

This is re-exported from `@aztec/stdlib/interfaces/client`. See the source module for full documentation.

**Value Type:** `Re-export`

## AztecNode**Type:** Type Alias

This is a type re-exported from `@aztec/stdlib/interfaces/client`. See the source module for full type definition and documentation.

**Signature:**

```
export type { AztecNode } from '@aztec/stdlib/interfaces/client'
```

---

## `utils/pub_key.ts`## generatePublicKey**Type:** Function

Method for generating a public grumpkin key from a private key.

**Signature:**

```
export generatePublicKey(privateKey: GrumpkinScalar): Promise<PublicKey>
```

**Parameters:**

* `privateKey`: `GrumpkinScalar`
  + The private key.

**Returns:**

`Promise<PublicKey>` - The generated public key.

## Wallet---

## `wallet/account_entrypoint_meta_payment_method.ts`## AccountEntrypointMetaPaymentMethod**Type:** Class

Fee payment method that allows an account contract to pay for its own deployment It works by rerouting the provided fee payment method through the account's entrypoint, which sets itself as fee payer. If no payment method is provided, it is assumed the account will pay with its own fee juice balance. Usually, in order to pay fees it is necessary to obtain an ExecutionPayload that encodes the necessary information that is sent to the user's account entrypoint, that has plumbing to handle it. If there's no account contract yet (it's being deployed) a MultiCallContract is used, which doesn't have a concept of fees or how to handle this payload. HOWEVER, the account contract's entrypoint does, so this method reshapes that fee payload into a call to the account contract entrypoint being deployed with the original fee payload. This class can be seen in action in DeployAccountMethod.ts#getSelfPaymentMethod

**Implements:** `FeePaymentMethod`

## Constructor**Signature:**

```
constructor(  
  private wallet: Wallet,  
  private artifact: ContractArtifact,  
  private feePaymentNameOrArtifact: string | FunctionArtifact,  
  private accountAddress: AztecAddress,  
  private paymentMethod?: FeePaymentMethod  
)
```

**Parameters:**

* `wallet`: `Wallet`
* `artifact`: `ContractArtifact`
* `feePaymentNameOrArtifact`: `string | FunctionArtifact`
* `accountAddress`: `AztecAddress`
* `paymentMethod` (optional): `FeePaymentMethod`

## Methods## getAsset**Signature:**

```
getAsset(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>`

## getExecutionPayload**Signature:**

```
async getExecutionPayload(): Promise<ExecutionPayload>
```

**Returns:**

`Promise<ExecutionPayload>`

## getFeePayer**Signature:**

```
getFeePayer(): Promise<AztecAddress>
```

**Returns:**

`Promise<AztecAddress>`

## getGasSettings**Signature:**

```
getGasSettings(): GasSettings | undefined
```

**Returns:**

`GasSettings | undefined`

---

## `wallet/account_manager.ts`## AccountManager**Type:** Class

Manages a user account. Provides methods for calculating the account's address and other related data, plus a helper to return a preconfigured deploy method.

## Constructor**Signature:**

```
private constructor(  
  private wallet: Wallet,  
  private secretKey: Fr,  
  private accountContract: AccountContract,  
  private instance: ContractInstanceWithAddress,  
  public readonly salt: Salt  
)
```

**Parameters:**

* `wallet`: `Wallet`
* `secretKey`: `Fr`
* `accountContract`: `AccountContract`
* `instance`: `ContractInstanceWithAddress`
* `salt`: `Salt`
  + Contract instantiation salt for the account contract

## Methods## create**Signature:**

```
static async create(  
  wallet: Wallet,  
  secretKey: Fr,  
  accountContract: AccountContract,  
  salt?: Salt  
)
```

**Parameters:**

* `wallet`: `Wallet`
* `secretKey`: `Fr`
* `accountContract`: `AccountContract`
* `salt` (optional): `Salt`

**Returns:**

`Promise<AccountManager>`

## getPublicKeys**Signature:**

```
protected getPublicKeys()
```

**Returns:**

`any`

## getPublicKeysHash**Signature:**

```
protected getPublicKeysHash()
```

**Returns:**

`any`

## getAccountInterfaceReturns the entrypoint for this account as defined by its account contract.

**Signature:**

```
public async getAccountInterface(): Promise<AccountInterface>
```

**Returns:**

`Promise<AccountInterface>` - An entrypoint.

## getCompleteAddressGets the calculated complete address associated with this account. Does not require the account to have been published for public execution.

**Signature:**

```
public getCompleteAddress(): Promise<CompleteAddress>
```

**Returns:**

`Promise<CompleteAddress>` - The address, partial address, and encryption public key.

## getSecretKeyReturns the secret key used to derive the rest of the privacy keys for this contract

**Signature:**

```
public getSecretKey()
```

**Returns:**

`Fr`

## getInstanceReturns the contract instance definition associated with this account. Does not require the account to have been published for public execution.

**Signature:**

```
public getInstance(): ContractInstanceWithAddress
```

**Returns:**

`ContractInstanceWithAddress` - ContractInstance instance.

## getAccountReturns a Wallet instance associated with this account. Use it to create Contract instances to be interacted with from this account.

**Signature:**

```
public async getAccount(): Promise<AccountWithSecretKey>
```

**Returns:**

`Promise<AccountWithSecretKey>` - A Wallet instance.

## getAccountContractReturns the account contract that backs this account.

**Signature:**

```
getAccountContract(): AccountContract
```

**Returns:**

`AccountContract` - The account contract

## getDeployMethodReturns a preconfigured deploy method that contains all the necessary function calls to deploy the account contract.

**Signature:**

```
public async getDeployMethod(): Promise<DeployAccountMethod>
```

**Returns:**

`Promise<DeployAccountMethod>`

## hasInitializerReturns whether this account contract has an initializer function.

**Signature:**

```
public async hasInitializer()
```

**Returns:**

`Promise<boolean>`

## Getters## address (getter)")

**Signature:**

```
get address() {
```

**Returns:** `any`

---

## `wallet/deploy_account_method.ts`## RequestDeployAccountOptions**Type:** Type Alias

The configuration options for the request method. Omits the contractAddressSalt, since for account contracts that is fixed in the constructor

**Signature:**

```
export type RequestDeployAccountOptions = Omit<RequestDeployOptions, 'contractAddressSalt'>;
```

## DeployAccountOptions**Type:** Type Alias

The configuration options for the send/prove methods. Omits: - The contractAddressSalt, since for account contracts that is fixed in the constructor. - UniversalDeployment flag, since account contracts are always deployed with it set to true

**Signature:**

```
export type DeployAccountOptions = Omit<DeployOptions, 'contractAddressSalt' | 'universalDeploy'>;
```

## SimulateDeployAccountOptions**Type:** Type Alias

The configuration options for the simulate method. Omits the contractAddressSalt, since for account contracts that is fixed in the constructor

**Signature:**

```
export type SimulateDeployAccountOptions = Omit<SimulateDeployOptions, 'contractAddressSalt'>;
```

## DeployAccountMethod**Type:** Class

Modified version of the DeployMethod used to deploy account contracts. Supports deploying contracts that can pay for their own fee, plus some preconfigured options to avoid errors.

**Extends:** `DeployMethod`

## Constructor**Signature:**

```
constructor(  
  publicKeys: PublicKeys,  
  wallet: Wallet,  
  artifact: ContractArtifact,  
  postDeployCtor: (instance: ContractInstanceWithAddress, wallet: Wallet) => TContract,  
  private salt: Fr,  
  args: any[] = [],  
  constructorNameOrArtifact?: string | FunctionArtifact  
)
```

**Parameters:**

* `publicKeys`: `PublicKeys`
* `wallet`: `Wallet`
* `artifact`: `ContractArtifact`
* `postDeployCtor`: `(instance: ContractInstanceWithAddress, wallet: Wallet) => TContract`
* `salt`: `Fr`
* `args` (optional): `any[]`
* `constructorNameOrArtifact` (optional): `string | FunctionArtifact`

## Methods## requestReturns the execution payload that allows this operation to happen on chain.

**Signature:**

```
public override async request(opts?: RequestDeployAccountOptions): Promise<ExecutionPayload>
```

**Parameters:**

* `opts` (optional): `RequestDeployAccountOptions`
  + Configuration options.

**Returns:**

`Promise<ExecutionPayload>` - The execution payload for this operation

## convertDeployOptionsToRequestOptions**Signature:**

```
override convertDeployOptionsToRequestOptions(options: DeployOptions): RequestDeployOptions
```

**Parameters:**

* `options`: `DeployOptions`

**Returns:**

`RequestDeployOptions`

---

## `wallet/wallet.ts`## Aliased**Type:** Type Alias

A wrapper type that allows any item to be associated with an alias.

**Signature:**

```
export type Aliased<T> = {  
 alias: string;  
 item: T;  
};
```

**Type Members:**

## aliasThe alias

**Type:** `string`

## itemThe item being aliased.

**Type:** `T`

## SimulateOptions**Type:** Type Alias

Options for simulating interactions with the wallet. Overrides the fee settings of an interaction with a simplified version that only hints at the wallet wether the interaction contains a fee payment method or not

**Signature:**

```
export type SimulateOptions = Omit<SimulateInteractionOptions, 'fee'> & {  
 fee?: GasSettingsOption & FeeEstimationOptions;  
};
```

**Type Members:**

## feeThe fee options

**Type:** `GasSettingsOption & FeeEstimationOptions`

## ProfileOptions**Type:** Type Alias

Options for profiling interactions with the wallet. Overrides the fee settings of an interaction with a simplified version that only hints at the wallet wether the interaction contains a fee payment method or not

**Signature:**

```
export type ProfileOptions = Omit<ProfileInteractionOptions, 'fee'> & {  
 fee?: GasSettingsOption;  
};
```

**Type Members:**

## feeThe fee options

**Type:** `GasSettingsOption`

## SendOptions**Type:** Type Alias

Options for sending/proving interactions with the wallet. Overrides the fee settings of an interaction with a simplified version that only hints at the wallet wether the interaction contains a fee payment method or not

**Signature:**

```
export type SendOptions = Omit<SendInteractionOptions, 'fee'> & {  
 fee?: GasSettingsOption;  
};
```

**Type Members:**

## feeThe fee options

**Type:** `GasSettingsOption`

## BatchableMethods**Type:** Type Alias

Helper type that represents all methods that can be batched.

**Signature:**

```
export type BatchableMethods = Pick<  
 Wallet,  
 'registerContract' | 'sendTx' | 'registerSender' | 'simulateUtility' | 'simulateTx'  
>;
```

## BatchedMethod**Type:** Type Alias

From the batchable methods, we create a type that represents a method call with its name and arguments. This is what the wallet will accept as arguments to the `batch` method.

**Signature:**

```
export type BatchedMethod<T extends keyof BatchableMethods> = {  
 name: T;  
 args: Parameters<BatchableMethods[T]>;  
};
```

**Type Members:**

## nameThe method name

**Type:** `T`

## argsThe method arguments

**Type:** `Parameters<BatchableMethods[T]>`

## BatchedMethodResult**Type:** Type Alias

Helper type to extract the return type of a batched method

**Signature:**

```
export type BatchedMethodResult<T> =  
 T extends BatchedMethod<infer K> ? Awaited<ReturnType<BatchableMethods[K]>> : never;
```

## BatchedMethodResultWrapper**Type:** Type Alias

Wrapper type for batch results that includes the method name for discriminated union deserialization. Each result is wrapped as { name: 'methodName', result: ActualResult } to allow proper deserialization when AztecAddress and TxHash would otherwise be ambiguous (both are hex strings).

**Signature:**

```
export type BatchedMethodResultWrapper<T extends BatchedMethod<keyof BatchableMethods>> = {  
 name: T['name'];  
 result: BatchedMethodResult<T>;  
};
```

**Type Members:**

## nameThe method name

**Type:** `T['name']`

## resultThe method result

**Type:** `BatchedMethodResult<T>`

## BatchResults**Type:** Type Alias

Maps a tuple of BatchedMethod to a tuple of their wrapped return types

**Signature:**

```
export type BatchResults<T extends readonly BatchedMethod<keyof BatchableMethods>[]> = {  
 [K in keyof T]: BatchedMethodResultWrapper<T[K]>;  
};
```

**Type Members:**

## [K in keyof T]**Signature:** `[K in keyof T]: BatchedMethodResultWrapper<T[K]>`

**Key Type:** `keyof T`

**Value Type:** `BatchedMethodResultWrapper<T[K]>`

## PrivateEventFilter**Type:** Type Alias

Filter options when querying private events.

**Signature:**

```
export type PrivateEventFilter = {  
 contractAddress: AztecAddress;  
 scopes: AztecAddress[];  
 txHash?: TxHash;  
 fromBlock?: BlockNumber;  
 toBlock?: BlockNumber;  
};
```

**Type Members:**

## contractAddressThe address of the contract that emitted the events.

**Type:** `AztecAddress`

## scopesAddresses of accounts that are in scope for this filter.

**Type:** `AztecAddress[]`

## txHashTransaction in which the events were emitted.

**Type:** `TxHash`

## fromBlockThe block number from which to start fetching events (inclusive). Optional. If provided, it must be greater or equal than 1. Defaults to the initial L2 block number (INITIAL\_L2\_BLOCK\_NUM).

**Type:** `BlockNumber`

## toBlockThe block number until which to fetch logs (not inclusive). Optional. If provided, it must be greater than fromBlock. Defaults to the latest known block to PXE + 1.

**Type:** `BlockNumber`

## PrivateEvent**Type:** Type Alias

An ABI decoded private event with associated metadata.

**Signature:**

```
export type PrivateEvent<T> = {  
 event: T;  
 metadata: InTx;  
};
```

**Type Members:**

## eventThe ABI decoded event

**Type:** `T`

## metadataMetadata describing event context information such as tx and block

**Type:** `InTx`

## Wallet**Type:** Type Alias

The wallet interface.

**Signature:**

```
export type Wallet = {  
 getContractClassMetadata(id: Fr, includeArtifact?: boolean): Promise<ContractClassMetadata>;  
 getContractMetadata(address: AztecAddress): Promise<ContractMetadata>;  
 getPrivateEvents<T>(  
 eventMetadata: EventMetadataDefinition,  
 eventFilter: PrivateEventFilter,  
 ): Promise<PrivateEvent<T>[]>;  
 getChainInfo(): Promise<ChainInfo>;  
 getTxReceipt(txHash: TxHash): Promise<TxReceipt>;  
 registerSender(address: AztecAddress, alias?: string): Promise<AztecAddress>;  
 getAddressBook(): Promise<Aliased<AztecAddress>[]>;  
 getAccounts(): Promise<Aliased<AztecAddress>[]>;  
 registerContract(  
 instance: ContractInstanceWithAddress,  
 artifact?: ContractArtifact,  
 secretKey?: Fr,  
 ): Promise<ContractInstanceWithAddress>;  
 simulateTx(exec: ExecutionPayload, opts: SimulateOptions): Promise<TxSimulationResult>;  
 simulateUtility(call: FunctionCall, authwits?: AuthWitness[]): Promise<UtilitySimulationResult>;  
 profileTx(exec: ExecutionPayload, opts: ProfileOptions): Promise<TxProfileResult>;  
 sendTx(exec: ExecutionPayload, opts: SendOptions): Promise<TxHash>;  
 createAuthWit(from: AztecAddress, messageHashOrIntent: Fr | IntentInnerHash | CallIntent): Promise<AuthWitness>;  
 batch<const T extends readonly BatchedMethod<keyof BatchableMethods>[]>(methods: T): Promise<BatchResults<T>>;  
};
```

**Type Members:**

## getContractClassMetadata**Signature:**

```
getContractClassMetadata(  
  id: Fr,  
  includeArtifact?: boolean  
): Promise<ContractClassMetadata>
```

**Parameters:**

* `id`: `Fr`
* `includeArtifact` (optional): `boolean`

**Returns:**

`Promise<ContractClassMetadata>`

## getContractMetadata**Signature:**

```
getContractMetadata(address: AztecAddress): Promise<ContractMetadata>
```

**Parameters:**

* `address`: `AztecAddress`

**Returns:**

`Promise<ContractMetadata>`

## getPrivateEvents**Signature:**

```
getPrivateEvents<T>(  
  eventMetadata: EventMetadataDefinition,  
  eventFilter: PrivateEventFilter  
): Promise<PrivateEvent<T>[]>
```

**Parameters:**

* `eventMetadata`: `EventMetadataDefinition`
* `eventFilter`: `PrivateEventFilter`

**Returns:**

`Promise<PrivateEvent<T>[]>`

## getChainInfo**Signature:**

```
getChainInfo(): Promise<ChainInfo>
```

**Returns:**

`Promise<ChainInfo>`

## getTxReceipt**Signature:**

```
getTxReceipt(txHash: TxHash): Promise<TxReceipt>
```

**Parameters:**

* `txHash`: `TxHash`

**Returns:**

`Promise<TxReceipt>`

## registerSender**Signature:**

```
registerSender(  
  address: AztecAddress,  
  alias?: string  
): Promise<AztecAddress>
```

**Parameters:**

* `address`: `AztecAddress`
* `alias` (optional): `string`

**Returns:**

`Promise<AztecAddress>`

## getAddressBook**Signature:**

```
getAddressBook(): Promise<Aliased<AztecAddress>[]>
```

**Returns:**

`Promise<Aliased<AztecAddress>[]>`

## getAccounts**Signature:**

```
getAccounts(): Promise<Aliased<AztecAddress>[]>
```

**Returns:**

`Promise<Aliased<AztecAddress>[]>`

## registerContract**Signature:**

```
registerContract(  
  instance: ContractInstanceWithAddress,  
  artifact?: ContractArtifact,  
  secretKey?: Fr  
): Promise<ContractInstanceWithAddress>
```

**Parameters:**

* `instance`: `ContractInstanceWithAddress`
* `artifact` (optional): `ContractArtifact`
* `secretKey` (optional): `Fr`

**Returns:**

`Promise<ContractInstanceWithAddress>`

## simulateTx**Signature:**

```
simulateTx(  
  exec: ExecutionPayload,  
  opts: SimulateOptions  
): Promise<TxSimulationResult>
```

**Parameters:**

* `exec`: `ExecutionPayload`
* `opts`: `SimulateOptions`

**Returns:**

`Promise<TxSimulationResult>`

## simulateUtility**Signature:**

```
simulateUtility(  
  call: FunctionCall,  
  authwits?: AuthWitness[]  
): Promise<UtilitySimulationResult>
```

**Parameters:**

* `call`: `FunctionCall`
* `authwits` (optional): `AuthWitness[]`

**Returns:**

`Promise<UtilitySimulationResult>`

## profileTx**Signature:**

```
profileTx(  
  exec: ExecutionPayload,  
  opts: ProfileOptions  
): Promise<TxProfileResult>
```

**Parameters:**

* `exec`: `ExecutionPayload`
* `opts`: `ProfileOptions`

**Returns:**

`Promise<TxProfileResult>`

## sendTx**Signature:**

```
sendTx(  
  exec: ExecutionPayload,  
  opts: SendOptions  
): Promise<TxHash>
```

**Parameters:**

* `exec`: `ExecutionPayload`
* `opts`: `SendOptions`

**Returns:**

`Promise<TxHash>`

## createAuthWit**Signature:**

```
createAuthWit(  
  from: AztecAddress,  
  messageHashOrIntent: Fr | IntentInnerHash | CallIntent  
): Promise<AuthWitness>
```

**Parameters:**

* `from`: `AztecAddress`
* `messageHashOrIntent`: `Fr | IntentInnerHash | CallIntent`

**Returns:**

`Promise<AuthWitness>`

## batch**Signature:**

```
batch<const T extends readonly BatchedMethod<keyof BatchableMethods>[]>(methods: T): Promise<BatchResults<T>>
```

**Parameters:**

* `methods`: `T`

**Returns:**

`Promise<BatchResults<T>>`

## FunctionCallSchema**Type:** Constant

**Value Type:** `any`

## ExecutionPayloadSchema**Type:** Constant

**Value Type:** `any`

## GasSettingsOptionSchema**Type:** Constant

**Value Type:** `any`

## WalletSimulationFeeOptionSchema**Type:** Constant

**Value Type:** `any`

## SendOptionsSchema**Type:** Constant

**Value Type:** `any`

## SimulateOptionsSchema**Type:** Constant

**Value Type:** `any`

## ProfileOptionsSchema**Type:** Constant

**Value Type:** `any`

## MessageHashOrIntentSchema**Type:** Constant

**Value Type:** `any`

## BatchedMethodSchema**Type:** Constant

**Value Type:** `any`

## ContractMetadataSchema**Type:** Constant

**Value Type:** `any`

## ContractClassMetadataSchema**Type:** Constant

**Value Type:** `any`

## EventMetadataDefinitionSchema**Type:** Constant

**Value Type:** `any`

## PrivateEventSchema**Type:** Constant

**Value Type:** `ZodFor<PrivateEvent<AbiDecoded>>`

## PrivateEventFilterSchema**Type:** Constant

**Value Type:** `any`

## WalletSchema**Type:** Constant

**Value Type:** `ApiSchemaFor<Wallet>`

---


# Aztec CLI

Source: https://docs.aztec.network/developers/docs/aztec-cli/cli_reference

Version: Devnet (v3.0.0-devnet.20251212)

On this page

*This documentation is auto-generated from the `aztec` CLI help output.*

*Generated: Tue 23 Dec 2025 02:06:21 AM UTC*

*Command: `aztec`*

## Table of Contents* [Table of Contents](#table-of-contents)
* [aztec](#aztec)
  + [Subcommands](#subcommands)
  + [aztec add-l1-validator](#aztec-add-l1-validator)
  + [aztec advance-epoch](#aztec-advance-epoch)
  + [aztec block-number](#aztec-block-number)
  + [aztec bridge-erc20](#aztec-bridge-erc20)
  + [aztec codegen](#aztec-codegen)
  + [aztec compute-selector](#aztec-compute-selector)
  + [aztec debug-rollup](#aztec-debug-rollup)
  + [aztec decode-enr](#aztec-decode-enr)
  + [aztec deploy-l1-contracts](#aztec-deploy-l1-contracts)
  + [aztec deploy-new-rollup](#aztec-deploy-new-rollup)
  + [aztec deposit-governance-tokens](#aztec-deposit-governance-tokens)
  + [aztec example-contracts](#aztec-example-contracts)
  + [aztec execute-governance-proposal](#aztec-execute-governance-proposal)
  + [aztec fast-forward-epochs](#aztec-fast-forward-epochs)
  + [aztec generate-bls-keypair](#aztec-generate-bls-keypair)
  + [aztec generate-bootnode-enr](#aztec-generate-bootnode-enr)
  + [aztec generate-keys](#aztec-generate-keys)
  + [aztec generate-l1-account](#aztec-generate-l1-account)
  + [aztec generate-p2p-private-key](#aztec-generate-p2p-private-key)
  + [aztec generate-secret-and-hash](#aztec-generate-secret-and-hash)
  + [aztec get-block](#aztec-get-block)
  + [aztec get-canonical-sponsored-fpc-address](#aztec-get-canonical-sponsored-fpc-address)
  + [aztec get-current-base-fee](#aztec-get-current-base-fee)
  + [aztec get-l1-addresses](#aztec-get-l1-addresses)
  + [aztec get-l1-balance](#aztec-get-l1-balance)
  + [aztec get-l1-to-l2-message-witness](#aztec-get-l1-to-l2-message-witness)
  + [aztec get-logs](#aztec-get-logs)
  + [aztec get-node-info](#aztec-get-node-info)
  + [aztec inspect-contract](#aztec-inspect-contract)
  + [aztec parse-parameter-struct](#aztec-parse-parameter-struct)
  + [aztec preload-crs](#aztec-preload-crs)
  + [aztec propose-with-lock](#aztec-propose-with-lock)
  + [aztec prune-rollup](#aztec-prune-rollup)
  + [aztec remove-l1-validator](#aztec-remove-l1-validator)
  + [aztec sequencers](#aztec-sequencers)
  + [aztec setup-protocol-contracts](#aztec-setup-protocol-contracts)
  + [aztec start](#aztec-start)
  + [aztec trigger-seed-snapshot](#aztec-trigger-seed-snapshot)
  + [aztec update](#aztec-update)
  + [aztec validator-keys|valKeys](#aztec-validator-keysvalkeys)
  + [aztec vote-on-governance-proposal](#aztec-vote-on-governance-proposal)

## aztecAztec command line interface

**Usage:**

```
aztec [options] [command]
```

**Available Commands:**

* `add-l1-validator [options]` - Adds a validator to the L1 rollup contract via a direct deposit.
* `advance-epoch [options]` - Use L1 cheat codes to warp time until the next epoch.
* `block-number [options]` - Gets the current Aztec L2 block number.
* `bridge-erc20 [options] <amount> <recipient>` - Bridges ERC20 tokens to L2.
* `codegen [options] <noir-abi-path>` - Validates and generates an Aztec Contract ABI from Noir ABI.
* `compute-selector <functionSignature>` - Given a function signature, it computes a selector
* `debug-rollup [options]` - Debugs the rollup contract.
* `decode-enr <enr>` - Decodes an ENR record
* `deploy-l1-contracts [options]` - Deploys all necessary Ethereum contracts for Aztec.
* `deploy-new-rollup [options]` - Deploys a new rollup contract and adds it to the registry (if you are the owner).
* `deposit-governance-tokens [options]` - Deposits governance tokens to the governance contract.
* `example-contracts` - Lists the example contracts available to deploy from @aztec/noir-contracts.js
* `execute-governance-proposal [options]` - Executes a governance proposal.
* `fast-forward-epochs [options]` - Fast forwards the epoch of the L1 rollup contract.
* `generate-bls-keypair [options]` - Generate a BLS keypair with convenience flags
* `generate-bootnode-enr [options] <privateKey> <p2pIp> <p2pPort>` - Generates the encoded ENR record for a bootnode.
* `generate-keys [options]` - Generates encryption and signing private keys.
* `generate-l1-account [options]` - Generates a new private key for an account on L1.
* `generate-p2p-private-key` - Generates a LibP2P peer private key.
* `generate-secret-and-hash` - Generates an arbitrary secret (Fr), and its hash (using aztec-nr defaults)
* `get-block [options] [blockNumber]` - Gets info for a given block or latest.
* `get-canonical-sponsored-fpc-address` - Gets the canonical SponsoredFPC address for this any testnet running on the same version as this CLI
* `get-current-base-fee [options]` - Gets the current base fee.
* `get-l1-addresses [options]` - Gets the addresses of the L1 contracts.
* `get-l1-balance [options] <who>` - Gets the balance of an ERC token in L1 for the given Ethereum address.
* `get-l1-to-l2-message-witness [options]` - Gets a L1 to L2 message witness.
* `get-logs [options]` - Gets all the public logs from an intersection of all the filter params.
* `get-node-info [options]` - Gets the information of an Aztec node from a PXE or directly from an Aztec node.
* `help [command]` - display help for command
* `inspect-contract <contractArtifactFile>` - Shows list of external callable functions for a contract
* `parse-parameter-struct [options] <encodedString>` - Helper for parsing an encoded string into a contract's parameter struct.
* `preload-crs` - Preload the points data needed for proving and verifying
* `propose-with-lock [options]` - Makes a proposal to governance with a lock
* `prune-rollup [options]` - Prunes the pending chain on the rollup contract.
* `remove-l1-validator [options]` - Removes a validator to the L1 rollup contract.
* `sequencers [options] <command> [who]` - Manages or queries registered sequencers on the L1 rollup contract.
* `setup-protocol-contracts [options]` - Bootstrap the blockchain by initializing all the protocol contracts
* `start [options]` - Starts Aztec modules. Options for each module can be set as key-value pairs (e.g. "option1=value1,option2=value2") or as environment variables.
* `trigger-seed-snapshot [options]` - Triggers a seed snapshot for the next epoch.
* `update [options] [projectPath]` - Updates Nodejs and Noir dependencies
* `validator-keys|valKeys` - Manage validator keystores for node operators
* `vote-on-governance-proposal [options]` - Votes on a governance proposal.

**Options:**

* `-V --version` - output the version number
* `-h --help` - display help for command
* `--name <name>` - Name of the package
* `--contract` - Use a contract template (default)
* `--lib` - Use a library template
* `--bin` - Use a binary template
* `--name <name>` - Name of the package
* `--contract` - Use a contract template (default)
* `--lib` - Use a library template
* `--bin` - Use a binary template

## Subcommands## aztec add-l1-validator```
Usage: aztec add-l1-validator [options]  
  
Adds a validator to the L1 rollup contract via a direct deposit.  
  
Options:  
  --l1-rpc-urls <string>       List of Ethereum host URLs. Chain identifiers  
                               localhost and testnet can be used (comma  
                               separated) (default:  
                               ["http://host.docker.internal:8545"], env:  
                               ETHEREUM_HOSTS)  
  --network <string>           Network to execute against (env: NETWORK)  
  -pk, --private-key <string>  The private key to use sending the transaction  
  -m, --mnemonic <string>      The mnemonic to use sending the transaction  
                               (default: "test test test test test test test  
                               test test test test junk")  
  -c, --l1-chain-id <number>   Chain ID of the ethereum host (default: 31337,  
                               env: L1_CHAIN_ID)  
  --attester <address>         ethereum address of the attester  
  --withdrawer <address>       ethereum address of the withdrawer  
  --bls-secret-key <string>    The BN254 scalar field element used as a secret  
                               key for BLS signatures. Will be associated with  
                               the attester address.  
  --move-with-latest-rollup    Whether to move with the latest rollup (default:  
                               true)  
  --rollup <string>            Rollup contract address  
  -h, --help                   display help for command
```

## aztec advance-epoch```
Usage: aztec advance-epoch [options]  
  
Use L1 cheat codes to warp time until the next epoch.  
  
Options:  
  --l1-rpc-urls <string>   List of Ethereum host URLs. Chain identifiers  
                           localhost and testnet can be used (comma separated)  
                           (default: ["http://host.docker.internal:8545"], env:  
                           ETHEREUM_HOSTS)  
  -n, --node-url <string>  URL of the Aztec node (default:  
                           "http://host.docker.internal:8080", env:  
                           AZTEC_NODE_URL)  
  -h, --help               display help for command
```

## aztec block-number```
Usage: aztec block-number [options]  
  
Gets the current Aztec L2 block number.  
  
Options:  
  -n, --node-url <string>  URL of the Aztec node (default:  
                           "http://host.docker.internal:8080", env:  
                           AZTEC_NODE_URL)  
  -h, --help               display help for command
```

## aztec bridge-erc20```
Usage: aztec bridge-erc20 [options] <amount> <recipient>  
  
Bridges ERC20 tokens to L2.  
  
Arguments:  
  amount                      The amount of Fee Juice to mint and bridge.  
  recipient                   Aztec address of the recipient.  
  
Options:  
  --l1-rpc-urls <string>      List of Ethereum host URLs. Chain identifiers  
                              localhost and testnet can be used (comma  
                              separated) (default:  
                              ["http://host.docker.internal:8545"], env:  
                              ETHEREUM_HOSTS)  
  -m, --mnemonic <string>     The mnemonic to use for deriving the Ethereum  
                              address that will mint and bridge (default: "test  
                              test test test test test test test test test test  
                              junk")  
  --mint                      Mint the tokens on L1 (default: false)  
  --private                   If the bridge should use the private flow  
                              (default: false)  
  -c, --l1-chain-id <number>  Chain ID of the ethereum host (default: 31337,  
                              env: L1_CHAIN_ID)  
  -t, --token <string>        The address of the token to bridge  
  -p, --portal <string>       The address of the portal contract  
  -f, --faucet <string>       The address of the faucet contract (only used if  
                              minting)  
  --l1-private-key <string>   The private key to use for deployment  
  --json                      Output the claim in JSON format  
  -h, --help                  display help for command
```

## aztec codegen```
Usage: aztec codegen [options] <noir-abi-path>  
  
Validates and generates an Aztec Contract ABI from Noir ABI.  
  
Arguments:  
  noir-abi-path        Path to the Noir ABI or project dir.  
  
Options:  
  -o, --outdir <path>  Output folder for the generated code.  
  -f, --force          Force code generation even when the contract has not  
                       changed.  
  -h, --help           display help for command
```

## aztec compute-selector```
Usage: aztec compute-selector [options] <functionSignature>  
  
Given a function signature, it computes a selector  
  
Arguments:  
  functionSignature  Function signature to compute selector for e.g. foo(Field)  
  
Options:  
  -h, --help         display help for command
```

## aztec debug-rollup```
Usage: aztec debug-rollup [options]  
  
Debugs the rollup contract.  
  
Options:  
  --l1-rpc-urls <string>      List of Ethereum host URLs. Chain identifiers  
                              localhost and testnet can be used (comma  
                              separated) (default:  
                              ["http://host.docker.internal:8545"], env:  
                              ETHEREUM_HOSTS)  
  -c, --l1-chain-id <number>  Chain ID of the ethereum host (default: 31337,  
                              env: L1_CHAIN_ID)  
  --rollup <address>          ethereum address of the rollup contract  
  -h, --help                  display help for command
```

## aztec decode-enr```
Usage: aztec decode-enr [options] <enr>  
  
Decodes and ENR record  
  
Arguments:  
  enr         The encoded ENR string  
  
Options:  
  -h, --help  display help for command
```

## aztec deploy-l1-contracts```
Usage: aztec deploy-l1-contracts [options]  
  
Deploys all necessary Ethereum contracts for Aztec.  
  
Options:  
  --l1-rpc-urls <string>             List of Ethereum host URLs. Chain  
                                     identifiers localhost and testnet can be  
                                     used (comma separated) (default:  
                                     ["http://host.docker.internal:8545"], env:  
                                     ETHEREUM_HOSTS)  
  -pk, --private-key <string>        The private key to use for deployment  
  --validators <string>              Comma separated list of validators  
  -m, --mnemonic <string>            The mnemonic to use in deployment  
                                     (default: "test test test test test test  
                                     test test test test test junk")  
  -i, --mnemonic-index <number>      The index of the mnemonic to use in  
                                     deployment (default: 0)  
  -c, --l1-chain-id <number>         Chain ID of the ethereum host (default:  
                                     31337, env: L1_CHAIN_ID)  
  --salt <number>                    The optional salt to use in deployment  
  --json                             Output the contract addresses in JSON  
                                     format  
  --test-accounts                    Populate genesis state with initial fee  
                                     juice for test accounts  
  --sponsored-fpc                    Populate genesis state with a testing  
                                     sponsored FPC contract  
  --accelerated-test-deployments     Fire and forget deployment transactions,  
                                     use in testing only (default: false)  
  --real-verifier                    Deploy the real verifier (default: false)  
  --existing-token <address>         Use an existing ERC20 for both fee and  
                                     staking  
  --create-verification-json [path]  Create JSON file for etherscan contract  
                                     verification (default: false)  
  -h, --help                         display help for command
```

## aztec deploy-new-rollup```
Usage: aztec deploy-new-rollup [options]  
  
Deploys a new rollup contract and adds it to the registry (if you are the  
owner).  
  
Options:  
  -r, --registry-address <string>    The address of the registry contract  
  --l1-rpc-urls <string>             List of Ethereum host URLs. Chain  
                                     identifiers localhost and testnet can be  
                                     used (comma separated) (default:  
                                     ["http://host.docker.internal:8545"], env:  
                                     ETHEREUM_HOSTS)  
  -pk, --private-key <string>        The private key to use for deployment  
  --validators <string>              Comma separated list of validators  
  -m, --mnemonic <string>            The mnemonic to use in deployment  
                                     (default: "test test test test test test  
                                     test test test test test junk")  
  -i, --mnemonic-index <number>      The index of the mnemonic to use in  
                                     deployment (default: 0)  
  -c, --l1-chain-id <number>         Chain ID of the ethereum host (default:  
                                     31337, env: L1_CHAIN_ID)  
  --salt <number>                    The optional salt to use in deployment  
  --json                             Output the contract addresses in JSON  
                                     format  
  --test-accounts                    Populate genesis state with initial fee  
                                     juice for test accounts  
  --sponsored-fpc                    Populate genesis state with a testing  
                                     sponsored FPC contract  
  --real-verifier                    Deploy the real verifier (default: false)  
  --create-verification-json [path]  Create JSON file for etherscan contract  
                                     verification (default: false)  
  -h, --help                         display help for command
```

## aztec deposit-governance-tokens```
Usage: aztec deposit-governance-tokens [options]  
  
Deposits governance tokens to the governance contract.  
  
Options:  
  -r, --registry-address <string>  The address of the registry contract  
  --recipient <string>             The recipient of the tokens  
  -a, --amount <string>            The amount of tokens to deposit  
  --mint                           Mint the tokens on L1 (default: false)  
  --l1-rpc-urls <string>           List of Ethereum host URLs. Chain  
                                   identifiers localhost and testnet can be  
                                   used (comma separated) (default:  
                                   ["http://host.docker.internal:8545"], env:  
                                   ETHEREUM_HOSTS)  
  -c, --l1-chain-id <number>       Chain ID of the ethereum host (default:  
                                   31337, env: L1_CHAIN_ID)  
  -p, --private-key <string>       The private key to use to deposit  
  -m, --mnemonic <string>          The mnemonic to use to deposit (default:  
                                   "test test test test test test test test  
                                   test test test junk")  
  -i, --mnemonic-index <number>    The index of the mnemonic to use to deposit  
                                   (default: 0)  
  -h, --help                       display help for command
```

## aztec example-contracts```
Usage: aztec example-contracts [options]  
  
Lists the example contracts available to deploy from @aztec/noir-contracts.js  
  
Options:  
  -h, --help  display help for command
```

## aztec execute-governance-proposal```
Usage: aztec execute-governance-proposal [options]  
  
Executes a governance proposal.  
  
Options:  
  -p, --proposal-id <string>       The ID of the proposal  
  -r, --registry-address <string>  The address of the registry contract  
  --wait <boolean>                 Whether to wait until the proposal is  
                                   executable  
  --l1-rpc-urls <string>           List of Ethereum host URLs. Chain  
                                   identifiers localhost and testnet can be  
                                   used (comma separated) (default:  
                                   ["http://host.docker.internal:8545"], env:  
                                   ETHEREUM_HOSTS)  
  -c, --l1-chain-id <number>       Chain ID of the ethereum host (default:  
                                   31337, env: L1_CHAIN_ID)  
  -pk, --private-key <string>      The private key to use to vote  
  -m, --mnemonic <string>          The mnemonic to use to vote (default: "test  
                                   test test test test test test test test test  
                                   test junk")  
  -i, --mnemonic-index <number>    The index of the mnemonic to use to vote  
                                   (default: 0)  
  -h, --help                       display help for command
```

## aztec fast-forward-epochs*Help for this command is currently unavailable due to a technical issue with option serialization.*

## aztec generate-bls-keypair```
Usage: aztec generate-bls-keypair [options]  
  
Generate a BLS keypair with convenience flags  
  
Options:  
  --mnemonic <mnemonic>  Mnemonic for BLS derivation  
  --ikm <hex>            Initial keying material for BLS (alternative to  
                         mnemonic)  
  --bls-path <path>      EIP-2334 path (default m/12381/3600/0/0/0)  
  --g2                   Derive on G2 subgroup  
  --compressed           Output compressed public key  
  --json                 Print JSON output to stdout  
  --out <file>           Write output to file  
  -h, --help             display help for command
```

## aztec generate-bootnode-enr```
Usage: aztec generate-bootnode-enr [options] <privateKey> <p2pIp> <p2pPort>  
  
Generates the encoded ENR record for a bootnode.  
  
Arguments:  
  privateKey                  The peer id private key of the bootnode  
  p2pIp                       The bootnode P2P IP address  
  p2pPort                     The bootnode P2P port  
  
Options:  
  -c, --l1-chain-id <number>  Chain ID of the ethereum host (default: 31337,  
                              env: L1_CHAIN_ID)  
  -h, --help                  display help for command
```

## aztec generate-keys```
Usage: aztec generate-keys [options]  
  
Generates and encryption and signing private key pair.  
  
Options:  
  --json      Output the keys in JSON format  
  -h, --help  display help for command
```

## aztec generate-l1-account```
Usage: aztec generate-l1-account [options]  
  
Generates a new private key for an account on L1.  
  
Options:  
  --json      Output the private key in JSON format  
  -h, --help  display help for command
```

## aztec generate-p2p-private-key```
Usage: aztec generate-p2p-private-key [options]  
  
Generates a private key that can be used for running a node on a LibP2P  
network.  
  
Options:  
  -h, --help  display help for command
```

## aztec generate-secret-and-hash```
Usage: aztec generate-secret-and-hash [options]  
  
Generates an arbitrary secret (Fr), and its hash (using aztec-nr defaults)  
  
Options:  
  -h, --help  display help for command
```

## aztec get-block```
Usage: aztec get-block [options] [blockNumber]  
  
Gets info for a given block or latest.  
  
Arguments:  
  blockNumber              Block height  
  
Options:  
  -n, --node-url <string>  URL of the Aztec node (default:  
                           "http://host.docker.internal:8080", env:  
                           AZTEC_NODE_URL)  
  -h, --help               display help for command
```

## aztec get-canonical-sponsored-fpc-address```
Usage: aztec get-canonical-sponsored-fpc-address [options]  
  
Gets the canonical SponsoredFPC address for this any testnet running on the  
same version as this CLI  
  
Options:  
  -h, --help  display help for command
```

## aztec get-current-base-fee```
Usage: aztec get-current-base-fee [options]  
  
Gets the current base fee.  
  
Options:  
  -n, --node-url <string>  URL of the Aztec node (default:  
                           "http://host.docker.internal:8080", env:  
                           AZTEC_NODE_URL)  
  -h, --help               display help for command
```

## aztec get-l1-addresses```
Usage: aztec get-l1-addresses [options]  
  
Gets the addresses of the L1 contracts.  
  
Options:  
  -r, --registry-address <string>  The address of the registry contract  
  --l1-rpc-urls <string>           List of Ethereum host URLs. Chain  
                                   identifiers localhost and testnet can be  
                                   used (comma separated) (default:  
                                   ["http://host.docker.internal:8545"], env:  
                                   ETHEREUM_HOSTS)  
  -v, --rollup-version <number>    The version of the rollup  
  -c, --l1-chain-id <number>       Chain ID of the ethereum host (default:  
                                   31337, env: L1_CHAIN_ID)  
  --json                           Output the addresses in JSON format  
  -h, --help                       display help for command
```

## aztec get-l1-balance```
Usage: aztec get-l1-balance [options] <who>  
  
Gets the balance of an ERC token in L1 for the given Ethereum address.  
  
Arguments:  
  who                         Ethereum address to check.  
  
Options:  
  --l1-rpc-urls <string>      List of Ethereum host URLs. Chain identifiers  
                              localhost and testnet can be used (comma  
                              separated) (default:  
                              ["http://host.docker.internal:8545"], env:  
                              ETHEREUM_HOSTS)  
  -t, --token <string>        The address of the token to check the balance of  
  -c, --l1-chain-id <number>  Chain ID of the ethereum host (default: 31337,  
                              env: L1_CHAIN_ID)  
  --json                      Output the balance in JSON format  
  -h, --help                  display help for command
```

## aztec get-l1-to-l2-message-witness```
Usage: aztec get-l1-to-l2-message-witness [options]  
  
Gets a L1 to L2 message witness.  
  
Options:  
  -ca, --contract-address <address>  Aztec address of the contract.  
  --message-hash <messageHash>       The L1 to L2 message hash.  
  --secret <secret>                  The secret used to claim the L1 to L2  
                                     message  
  -n, --node-url <string>            URL of the Aztec node (default:  
                                     "http://host.docker.internal:8080", env:  
                                     AZTEC_NODE_URL)  
  -h, --help                         display help for command
```

## aztec get-logs```
Usage: aztec get-logs [options]  
  
Gets all the public logs from an intersection of all the filter params.  
  
Options:  
  -tx, --tx-hash <txHash>            A transaction hash to get the receipt for.  
  -fb, --from-block <blockNum>       Initial block number for getting logs  
                                     (defaults to 1).  
  -tb, --to-block <blockNum>         Up to which block to fetch logs (defaults  
                                     to latest).  
  -al --after-log <logId>            ID of a log after which to fetch the logs.  
  -ca, --contract-address <address>  Contract address to filter logs by.  
  -n, --node-url <string>            URL of the Aztec node (default:  
                                     "http://host.docker.internal:8080", env:  
                                     AZTEC_NODE_URL)  
  --follow                           If set, will keep polling for new logs  
                                     until interrupted.  
  -h, --help                         display help for command
```

## aztec get-node-info```
Usage: aztec get-node-info [options]  
  
Gets the information of an Aztec node from a PXE or directly from an Aztec  
node.  
  
Options:  
  --json                   Emit output as json  
  -n, --node-url <string>  URL of the Aztec node (default:  
                           "http://host.docker.internal:8080", env:  
                           AZTEC_NODE_URL)  
  -h, --help               display help for command
```

## aztec inspect-contract```
Usage: aztec inspect-contract [options] <contractArtifactFile>  
  
Shows list of external callable functions for a contract  
  
Arguments:  
  contractArtifactFile  A compiled Noir contract's artifact in JSON format or  
                        name of a contract artifact exported by  
                        @aztec/noir-contracts.js  
  
Options:  
  -h, --help            display help for command
```

## aztec parse-parameter-struct```
Usage: aztec parse-parameter-struct [options] <encodedString>  
  
Helper for parsing an encoded string into a contract's parameter struct.  
  
Arguments:  
  encodedString                           The encoded hex string  
  
Options:  
  -c, --contract-artifact <fileLocation>  A compiled Aztec.nr contract's ABI in JSON format or name of a contract ABI exported by @aztec/noir-contracts.js  
  -p, --parameter <parameterName>         The name of the struct parameter to decode into  
  -h, --help                              display help for command
```

## aztec preload-crs```
Usage: aztec preload-crs [options]  
  
Preload the points data needed for proving and verifying  
  
Options:  
  -h, --help  display help for command
```

## aztec propose-with-lock```
Usage: aztec propose-with-lock [options]  
  
Makes a proposal to governance with a lock  
  
Options:  
  -r, --registry-address <string>  The address of the registry contract  
  -p, --payload-address <string>   The address of the payload contract  
  --l1-rpc-urls <string>           List of Ethereum host URLs. Chain  
                                   identifiers localhost and testnet can be  
                                   used (comma separated) (default:  
                                   ["http://host.docker.internal:8545"], env:  
                                   ETHEREUM_HOSTS)  
  -c, --l1-chain-id <number>       Chain ID of the ethereum host (default:  
                                   31337, env: L1_CHAIN_ID)  
  -pk, --private-key <string>      The private key to use to propose  
  -m, --mnemonic <string>          The mnemonic to use to propose (default:  
                                   "test test test test test test test test  
                                   test test test junk")  
  -i, --mnemonic-index <number>    The index of the mnemonic to use to propose  
                                   (default: 0)  
  --json                           Output the proposal ID in JSON format  
  -h, --help                       display help for command
```

## aztec prune-rollup```
Usage: aztec prune-rollup [options]  
  
Prunes the pending chain on the rollup contract.  
  
Options:  
  --l1-rpc-urls <string>       List of Ethereum host URLs. Chain identifiers  
                               localhost and testnet can be used (comma  
                               separated) (default:  
                               ["http://host.docker.internal:8545"], env:  
                               ETHEREUM_HOSTS)  
  -pk, --private-key <string>  The private key to use for deployment  
  -m, --mnemonic <string>      The mnemonic to use in deployment (default:  
                               "test test test test test test test test test  
                               test test junk")  
  -c, --l1-chain-id <number>   Chain ID of the ethereum host (default: 31337,  
                               env: L1_CHAIN_ID)  
  --rollup <address>           ethereum address of the rollup contract  
  -h, --help                   display help for command
```

## aztec remove-l1-validator```
Usage: aztec remove-l1-validator [options]  
  
Removes a validator to the L1 rollup contract.  
  
Options:  
  --l1-rpc-urls <string>       List of Ethereum host URLs. Chain identifiers  
                               localhost and testnet can be used (comma  
                               separated) (default:  
                               ["http://host.docker.internal:8545"], env:  
                               ETHEREUM_HOSTS)  
  -pk, --private-key <string>  The private key to use for deployment  
  -m, --mnemonic <string>      The mnemonic to use in deployment (default:  
                               "test test test test test test test test test  
                               test test junk")  
  -c, --l1-chain-id <number>   Chain ID of the ethereum host (default: 31337,  
                               env: L1_CHAIN_ID)  
  --validator <address>        ethereum address of the validator  
  --rollup <address>           ethereum address of the rollup contract  
  -h, --help                   display help for command
```

## aztec sequencers```
Usage: aztec sequencers [options] <command> [who]  
  
Manages or queries registered sequencers on the L1 rollup contract.  
  
Arguments:  
  command                     Command to run: list, add, remove, who-next  
  who                         Who to add/remove  
  
Options:  
  --l1-rpc-urls <string>      List of Ethereum host URLs. Chain identifiers  
                              localhost and testnet can be used (comma  
                              separated) (default:  
                              ["http://host.docker.internal:8545"])  
  -m, --mnemonic <string>     The mnemonic for the sender of the tx (default:  
                              "test test test test test test test test test  
                              test test junk")  
  --block-number <number>     Block number to query next sequencer for  
  -n, --node-url <string>     URL of the Aztec node (default:  
                              "http://host.docker.internal:8080", env:  
                              AZTEC_NODE_URL)  
  -c, --l1-chain-id <number>  Chain ID of the ethereum host (default: 31337,  
                              env: L1_CHAIN_ID)  
  -h, --help                  display help for command
```

## aztec setup-protocol-contracts```
Usage: aztec setup-protocol-contracts [options]  
  
Bootstrap the blockchain by initializing all the protocol contracts  
  
Options:  
  -n, --node-url <string>  URL of the Aztec node (default:  
                           "http://host.docker.internal:8080", env:  
                           AZTEC_NODE_URL)  
  --testAccounts           Deploy funded test accounts.  
  --json                   Output the contract addresses in JSON format  
  -h, --help               display help for command
```

## aztec start**MISC**

* `--network <value>`
  Network to run Aztec on
  *Environment: `$NETWORK`*
* `--auto-update <value>` (default: `disabled`)
  The auto update mode for this node
  *Environment: `$AUTO_UPDATE`*
* `--auto-update-url <value>`
  Base URL to check for updates
  *Environment: `$AUTO_UPDATE_URL`*
* `--sync-mode <value>` (default: `snapshot`)
  Set sync mode to `full` to always sync via L1, `snapshot` to download a snapshot if there is no local data, `force-snapshot` to download even if there is local data.
  *Environment: `$SYNC_MODE`*
* `--snapshots-urls <value>`
  Base URLs for snapshots index, comma-separated.
  *Environment: `$SYNC_SNAPSHOTS_URLS`*
* `--fisherman-mode`
  Whether to run in fisherman mode.
  *Environment: `$FISHERMAN_MODE`*
* `--local-network`
  Starts Aztec Local Network
* `--local-network.l1Mnemonic <value>` (default: `test test test test test test test test test test test junk`)
  Mnemonic for L1 accounts. Will be used
  *Environment: `$MNEMONIC`*
* `--local-network.deployAztecContractsSalt <value>`
  Numeric salt for deploying L1 Aztec contracts before starting the local network. Needs mnemonic or private key to be set.
  *Environment: `$DEPLOY_AZTEC_CONTRACTS_SALT`*

**API**

* `--port <value>` (default: `8080`)
  Port to run the Aztec Services on
  *Environment: `$AZTEC_PORT`*
* `--admin-port <value>` (default: `8880`)
  Port to run admin APIs of Aztec Services on
  *Environment: `$AZTEC_ADMIN_PORT`*
* `--api-prefix <value>`
  Prefix for API routes on any service that is started
  *Environment: `$API_PREFIX`*

**ETHEREUM**

* `--l1-chain-id <value>`
  The chain ID of the ethereum host.
  *Environment: `$L1_CHAIN_ID`*
* `--l1-rpc-urls <value>`
  List of URLs of Ethereum RPC nodes that services will connect to (comma separated).
  *Environment: `$ETHEREUM_HOSTS`*
* `--l1-consensus-host-urls <value>`
  List of URLs of the Ethereum consensus nodes that services will connect to (comma separated)
  *Environment: `$L1_CONSENSUS_HOST_URLS`*
* `--l1-consensus-host-api-keys <value>`
  List of API keys for the corresponding L1 consensus clients, if needed. Added to the end of the corresponding URL as "?key=<api-key>" unless a header is defined
  *Environment: `$L1_CONSENSUS_HOST_API_KEYS`*
* `--l1-consensus-host-api-key-headers <value>`
  List of header names for the corresponding L1 consensus client API keys, if needed. Added to the corresponding request as "<api-key-header>: <api-key>"
  *Environment: `$L1_CONSENSUS_HOST_API_KEY_HEADERS`*
* `--registry-address <value>`
  The deployed L1 registry contract address.
  *Environment: `$REGISTRY_CONTRACT_ADDRESS`*
* `--rollup-version <value>`
  The version of the rollup.
  *Environment: `$ROLLUP_VERSION`*

**STORAGE**

* `--data-directory <value>`
  Optional dir to store data. If omitted will store in memory.
  *Environment: `$DATA_DIRECTORY`*
* `--data-store-map-size-kb <value>` (default: `134217728`)
  The maximum possible size of a data store DB in KB. Can be overridden by component-specific options.
  *Environment: `$DATA_STORE_MAP_SIZE_KB`*

**WORLD STATE**

* `--world-state-data-directory <value>`
  Optional directory for the world state database
  *Environment: `$WS_DATA_DIRECTORY`*
* `--world-state-db-map-size-kb <value>`
  The maximum possible size of the world state DB in KB. Overwrites the general dataStoreMapSizeKb.
  *Environment: `$WS_DB_MAP_SIZE_KB`*
* `--world-state-block-history <value>` (default: `64`)
  The number of historic blocks to maintain. Values less than 1 mean all history is maintained
  *Environment: `$WS_NUM_HISTORIC_BLOCKS`*

**AZTEC NODE**

* `--node`
  Starts Aztec Node with options

**ARCHIVER**

* `--archiver`
  Starts Aztec Archiver with options
* `--archiver.blobSinkUrl <value>`
  The URL of the blob sink
  *Environment: `$BLOB_SINK_URL`*
* `--archiver.blobSinkMapSizeKb <value>`
  The maximum possible size of the blob sink DB in KB. Overwrites the general dataStoreMapSizeKb.
  *Environment: `$BLOB_SINK_MAP_SIZE_KB`*
* `--archiver.blobAllowEmptySources <value>`
  Whether to allow having no blob sources configured during startup
  *Environment: `$BLOB_ALLOW_EMPTY_SOURCES`*
* `--archiver.archiveApiUrl <value>`
  The URL of the archive API
  *Environment: `$BLOB_SINK_ARCHIVE_API_URL`*
* `--archiver.archiverPollingIntervalMS <value>` (default: `500`)
  The polling interval in ms for retrieving new L2 blocks and encrypted logs.
  *Environment: `$ARCHIVER_POLLING_INTERVAL_MS`*
* `--archiver.archiverBatchSize <value>` (default: `100`)
  The number of L2 blocks the archiver will attempt to download at a time.
  *Environment: `$ARCHIVER_BATCH_SIZE`*
* `--archiver.maxLogs <value>` (default: `1000`)
  The max number of logs that can be obtained in 1 "getPublicLogs" call.
  *Environment: `$ARCHIVER_MAX_LOGS`*
* `--archiver.archiverStoreMapSizeKb <value>`
  The maximum possible size of the archiver DB in KB. Overwrites the general dataStoreMapSizeKb.
  *Environment: `$ARCHIVER_STORE_MAP_SIZE_KB`*
* `--archiver.skipValidateBlockAttestations <value>`
  Whether to skip validating block attestations (use only for testing).
* `--archiver.maxAllowedEthClientDriftSeconds <value>` (default: `300`)
  Maximum allowed drift in seconds between the Ethereum client and current time.
  *Environment: `$MAX_ALLOWED_ETH_CLIENT_DRIFT_SECONDS`*
* `--archiver.ethereumAllowNoDebugHosts <value>` (default: `true`)
  Whether to allow starting the archiver without debug/trace method support on Ethereum hosts
  *Environment: `$ETHEREUM_ALLOW_NO_DEBUG_HOSTS`*

**SEQUENCER**

* `--sequencer`
  Starts Aztec Sequencer with options
* `--sequencer.validatorPrivateKeys <value>` (default: `[Redacted]`)
  List of private keys of the validators participating in attestation duties
  *Environment: `$VALIDATOR_PRIVATE_KEYS`*
* `--sequencer.validatorAddresses <value>`
  List of addresses of the validators to use with remote signers
  *Environment: `$VALIDATOR_ADDRESSES`*
* `--sequencer.disableValidator <value>`
  Do not run the validator
  *Environment: `$VALIDATOR_DISABLED`*
* `--sequencer.disabledValidators <value>`
  Temporarily disable these specific validator addresses
* `--sequencer.attestationPollingIntervalMs <value>` (default: `200`)
  Interval between polling for new attestations
  *Environment: `$VALIDATOR_ATTESTATIONS_POLLING_INTERVAL_MS`*
* `--sequencer.validatorReexecute <value>` (default: `true`)
  Re-execute transactions before attesting
  *Environment: `$VALIDATOR_REEXECUTE`*
* `--sequencer.validatorReexecuteDeadlineMs <value>` (default: `6000`)
  Will re-execute until this many milliseconds are left in the slot
  *Environment: `$VALIDATOR_REEXECUTE_DEADLINE_MS`*
* `--sequencer.alwaysReexecuteBlockProposals <value>`
  Whether to always reexecute block proposals, even for non-validator nodes (useful for monitoring network status).
  *Environment: `$ALWAYS_REEXECUTE_BLOCK_PROPOSALS`*
* `--sequencer.transactionPollingIntervalMS <value>` (default: `500`)
  The number of ms to wait between polling for pending txs.
  *Environment: `$SEQ_TX_POLLING_INTERVAL_MS`*
* `--sequencer.maxTxsPerBlock <value>` (default: `32`)
  The maximum number of txs to include in a block.
  *Environment: `$SEQ_MAX_TX_PER_BLOCK`*
* `--sequencer.minTxsPerBlock <value>` (default: `1`)
  The minimum number of txs to include in a block.
  *Environment: `$SEQ_MIN_TX_PER_BLOCK`*
* `--sequencer.publishTxsWithProposals <value>`
  Whether to publish txs with proposals.
  *Environment: `$SEQ_PUBLISH_TXS_WITH_PROPOSALS`*
* `--sequencer.maxL2BlockGas <value>` (default: `10000000000`)
  The maximum L2 block gas.
  *Environment: `$SEQ_MAX_L2_BLOCK_GAS`*
* `--sequencer.maxDABlockGas <value>` (default: `10000000000`)
  The maximum DA block gas.
  *Environment: `$SEQ_MAX_DA_BLOCK_GAS`*
* `--sequencer.coinbase <value>`
  Recipient of block reward.
  *Environment: `$COINBASE`*
* `--sequencer.feeRecipient <value>`
  Address to receive fees.
  *Environment: `$FEE_RECIPIENT`*
* `--sequencer.acvmWorkingDirectory <value>`
  The working directory to use for simulation/proving
  *Environment: `$ACVM_WORKING_DIRECTORY`*
* `--sequencer.acvmBinaryPath <value>`
  The path to the ACVM binary
  *Environment: `$ACVM_BINARY_PATH`*
* `--sequencer.maxBlockSizeInBytes <value>` (default: `1048576`)
  Max block size
  *Environment: `$SEQ_MAX_BLOCK_SIZE_IN_BYTES`*
* `--sequencer.enforceTimeTable <value>` (default: `true`)
  Whether to enforce the time table when building blocks
  *Environment: `$SEQ_ENFORCE_TIME_TABLE`*
* `--sequencer.governanceProposerPayload <value>` (default: `0x0000000000000000000000000000000000000000`)
  The address of the payload for the governanceProposer
  *Environment: `$GOVERNANCE_PROPOSER_PAYLOAD_ADDRESS`*
* `--sequencer.maxL1TxInclusionTimeIntoSlot <value>`
  How many seconds into an L1 slot we can still send a tx and get it mined.
  *Environment: `$SEQ_MAX_L1_TX_INCLUSION_TIME_INTO_SLOT`*
* `--sequencer.attestationPropagationTime <value>` (default: `2`)
  How many seconds it takes for proposals and attestations to travel across the p2p layer (one-way)
  *Environment: `$SEQ_ATTESTATION_PROPAGATION_TIME`*
* `--sequencer.secondsBeforeInvalidatingBlockAsCommitteeMember <value>` (default: `144`)
  How many seconds to wait before trying to invalidate a block from the pending chain as a committee member (zero to never invalidate). The next proposer is expected to invalidate, so the committee acts as a fallback.
  *Environment: `$SEQ_SECONDS_BEFORE_INVALIDATING_BLOCK_AS_COMMITTEE_MEMBER`*
* `--sequencer.secondsBeforeInvalidatingBlockAsNonCommitteeMember <value>` (default: `432`)
  How many seconds to wait before trying to invalidate a block from the pending chain as a non-committee member (zero to never invalidate). The next proposer is expected to invalidate, then the committee, so other sequencers act as a fallback.
  *Environment: `$SEQ_SECONDS_BEFORE_INVALIDATING_BLOCK_AS_NON_COMMITTEE_MEMBER`*
* `--sequencer.broadcastInvalidBlockProposal <value>`
  Broadcast invalid block proposals with corrupted state (for testing only)
* `--sequencer.injectFakeAttestation <value>`
  Inject a fake attestation (for testing only)
* `--sequencer.shuffleAttestationOrdering <value>`
  Shuffle attestation ordering to create invalid ordering (for testing only)
* `--sequencer.txPublicSetupAllowList <value>`
  The list of functions calls allowed to run in setup
  *Environment: `$TX_PUBLIC_SETUP_ALLOWLIST`*
* `--sequencer.keyStoreDirectory <value>`
  Location of key store directory
  *Environment: `$KEY_STORE_DIRECTORY`*
* `--sequencer.publisherPrivateKeys <value>`
  The private keys to be used by the publisher.
  *Environment: `$SEQ_PUBLISHER_PRIVATE_KEYS`*
* `--sequencer.publisherAddresses <value>`
  The addresses of the publishers to use with remote signers
  *Environment: `$SEQ_PUBLISHER_ADDRESSES`*
* `--sequencer.publisherAllowInvalidStates <value>` (default: `true`)
  True to use publishers in invalid states (timed out, cancelled, etc) if no other is available
  *Environment: `$SEQ_PUBLISHER_ALLOW_INVALID_STATES`*
* `--sequencer.publisherForwarderAddress <value>`
  Address of the forwarder contract to wrap all L1 transactions through (for testing purposes only)
  *Environment: `$SEQ_PUBLISHER_FORWARDER_ADDRESS`*
* `--sequencer.blobSinkUrl <value>`
  The URL of the blob sink
  *Environment: `$BLOB_SINK_URL`*
* `--sequencer.blobAllowEmptySources <value>`
  Whether to allow having no blob sources configured during startup
  *Environment: `$BLOB_ALLOW_EMPTY_SOURCES`*
* `--sequencer.archiveApiUrl <value>`
  The URL of the archive API
  *Environment: `$BLOB_SINK_ARCHIVE_API_URL`*

**BLOB SINK**

* `--blob-sink`
  Starts Aztec Blob Sink with options
* `--blobSink.port <value>`
  The port to run the blob sink server on
  *Environment: `$BLOB_SINK_PORT`*
* `--blobSink.blobSinkMapSizeKb <value>`
  The maximum possible size of the blob sink DB in KB. Overwrites the general dataStoreMapSizeKb.
  *Environment: `$BLOB_SINK_MAP_SIZE_KB`*
* `--blobSink.blobAllowEmptySources <value>`
  Whether to allow having no blob sources configured during startup
  *Environment: `$BLOB_ALLOW_EMPTY_SOURCES`*
* `--blobSink.archiveApiUrl <value>`
  The URL of the archive API
  *Environment: `$BLOB_SINK_ARCHIVE_API_URL`*

**PROVER NODE**

* `--prover-node`
  Starts Aztec Prover Node with options
* `--proverNode.keyStoreDirectory <value>`
  Location of key store directory
  *Environment: `$KEY_STORE_DIRECTORY`*
* `--proverNode.acvmWorkingDirectory <value>`
  The working directory to use for simulation/proving
  *Environment: `$ACVM_WORKING_DIRECTORY`*
* `--proverNode.acvmBinaryPath <value>`
  The path to the ACVM binary
  *Environment: `$ACVM_BINARY_PATH`*
* `--proverNode.bbWorkingDirectory <value>`
  The working directory to use for proving
  *Environment: `$BB_WORKING_DIRECTORY`*
* `--proverNode.bbBinaryPath <value>`
  The path to the bb binary
  *Environment: `$BB_BINARY_PATH`*
* `--proverNode.bbSkipCleanup <value>`
  Whether to skip cleanup of bb temporary files
  *Environment: `$BB_SKIP_CLEANUP`*
* `--proverNode.numConcurrentIVCVerifiers <value>` (default: `8`)
  Max number of chonk verifiers to run concurrently
  *Environment: `$BB_NUM_IVC_VERIFIERS`*
* `--proverNode.bbIVCConcurrency <value>` (default: `1`)
  Number of threads to use for IVC verification
  *Environment: `$BB_IVC_CONCURRENCY`*
* `--proverNode.nodeUrl <value>`
  The URL to the Aztec node to take proving jobs from
  *Environment: `$AZTEC_NODE_URL`*
* `--proverNode.proverId <value>`
  Hex value that identifies the prover. Defaults to the address used for submitting proofs if not set.
  *Environment: `$PROVER_ID`*
* `--proverNode.failedProofStore <value>`
  Store for failed proof inputs. Google cloud storage is only supported at the moment. Set this value as gs://bucket-name/path/to/store.
  *Environment: `$PROVER_FAILED_PROOF_STORE`*
* `--proverNode.publisherAllowInvalidStates <value>` (default: `true`)
  True to use publishers in invalid states (timed out, cancelled, etc) if no other is available
  *Environment: `$PROVER_PUBLISHER_ALLOW_INVALID_STATES`*
* `--proverNode.publisherForwarderAddress <value>`
  Address of the forwarder contract to wrap all L1 transactions through (for testing purposes only)
  *Environment: `$PROVER_PUBLISHER_FORWARDER_ADDRESS`*
* `--proverNode.publisherPrivateKeys <value>`
  The private keys to be used by the publisher.
  *Environment: `$PROVER_PUBLISHER_PRIVATE_KEYS`*
* `--proverNode.publisherAddresses <value>`
  The addresses of the publishers to use with remote signers
  *Environment: `$PROVER_PUBLISHER_ADDRESSES`*
* `--proverNode.proverNodeMaxPendingJobs <value>` (default: `10`)
  The maximum number of pending jobs for the prover node
  *Environment: `$PROVER_NODE_MAX_PENDING_JOBS`*
* `--proverNode.proverNodePollingIntervalMs <value>` (default: `1000`)
  The interval in milliseconds to poll for new jobs
  *Environment: `$PROVER_NODE_POLLING_INTERVAL_MS`*
* `--proverNode.proverNodeMaxParallelBlocksPerEpoch <value>` (default: `32`)
  The Maximum number of blocks to process in parallel while proving an epoch
  *Environment: `$PROVER_NODE_MAX_PARALLEL_BLOCKS_PER_EPOCH`*
* `--proverNode.proverNodeFailedEpochStore <value>`
  File store where to upload node state when an epoch fails to be proven
  *Environment: `$PROVER_NODE_FAILED_EPOCH_STORE`*
* `--proverNode.proverNodeEpochProvingDelayMs <value>`
  Optional delay in milliseconds to wait before proving a new epoch
* `--proverNode.txGatheringIntervalMs <value>` (default: `1000`)
  How often to check that tx data is available
  *Environment: `$PROVER_NODE_TX_GATHERING_INTERVAL_MS`*
* `--proverNode.txGatheringBatchSize <value>` (default: `10`)
  How many transactions to gather from a node in a single request
  *Environment: `$PROVER_NODE_TX_GATHERING_BATCH_SIZE`*
* `--proverNode.txGatheringMaxParallelRequestsPerNode <value>` (default: `100`)
  How many tx requests to make in parallel to each node
  *Environment: `$PROVER_NODE_TX_GATHERING_MAX_PARALLEL_REQUESTS_PER_NODE`*
* `--proverNode.txGatheringTimeoutMs <value>` (default: `120000`)
  How long to wait for tx data to be available before giving up
  *Environment: `$PROVER_NODE_TX_GATHERING_TIMEOUT_MS`*
* `--proverNode.proverNodeDisableProofPublish <value>`
  Whether the prover node skips publishing proofs to L1
  *Environment: `$PROVER_NODE_DISABLE_PROOF_PUBLISH`*

**PROVER BROKER**

* `--prover-broker`
  Starts Aztec proving job broker
* `--proverBroker.proverBrokerJobTimeoutMs <value>` (default: `30000`)
  Jobs are retried if not kept alive for this long
  *Environment: `$PROVER_BROKER_JOB_TIMEOUT_MS`*
* `--proverBroker.proverBrokerPollIntervalMs <value>` (default: `1000`)
  The interval to check job health status
  *Environment: `$PROVER_BROKER_POLL_INTERVAL_MS`*
* `--proverBroker.proverBrokerJobMaxRetries <value>` (default: `3`)
  If starting a prover broker locally, the max number of retries per proving job
  *Environment: `$PROVER_BROKER_JOB_MAX_RETRIES`*
* `--proverBroker.proverBrokerBatchSize <value>` (default: `100`)
  The prover broker writes jobs to disk in batches
  *Environment: `$PROVER_BROKER_BATCH_SIZE`*
* `--proverBroker.proverBrokerBatchIntervalMs <value>` (default: `50`)
  How often to flush batches to disk
  *Environment: `$PROVER_BROKER_BATCH_INTERVAL_MS`*
* `--proverBroker.proverBrokerMaxEpochsToKeepResultsFor <value>` (default: `1`)
  The maximum number of epochs to keep results for
  *Environment: `$PROVER_BROKER_MAX_EPOCHS_TO_KEEP_RESULTS_FOR`*
* `--proverBroker.proverBrokerStoreMapSizeKb <value>`
  The size of the prover broker's database. Will override the dataStoreMapSizeKb if set.
  *Environment: `$PROVER_BROKER_STORE_MAP_SIZE_KB`*

**PROVER AGENT**

* `--prover-agent`
  Starts Aztec Prover Agent with options
* `--proverAgent.proverAgentCount <value>` (default: `1`)
  Whether this prover has a local prover agent
  *Environment: `$PROVER_AGENT_COUNT`*
* `--proverAgent.proverAgentPollIntervalMs <value>` (default: `1000`)
  The interval agents poll for jobs at
  *Environment: `$PROVER_AGENT_POLL_INTERVAL_MS`*
* `--proverAgent.proverAgentProofTypes <value>`
  The types of proofs the prover agent can generate
  *Environment: `$PROVER_AGENT_PROOF_TYPES`*
* `--proverAgent.proverBrokerUrl <value>`
  The URL where this agent takes jobs from
  *Environment: `$PROVER_BROKER_HOST`*
* `--proverAgent.realProofs <value>` (default: `true`)
  Whether to construct real proofs
  *Environment: `$PROVER_REAL_PROOFS`*
* `--proverAgent.proverTestDelayType <value>` (default: `fixed`)
  The type of artificial delay to introduce
  *Environment: `$PROVER_TEST_DELAY_TYPE`*
* `--proverAgent.proverTestDelayMs <value>`
  Artificial delay to introduce to all operations to the test prover.
  *Environment: `$PROVER_TEST_DELAY_MS`*
* `--proverAgent.proverTestDelayFactor <value>` (default: `1`)
  If using realistic delays, what percentage of realistic times to apply.
  *Environment: `$PROVER_TEST_DELAY_FACTOR`*
* `--proverAgent.proverTestVerificationDelayMs <value>` (default: `10`)
  The delay (ms) to inject during fake proof verification
  *Environment: `$PROVER_TEST_VERIFICATION_DELAY_MS`*
* `--p2p-enabled [value]`
  Enable P2P subsystem
  *Environment: `$P2P_ENABLED`*
* `--p2p.p2pDiscoveryDisabled <value>`
  A flag dictating whether the P2P discovery system should be disabled.
  *Environment: `$P2P_DISCOVERY_DISABLED`*
* `--p2p.blockCheckIntervalMS <value>` (default: `100`)
  The frequency in which to check for new L2 blocks.
  *Environment: `$P2P_BLOCK_CHECK_INTERVAL_MS`*
* `--p2p.debugDisableColocationPenalty <value>`
  DEBUG: Disable colocation penalty - NEVER set to true in production
  *Environment: `$DEBUG_P2P_DISABLE_COLOCATION_PENALTY`*
* `--p2p.peerCheckIntervalMS <value>` (default: `30000`)
  The frequency in which to check for new peers.
  *Environment: `$P2P_PEER_CHECK_INTERVAL_MS`*
* `--p2p.l2QueueSize <value>` (default: `1000`)
  Size of queue of L2 blocks to store.
  *Environment: `$P2P_L2_QUEUE_SIZE`*
* `--p2p.listenAddress <value>` (default: `0.0.0.0`)
  The listen address. ipv4 address.
  *Environment: `$P2P_LISTEN_ADDR`*
* `--p2p.p2pPort <value>` (default: `40400`)
  The port for the P2P service. Defaults to 40400
  *Environment: `$P2P_PORT`*
* `--p2p.p2pBroadcastPort <value>`
  The port to broadcast the P2P service on (included in the node's ENR). Defaults to P2P*PORT.
  \_Environment: `$P2P_BROADCAST_PORT`*
* `--p2p.p2pIp <value>`
  The IP address for the P2P service. ipv4 address.
  *Environment: `$P2P_IP`*
* `--p2p.peerIdPrivateKey <value>`
  An optional peer id private key. If blank, will generate a random key.
  *Environment: `$PEER_ID_PRIVATE_KEY`*
* `--p2p.peerIdPrivateKeyPath <value>`
  An optional path to store generated peer id private keys. If blank, will default to storing any generated keys in the root of the data directory.
  *Environment: `$PEER_ID_PRIVATE_KEY_PATH`*
* `--p2p.bootstrapNodes <value>`
  A list of bootstrap peer ENRs to connect to. Separated by commas.
  *Environment: `$BOOTSTRAP_NODES`*
* `--p2p.bootstrapNodeEnrVersionCheck <value>`
  Whether to check the version of the bootstrap node ENR.
  *Environment: `$P2P_BOOTSTRAP_NODE_ENR_VERSION_CHECK`*
* `--p2p.bootstrapNodesAsFullPeers <value>`
  Whether to consider our configured bootnodes as full peers
  *Environment: `$P2P_BOOTSTRAP_NODES_AS_FULL_PEERS`*
* `--p2p.maxPeerCount <value>` (default: `100`)
  The maximum number of peers to connect to.
  *Environment: `$P2P_MAX_PEERS`*
* `--p2p.queryForIp <value>`
  If announceUdpAddress or announceTcpAddress are not provided, query for the IP address of the machine. Default is false.
  *Environment: `$P2P_QUERY_FOR_IP`*
* `--p2p.gossipsubInterval <value>` (default: `700`)
  The interval of the gossipsub heartbeat to perform maintenance tasks.
  *Environment: `$P2P_GOSSIPSUB_INTERVAL_MS`*
* `--p2p.gossipsubD <value>` (default: `8`)
  The D parameter for the gossipsub protocol.
  *Environment: `$P2P_GOSSIPSUB_D`*
* `--p2p.gossipsubDlo <value>` (default: `4`)
  The Dlo parameter for the gossipsub protocol.
  *Environment: `$P2P_GOSSIPSUB_DLO`*
* `--p2p.gossipsubDhi <value>` (default: `12`)
  The Dhi parameter for the gossipsub protocol.
  *Environment: `$P2P_GOSSIPSUB_DHI`*
* `--p2p.gossipsubDLazy <value>` (default: `8`)
  The Dlazy parameter for the gossipsub protocol.
  *Environment: `$P2P_GOSSIPSUB_DLAZY`*
* `--p2p.gossipsubFloodPublish <value>`
  Whether to flood publish messages. - For testing purposes only
  *Environment: `$P2P_GOSSIPSUB_FLOOD_PUBLISH`*
* `--p2p.gossipsubMcacheLength <value>` (default: `6`)
  The number of gossipsub interval message cache windows to keep.
  *Environment: `$P2P_GOSSIPSUB_MCACHE_LENGTH`*
* `--p2p.gossipsubMcacheGossip <value>` (default: `3`)
  How many message cache windows to include when gossiping with other peers.
  *Environment: `$P2P_GOSSIPSUB_MCACHE_GOSSIP`*
* `--p2p.gossipsubSeenTTL <value>` (default: `1200000`)
  How long to keep message IDs in the seen cache.
  *Environment: `$P2P_GOSSIPSUB_SEEN_TTL`*
* `--p2p.gossipsubTxTopicWeight <value>` (default: `1`)
  The weight of the tx topic for the gossipsub protocol.
  *Environment: `$P2P_GOSSIPSUB_TX_TOPIC_WEIGHT`*
* `--p2p.gossipsubTxInvalidMessageDeliveriesWeight <value>` (default: `-20`)
  The weight of the tx invalid message deliveries for the gossipsub protocol.
  *Environment: `$P2P_GOSSIPSUB_TX_INVALID_MESSAGE_DELIVERIES_WEIGHT`*
* `--p2p.gossipsubTxInvalidMessageDeliveriesDecay <value>` (default: `0.5`)
  Determines how quickly the penalty for invalid message deliveries decays over time. Between 0 and 1.
  *Environment: `$P2P_GOSSIPSUB_TX_INVALID_MESSAGE_DELIVERIES_DECAY`*
* `--p2p.peerPenaltyValues <value>` (default: `2,10,50`)
  The values for the peer scoring system. Passed as a comma separated list of values in order: low, mid, high tolerance errors.
  *Environment: `$P2P_PEER_PENALTY_VALUES`*
* `--p2p.doubleSpendSeverePeerPenaltyWindow <value>` (default: `30`)
  The "age" (in L2 blocks) of a tx after which we heavily penalize a peer for sending it.
  *Environment: `$P2P_DOUBLE_SPEND_SEVERE_PEER_PENALTY_WINDOW`*
* `--p2p.blockRequestBatchSize <value>` (default: `20`)
  The number of blocks to fetch in a single batch.
  *Environment: `$P2P_BLOCK_REQUEST_BATCH_SIZE`*
* `--p2p.archivedTxLimit <value>`
  The number of transactions that will be archived. If the limit is set to 0 then archiving will be disabled.
  *Environment: `$P2P_ARCHIVED_TX_LIMIT`*
* `--p2p.trustedPeers <value>`
  A list of trusted peer ENRs that will always be persisted. Separated by commas.
  *Environment: `$P2P_TRUSTED_PEERS`*
* `--p2p.privatePeers <value>`
  A list of private peer ENRs that will always be persisted and not be used for discovery. Separated by commas.
  *Environment: `$P2P_PRIVATE_PEERS`*
* `--p2p.preferredPeers <value>`
  A list of preferred peer ENRs that will always be persisted and not be used for discovery. Separated by commas.
  *Environment: `$P2P_PREFERRED_PEERS`*
* `--p2p.p2pStoreMapSizeKb <value>`
  The maximum possible size of the P2P DB in KB. Overwrites the general dataStoreMapSizeKb.
  *Environment: `$P2P_STORE_MAP_SIZE_KB`*
* `--p2p.txPublicSetupAllowList <value>`
  The list of functions calls allowed to run in setup
  *Environment: `$TX_PUBLIC_SETUP_ALLOWLIST`*
* `--p2p.maxTxPoolSize <value>` (default: `100000000`)
  The maximum cumulative tx size of pending txs (in bytes) before evicting lower priority txs.
  *Environment: `$P2P_MAX_TX_POOL_SIZE`*
* `--p2p.txPoolOverflowFactor <value>` (default: `1.1`)
  How much the tx pool can overflow before it starts evicting txs. Must be greater than 1
  *Environment: `$P2P_TX_POOL_OVERFLOW_FACTOR`*
* `--p2p.seenMessageCacheSize <value>` (default: `100000`)
  The number of messages to keep in the seen message cache
  *Environment: `$P2P_SEEN_MSG_CACHE_SIZE`*
* `--p2p.p2pDisableStatusHandshake <value>`
  True to disable the status handshake on peer connected.
  *Environment: `$P2P_DISABLE_STATUS_HANDSHAKE`*
* `--p2p.p2pAllowOnlyValidators <value>`
  True to only permit validators to connect.
  *Environment: `$P2P_ALLOW_ONLY_VALIDATORS`*
* `--p2p.p2pMaxFailedAuthAttemptsAllowed <value>` (default: `3`)
  Number of auth attempts to allow before peer is banned. Number is inclusive
  *Environment: `$P2P_MAX_AUTH_FAILED_ATTEMPTS_ALLOWED`*
* `--p2p.dropTransactions <value>`
  True to simulate discarding transactions. - For testing purposes only
  *Environment: `$P2P_DROP_TX`*
* `--p2p.dropTransactionsProbability <value>`
  The probability that a transaction is discarded (0 - 1). - For testing purposes only
  *Environment: `$P2P_DROP_TX_CHANCE`*
* `--p2p.disableTransactions <value>`
  Whether transactions are disabled for this node. This means transactions will be rejected at the RPC and P2P layers.
  *Environment: `$TRANSACTIONS_DISABLED`*
* `--p2p.txPoolDeleteTxsAfterReorg <value>`
  Whether to delete transactions from the pool after a reorg instead of moving them back to pending.
  *Environment: `$P2P_TX_POOL_DELETE_TXS_AFTER_REORG`*
* `--p2p.debugP2PInstrumentMessages <value>`
  Alters the format of p2p messages to include things like broadcast timestamp FOR TESTING ONLY
  *Environment: `$DEBUG_P2P_INSTRUMENT_MESSAGES`*
* `--p2p.overallRequestTimeoutMs <value>` (default: `10000`)
  The overall timeout for a request response operation.
  *Environment: `$P2P_REQRESP_OVERALL_REQUEST_TIMEOUT_MS`*
* `--p2p.individualRequestTimeoutMs <value>` (default: `10000`)
  The timeout for an individual request response peer interaction.
  *Environment: `$P2P_REQRESP_INDIVIDUAL_REQUEST_TIMEOUT_MS`*
* `--p2p.dialTimeoutMs <value>` (default: `5000`)
  How long to wait for the dial protocol to establish a connection
  *Environment: `$P2P_REQRESP_DIAL_TIMEOUT_MS`*
* `--p2p.p2pOptimisticNegotiation <value>`
  Whether to use optimistic protocol negotiation when dialing to another peer (opposite of `negotiateFully`).
  *Environment: `$P2P_REQRESP_OPTIMISTIC_NEGOTIATION`*
* `--p2p.txCollectionFastNodesTimeoutBeforeReqRespMs <value>` (default: `200`)
  How long to wait before starting reqresp for fast collection
  *Environment: `$TX_COLLECTION_FAST_NODES_TIMEOUT_BEFORE_REQ_RESP_MS`*
* `--p2p.txCollectionSlowNodesIntervalMs <value>` (default: `12000`)
  How often to collect from configured nodes in the slow collection loop
  *Environment: `$TX_COLLECTION_SLOW_NODES_INTERVAL_MS`*
* `--p2p.txCollectionSlowReqRespIntervalMs <value>` (default: `12000`)
  How often to collect from peers via reqresp in the slow collection loop
  *Environment: `$TX_COLLECTION_SLOW_REQ_RESP_INTERVAL_MS`*
* `--p2p.txCollectionSlowReqRespTimeoutMs <value>` (default: `20000`)
  How long to wait for a reqresp response during slow collection
  *Environment: `$TX_COLLECTION_SLOW_REQ_RESP_TIMEOUT_MS`*
* `--p2p.txCollectionReconcileIntervalMs <value>` (default: `60000`)
  How often to reconcile found txs from the tx pool
  *Environment: `$TX_COLLECTION_RECONCILE_INTERVAL_MS`*
* `--p2p.txCollectionDisableSlowDuringFastRequests <value>` (default: `true`)
  Whether to disable the slow collection loop if we are dealing with any immediate requests
  *Environment: `$TX_COLLECTION_DISABLE_SLOW_DURING_FAST_REQUESTS`*
* `--p2p.txCollectionFastNodeIntervalMs <value>` (default: `500`)
  How many ms to wait between retried request to a node via RPC during fast collection
  *Environment: `$TX_COLLECTION_FAST_NODE_INTERVAL_MS`*
* `--p2p.txCollectionNodeRpcUrls <value>`
  A comma-separated list of Aztec node RPC URLs to use for tx collection
  *Environment: `$TX_COLLECTION_NODE_RPC_URLS`*
* `--p2p.txCollectionFastMaxParallelRequestsPerNode <value>` (default: `4`)
  Maximum number of parallel requests to make to a node during fast collection
  *Environment: `$TX_COLLECTION_FAST_MAX_PARALLEL_REQUESTS_PER_NODE`*
* `--p2p.txCollectionNodeRpcMaxBatchSize <value>` (default: `50`)
  Maximum number of transactions to request from a node in a single batch
  *Environment: `$TX_COLLECTION_NODE_RPC_MAX_BATCH_SIZE`*
* `--p2p-bootstrap`
  Starts Aztec P2P Bootstrap with options
* `--p2pBootstrap.p2pBroadcastPort <value>`
  The port to broadcast the P2P service on (included in the node's ENR). Defaults to P2P*PORT.
  \_Environment: `$P2P_BROADCAST_PORT`*
* `--p2pBootstrap.peerIdPrivateKeyPath <value>`
  An optional path to store generated peer id private keys. If blank, will default to storing any generated keys in the root of the data directory.
  *Environment: `$PEER_ID_PRIVATE_KEY_PATH`*
* `--p2pBootstrap.queryForIp <value>`
  If announceUdpAddress or announceTcpAddress are not provided, query for the IP address of the machine. Default is false.
  *Environment: `$P2P_QUERY_FOR_IP`*

**TELEMETRY**

* `--tel.metricsCollectorUrl <value>`
  The URL of the telemetry collector for metrics
  *Environment: `$OTEL_EXPORTER_OTLP_METRICS_ENDPOINT`*
* `--tel.tracesCollectorUrl <value>`
  The URL of the telemetry collector for traces
  *Environment: `$OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`*
* `--tel.logsCollectorUrl <value>`
  The URL of the telemetry collector for logs
  *Environment: `$OTEL_EXPORTER_OTLP_LOGS_ENDPOINT`*
* `--tel.otelCollectIntervalMs <value>` (default: `60000`)
  The interval at which to collect metrics
  *Environment: `$OTEL_COLLECT_INTERVAL_MS`*
* `--tel.otelExportTimeoutMs <value>` (default: `30000`)
  The timeout for exporting metrics
  *Environment: `$OTEL_EXPORT_TIMEOUT_MS`*
* `--tel.otelExcludeMetrics <value>`
  A list of metric prefixes to exclude from export
  *Environment: `$OTEL_EXCLUDE_METRICS`*
* `--tel.otelIncludeMetrics <value>`
  A list of metric prefixes to include in export (ignored if OTEL*EXCLUDE\_METRICS is set)
  \_Environment: `$OTEL_INCLUDE_METRICS`*
* `--tel.publicMetricsCollectorUrl <value>`
  A URL to publish a subset of metrics for public consumption
  *Environment: `$PUBLIC_OTEL_EXPORTER_OTLP_METRICS_ENDPOINT`*
* `--tel.publicMetricsCollectFrom <value>`
  The role types to collect metrics from
  *Environment: `$PUBLIC_OTEL_COLLECT_FROM`*
* `--tel.publicIncludeMetrics <value>`
  A list of metric prefixes to publicly export
  *Environment: `$PUBLIC_OTEL_INCLUDE_METRICS`*
* `--tel.publicMetricsOptOut <value>` (default: `true`)
  Whether to opt out of sharing optional telemetry
  *Environment: `$PUBLIC_OTEL_OPT_OUT`*

**BOT**

* `--bot`
  Starts Aztec Bot with options
* `--bot.nodeUrl <value>`
  The URL to the Aztec node to check for tx pool status.
  *Environment: `$AZTEC_NODE_URL`*
* `--bot.nodeAdminUrl <value>`
  The URL to the Aztec node admin API to force-flush txs if configured.
  *Environment: `$AZTEC_NODE_ADMIN_URL`*
* `--bot.l1Mnemonic <value>`
  The mnemonic for the account to bridge fee juice from L1.
  *Environment: `$BOT_L1_MNEMONIC`*
* `--bot.l1PrivateKey <value>`
  The private key for the account to bridge fee juice from L1.
  *Environment: `$BOT_L1_PRIVATE_KEY`*
* `--bot.l1ToL2MessageTimeoutSeconds <value>` (default: `3600`)
  How long to wait for L1 to L2 messages to become available on L2
  *Environment: `$BOT_L1_TO_L2_TIMEOUT_SECONDS`*
* `--bot.senderPrivateKey <value>`
  Signing private key for the sender account.
  *Environment: `$BOT_PRIVATE_KEY`*
* `--bot.senderSalt <value>`
  The salt to use to deploy the sender account.
  *Environment: `$BOT_ACCOUNT_SALT`*
* `--bot.tokenSalt <value>` (default: `0x0000000000000000000000000000000000000000000000000000000000000001`)
  The salt to use to deploy the token contract.
  *Environment: `$BOT_TOKEN_SALT`*
* `--bot.txIntervalSeconds <value>` (default: `60`)
  Every how many seconds should a new tx be sent.
  *Environment: `$BOT_TX_INTERVAL_SECONDS`*
* `--bot.privateTransfersPerTx <value>` (default: `1`)
  How many private token transfers are executed per tx.
  *Environment: `$BOT_PRIVATE_TRANSFERS_PER_TX`*
* `--bot.publicTransfersPerTx <value>` (default: `1`)
  How many public token transfers are executed per tx.
  *Environment: `$BOT_PUBLIC_TRANSFERS_PER_TX`*
* `--bot.feePaymentMethod <value>` (default: `fee_juice`)
  How to handle fee payments. (Options: fee*juice)
  \_Environment: `$BOT_FEE_PAYMENT_METHOD`*
* `--bot.baseFeePadding <value>` (default: `3`)
  How much is the bot willing to overpay vs. the current base fee
  *Environment: `$BOT_BASE_FEE_PADDING`*
* `--bot.noStart <value>`
  True to not automatically setup or start the bot on initialization.
  *Environment: `$BOT_NO_START`*
* `--bot.txMinedWaitSeconds <value>` (default: `180`)
  How long to wait for a tx to be mined before reporting an error.
  *Environment: `$BOT_TX_MINED_WAIT_SECONDS`*
* `--bot.followChain <value>` (default: `NONE`)
  Which chain the bot follows
  *Environment: `$BOT_FOLLOW_CHAIN`*
* `--bot.maxPendingTxs <value>` (default: `128`)
  Do not send a tx if the node's tx pool already has this many pending txs.
  *Environment: `$BOT_MAX_PENDING_TXS`*
* `--bot.flushSetupTransactions <value>`
  Make a request for the sequencer to build a block after each setup transaction.
  *Environment: `$BOT_FLUSH_SETUP_TRANSACTIONS`*
* `--bot.l2GasLimit <value>`
  L2 gas limit for the tx (empty to have the bot trigger an estimate gas).
  *Environment: `$BOT_L2_GAS_LIMIT`*
* `--bot.daGasLimit <value>`
  DA gas limit for the tx (empty to have the bot trigger an estimate gas).
  *Environment: `$BOT_DA_GAS_LIMIT`*
* `--bot.contract <value>` (default: `TokenContract`)
  Token contract to use
  *Environment: `$BOT_TOKEN_CONTRACT`*
* `--bot.maxConsecutiveErrors <value>`
  The maximum number of consecutive errors before the bot shuts down
  *Environment: `$BOT_MAX_CONSECUTIVE_ERRORS`*
* `--bot.stopWhenUnhealthy <value>`
  Stops the bot if service becomes unhealthy
  *Environment: `$BOT_STOP_WHEN_UNHEALTHY`*
* `--bot.ammTxs <value>`
  Deploy an AMM and send swaps to it
  *Environment: `$BOT_AMM_TXS`*

**PXE**

* `--pxe`
  Starts Aztec PXE with options
* `--pxe.l2BlockBatchSize <value>` (default: `50`)
  Maximum amount of blocks to pull from the stream in one request when synchronizing
  *Environment: `$PXE_L2_BLOCK_BATCH_SIZE`*
* `--pxe.proverEnabled <value>` (default: `true`)
  Enable real proofs
  *Environment: `$PXE_PROVER_ENABLED`*
* `--pxe.nodeUrl <value>`
  Custom Aztec Node URL to connect to
  *Environment: `$AZTEC_NODE_URL`*

**TXE**

* `--txe`
  Starts Aztec TXE with options

## aztec trigger-seed-snapshot```
Usage: aztec trigger-seed-snapshot [options]  
  
Triggers a seed snapshot for the next epoch.  
  
Options:  
  -pk, --private-key <string>  The private key to use for deployment  
  -m, --mnemonic <string>      The mnemonic to use in deployment (default:  
                               "test test test test test test test test test  
                               test test junk")  
  --rollup <address>           ethereum address of the rollup contract  
  --l1-rpc-urls <string>       List of Ethereum host URLs. Chain identifiers  
                               localhost and testnet can be used (comma  
                               separated) (default:  
                               ["http://host.docker.internal:8545"], env:  
                               ETHEREUM_HOSTS)  
  -c, --l1-chain-id <number>   Chain ID of the ethereum host (default: 31337,  
                               env: L1_CHAIN_ID)  
  -h, --help                   display help for command
```

## aztec update```
Usage: aztec update [options] [projectPath]  
  
Updates Nodejs and Noir dependencies  
  
Arguments:  
  projectPath               Path to the project directory (default:  
                            "/home/user/Documents/Github/aztec-packages/docs")  
  
Options:  
  --contract [paths...]     Paths to contracts to update dependencies (default:  
                            [])  
  --aztec-version <semver>  The version to update Aztec packages to. Defaults  
                            to latest (default: "latest")  
  -h, --help                display help for command
```

## aztec validator-keys|valKeys*This command help is currently unavailable due to a technical issue.*

## aztec vote-on-governance-proposal```
Usage: aztec vote-on-governance-proposal [options]  
  
Votes on a governance proposal.  
  
Options:  
  -p, --proposal-id <string>       The ID of the proposal  
  -a, --vote-amount <string>       The amount of tokens to vote  
  --in-favor <boolean>             Whether to vote in favor of the proposal.  
                                   Use "yea" for true, any other value for  
                                   false.  
  --wait <boolean>                 Whether to wait until the proposal is active  
  -r, --registry-address <string>  The address of the registry contract  
  --l1-rpc-urls <string>           List of Ethereum host URLs. Chain  
                                   identifiers localhost and testnet can be  
                                   used (comma separated) (default:  
                                   ["http://host.docker.internal:8545"], env:  
                                   ETHEREUM_HOSTS)  
  -c, --l1-chain-id <number>       Chain ID of the ethereum host (default:  
                                   31337, env: L1_CHAIN_ID)  
  -pk, --private-key <string>      The private key to use to vote  
  -m, --mnemonic <string>          The mnemonic to use to vote (default: "test  
                                   test test test test test test test test test  
                                   test junk")  
  -i, --mnemonic-index <number>    The index of the mnemonic to use to vote  
                                   (default: 0)  
  -h, --help                       display help for command
```

## aztec init*No help information available for this command.*

## aztec new*No help information available for this command.*

## aztec compile```
Aztec Compile - Compile Aztec Noir contracts  
  
This command compiles Aztec Noir contracts using nargo and then automatically  
postprocesses them to generate Aztec-specific artifacts including:  
  - Transpiled contract artifacts  
  - Verification keys  
  
The compiled contracts will be placed in the target/ directory by default.  
  
AZTEC-SPECIFIC NOTES:  
  - Working directory must be under $HOME due to Docker containerization  
  - Compilation automatically includes contract postprocessing  
  - Use standard nargo compile options (see below)  
  
ENVIRONMENT VARIABLES:  
  AZTEC_PATH    Path to Aztec installation (default: $HOME/.aztec)  
  VERSION       Aztec version to use (default: from $AZTEC_PATH/default_version)  
  DOCKER_REPO   Docker repository (default: aztecprotocol/aztec)  
  
---  
Underlying nargo compile options:  
  
Compile the program and its secret execution trace into ACIR format  
  
Usage: nargo compile [OPTIONS]  
  
Options:  
      --package <PACKAGE>  
          The name of the package to run the command on. By default run on the first one found moving up along the ancestors of the current directory  
  
      --workspace  
          Run on all packages in the workspace  
  
      --expression-width <EXPRESSION_WIDTH>  
          Specify the backend expression width that should be targeted  
  
      --bounded-codegen  
          Generate ACIR with the target backend expression width. The default is to generate ACIR without a bound and split expressions after code generation. Activating this flag can sometimes provide optimizations for certain programs  
  
      --force  
          Force a full recompilation  
  
      --print-acir  
          Display the ACIR for compiled circuit  
  
      --deny-warnings  
          Treat all warnings as errors  
  
      --silence-warnings  
          Suppress warnings  
  
      --debug-comptime-in-file <DEBUG_COMPTIME_IN_FILE>  
          Enable printing results of comptime evaluation: provide a path suffix for the module to debug, e.g. "package_name/src/main.nr"  
  
      --skip-underconstrained-check  
          Flag to turn off the compiler check for under constrained values. Warning: This can improve compilation speed but can also lead to correctness errors. This check should always be run on production code  
  
      --skip-brillig-constraints-check  
          Flag to turn off the compiler check for missing Brillig call constraints. Warning: This can improve compilation speed but can also lead to correctness errors. This check should always be run on production code  
  
      --count-array-copies  
          Count the number of arrays that are copied in an unconstrained context for performance debugging  
  
      --enable-brillig-constraints-check-lookback  
          Flag to turn on the lookback feature of the Brillig call constraints check, allowing tracking argument values before the call happens preventing certain rare false positives (leads to a slowdown on large rollout functions)  
  
      --inliner-aggressiveness <INLINER_AGGRESSIVENESS>  
          Setting to decide on an inlining strategy for Brillig functions. A more aggressive inliner should generate larger programs but more optimized A less aggressive inliner should generate smaller programs  
  
          [default: 9223372036854775807]  
  
      --pedantic-solving  
          Use pedantic ACVM solving, i.e. double-check some black-box function assumptions when solving. This is disabled by default  
  
  -Z, --unstable-features <UNSTABLE_FEATURES>  
          Unstable features to enable for this current build.  
  
          If non-empty, it disables unstable features required in crate manifests.  
  
      --no-unstable-features  
          Disable any unstable features required in crate manifests  
  
  -h, --help  
          Print help (see a summary with '-h')
```

## aztec fmt```
Format the Noir files in a workspace  
  
Usage: nargo fmt [OPTIONS]  
  
Options:  
      --check              Run noirfmt in check mode  
      --package <PACKAGE>  The name of the package to run the command on. By default run on the first one found moving up along the ancestors of the current directory  
      --workspace          Run on all packages in the workspace  
  -h, --help               Print help
```

## aztec check```
Check a local package and all of its dependencies for errors  
  
Usage: nargo check [OPTIONS]  
  
Options:  
      --package <PACKAGE>  
          The name of the package to run the command on. By default run on the first one found moving up along the ancestors of the current directory  
  
      --workspace  
          Run on all packages in the workspace  
  
      --overwrite  
          Force overwrite of existing files  
  
      --expression-width <EXPRESSION_WIDTH>  
          Specify the backend expression width that should be targeted  
  
      --bounded-codegen  
          Generate ACIR with the target backend expression width. The default is to generate ACIR without a bound and split expressions after code generation. Activating this flag can sometimes provide optimizations for certain programs  
  
      --force  
          Force a full recompilation  
  
      --print-acir  
          Display the ACIR for compiled circuit  
  
      --deny-warnings  
          Treat all warnings as errors  
  
      --silence-warnings  
          Suppress warnings  
  
      --debug-comptime-in-file <DEBUG_COMPTIME_IN_FILE>  
          Enable printing results of comptime evaluation: provide a path suffix for the module to debug, e.g. "package_name/src/main.nr"  
  
      --skip-underconstrained-check  
          Flag to turn off the compiler check for under constrained values. Warning: This can improve compilation speed but can also lead to correctness errors. This check should always be run on production code  
  
      --skip-brillig-constraints-check  
          Flag to turn off the compiler check for missing Brillig call constraints. Warning: This can improve compilation speed but can also lead to correctness errors. This check should always be run on production code  
  
      --count-array-copies  
          Count the number of arrays that are copied in an unconstrained context for performance debugging  
  
      --enable-brillig-constraints-check-lookback  
          Flag to turn on the lookback feature of the Brillig call constraints check, allowing tracking argument values before the call happens preventing certain rare false positives (leads to a slowdown on large rollout functions)  
  
      --inliner-aggressiveness <INLINER_AGGRESSIVENESS>  
          Setting to decide on an inlining strategy for Brillig functions. A more aggressive inliner should generate larger programs but more optimized A less aggressive inliner should generate smaller programs  
  
          [default: 9223372036854775807]  
  
      --pedantic-solving  
          Use pedantic ACVM solving, i.e. double-check some black-box function assumptions when solving. This is disabled by default  
  
  -Z, --unstable-features <UNSTABLE_FEATURES>  
          Unstable features to enable for this current build.  
  
          If non-empty, it disables unstable features required in crate manifests.  
  
      --no-unstable-features  
          Disable any unstable features required in crate manifests  
  
  -h, --help  
          Print help (see a summary with '-h')
```

## aztec test```
Run the tests for this program  
  
Usage: nargo test [OPTIONS] [TEST_NAMES]...  
  
Arguments:  
  [TEST_NAMES]...  
          If given, only tests with names containing this string will be run  
  
Options:  
      --show-output  
          Display output of `println` statements  
  
      --exact  
          Only run tests that match exactly  
  
      --package <PACKAGE>  
          The name of the package to run the command on. By default run on the first one found moving up along the ancestors of the current directory  
  
      --workspace  
          Run on all packages in the workspace  
  
      --expression-width <EXPRESSION_WIDTH>  
          Specify the backend expression width that should be targeted  
  
      --bounded-codegen  
          Generate ACIR with the target backend expression width. The default is to generate ACIR without a bound and split expressions after code generation. Activating this flag can sometimes provide optimizations for certain programs  
  
      --force  
          Force a full recompilation  
  
      --print-acir  
          Display the ACIR for compiled circuit  
  
      --deny-warnings  
          Treat all warnings as errors  
  
      --silence-warnings  
          Suppress warnings  
  
      --debug-comptime-in-file <DEBUG_COMPTIME_IN_FILE>  
          Enable printing results of comptime evaluation: provide a path suffix for the module to debug, e.g. "package_name/src/main.nr"  
  
      --skip-underconstrained-check  
          Flag to turn off the compiler check for under constrained values. Warning: This can improve compilation speed but can also lead to correctness errors. This check should always be run on production code  
  
      --skip-brillig-constraints-check  
          Flag to turn off the compiler check for missing Brillig call constraints. Warning: This can improve compilation speed but can also lead to correctness errors. This check should always be run on production code  
  
      --count-array-copies  
          Count the number of arrays that are copied in an unconstrained context for performance debugging  
  
      --enable-brillig-constraints-check-lookback  
          Flag to turn on the lookback feature of the Brillig call constraints check, allowing tracking argument values before the call happens preventing certain rare false positives (leads to a slowdown on large rollout functions)  
  
      --inliner-aggressiveness <INLINER_AGGRESSIVENESS>  
          Setting to decide on an inlining strategy for Brillig functions. A more aggressive inliner should generate larger programs but more optimized A less aggressive inliner should generate smaller programs  
  
          [default: 9223372036854775807]  
  
      --pedantic-solving  
          Use pedantic ACVM solving, i.e. double-check some black-box function assumptions when solving. This is disabled by default  
  
  -Z, --unstable-features <UNSTABLE_FEATURES>  
          Unstable features to enable for this current build.  
  
          If non-empty, it disables unstable features required in crate manifests.  
  
      --no-unstable-features  
          Disable any unstable features required in crate manifests  
  
      --oracle-resolver <ORACLE_RESOLVER>  
          JSON RPC url to solve oracle calls  
  
      --test-threads <TEST_THREADS>  
          Number of threads used for running tests in parallel  
  
          [default: 14]  
  
      --format <FORMAT>  
          Configure formatting of output  
  
          Possible values:  
          - pretty: Print verbose output  
          - terse:  Display one character per test  
          - json:   Output a JSON Lines document  
  
  -q, --quiet  
          Display one character per test instead of one line  
  
      --no-fuzz  
          Do not run fuzz tests (tests that have arguments)  
  
      --only-fuzz  
          Only run fuzz tests (tests that have arguments)  
  
      --corpus-dir <CORPUS_DIR>  
          If given, load/store fuzzer corpus from this folder  
  
      --minimized-corpus-dir <MINIMIZED_CORPUS_DIR>  
          If given, perform corpus minimization instead of fuzzing and store results in the given folder  
  
      --fuzzing-failure-dir <FUZZING_FAILURE_DIR>  
          If given, store the failing input in the given folder  
  
      --fuzz-timeout <FUZZ_TIMEOUT>  
          Maximum time in seconds to spend fuzzing (default: 1 seconds)  
  
          [default: 1]  
  
      --fuzz-max-executions <FUZZ_MAX_EXECUTIONS>  
          Maximum number of executions to run for each fuzz test (default: 100000)  
  
          [default: 100000]  
  
      --fuzz-show-progress  
          Show progress of fuzzing (default: false)  
  
  -h, --help  
          Print help (see a summary with '-h')
```

## aztec lsp*No help information available for this command.*

---


# Wallet CLI

Source: https://docs.aztec.network/developers/docs/wallet-cli/faceid_wallet

Version: Devnet (v3.0.0-devnet.20251212)

On this page

In this tutorial, we will use Apple Mac's Secure Enclave to store the private key, and use it in Aztec's [CLI Wallet](/developers/docs/wallet-cli/cli_wallet_reference). This enables fully private, native, and seedless account abstraction!

warning

Aztec is in active development and this has only been tested on MacOS. Please reach out if this tutorial does not work for you, and let us know your operating system.

note

This tutorial is for the local network and will need adjustments if you want to use it on testnet. Install the local network [here](/developers/getting_started_on_local_network).

## PrerequisitesFor this tutorial, we will need to have the [Local Network](/developers/getting_started_on_local_network) installed.

We also need to install Secretive, a nice open-source package that allows us to store keys on the Secure Enclave. You can head to the [secretive releases page](https://github.com/maxgoedjen/secretive/releases) and get the last release's `zip`, unzip and move to Applications, or use [Homebrew](https://brew.sh/):

```
brew install secretive
```

Open it from the Applications folder and copy the provided Socket Path (the one it tells you to add to your .ssh config). Export it as a terminal environment variable. For example:

```
export SSH_AUTH_SOCK="/Users/your_user/Library/Containers/com.maxgoedjen.Secretive.SecretAgent/Data/socket.ssh"
```

Let's also install `socat` which helps us manage the socket connections. If using Homebrew:

```
brew install socat
```

## Creating a keyWe will create our private key, which will be stored in the Secure Enclave. Open Secretive, click the "+" sign and create a key with authentication. You can give it any name you like. Secretive will then store it in the Secure Enclave.

Make sure Secretive's "Secret Agent" is running.

info

The Secure Enclave is a protected chip on most recent iPhones and Macs and it's meant to be airgapped. It is not safe to use in production.

Fortunately, Aztec implements [Account Abstraction](/developers/docs/foundational-topics/accounts#what-is-account-abstraction) at the protocol level. You could write logic to allow someone else to recover your account, or use a different key or keys for recovery.

## Creating an accountNow we can use the key to create an account. Every account on Aztec is a contract, so you can write your own contract with its own account logic.

The Aztec team already wrote some account contract boilerplates we can use. One of them is an account that uses the `secp256r1` elliptic curve (the one the Secure Enclave uses).

Let's create an account in our wallet:

```
aztec-wallet create-account -a my-faceid-wallet -t ecdsasecp256r1ssh
```

This command creates an account using the `ecdsasecp256r1ssh` type and aliases it to `my-faceid-wallet`.

You should see a prompt like `? What public key to use?` with the public key you created in Secretive. Select this. If you see the message `Account stored in database with aliases last & my-faceid-wallet` then you have successfully created the account!

You can find other accounts by running `aztec-wallet create-account -h`.

## Using the walletYour FaceID-backed wallet is now ready to use. You can interact with it via the alias `accounts:my-faceid-wallet` just like any other wallet in the CLI.

Verify your account was stored correctly:

```
aztec-wallet get-alias accounts:my-faceid-wallet
```

From here, you can deploy contracts, send transactions, and interact with the network - each transaction will prompt you to authenticate with TouchID or your password.

Check out the [CLI Wallet Reference](/developers/docs/wallet-cli/cli_wallet_reference) for the full set of available commands, or follow the [Getting Started on Local Network](/developers/getting_started_on_local_network) guide to deploy contracts and interact with the network using your new wallet.

## What nextIn this tutorial, we created an account with the Aztec's [CLI Wallet](/developers/docs/wallet-cli/cli_wallet_reference), using the Apple Mac's Secure Enclave to store the private key.

You can use a multitude of authentication methods, for example with RSA you could use a passport as a recovery, or even as a signer in a multisig. All of this is based on the [account contract](https://github.com/AztecProtocol/aztec-packages/tree/v3.0.0-devnet.20251212/noir-projects/noir-contracts/contracts/account).

---


# Resources

Source: https://docs.aztec.network/developers/docs/resources/considerations/privacy_considerations

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Privacy is important.

Keeping information private is difficult.

Once information is leaked, it cannot be unleaked.

---

## What can Aztec keep private?Aztec provides a set of tools to enable developers to build private smart contracts. The following can be kept private:

**Private persistent state**

Store state variables in an encrypted form, so that no one can see what those variables are, except those with the decryption key.

**Private events and messages**

Emit encrypted events, or encrypted messages from a private smart contract function. Only those with the decryption key will learn the message.

**Private function execution**

Execute a private function without the world knowing which function you've executed.

**Private bytecode**

The bytecode of private functions does not need to be distributed to the world; much like real-world contracts.

danger

Privacy is not guaranteed without care.
Although Aztec provides the tools for private smart contracts, information can still be leaked unless the dapp developer is careful.
Aztec is still under development, so real-world, meaningful, valuable secrets *should not* be entrusted to the system.
This page outlines some best practices to aid dapp developers.

---

## Leaky practicesThere are many caveats to the above. Since Aztec also enables interaction with the *public* world (public L2 functions and L1 functions), private information can be accidentally leaked if developers aren't careful.

## Crossing the private -> public boundaryAny time a private function makes a call to a public function, information is leaked. Now, that might be perfectly fine in some use cases (it's up to the smart contract developer). Indeed, most interesting apps will require some public state. But let's have a look at some leaky patterns:

* Calling a public function from a private function. The public function execution will be publicly visible.
* Calling a public function from a private function, without revealing the `msg_sender` of that call. (Otherwise the `msg_sender` will be publicly visible).
* Passing arguments to a public function from a private function. All of those arguments will be publicly visible.
* Calling an internal public function from a private function. The fact that the call originated from a private function of that same contract will be trivially known.
* Emitting unencrypted events from a private function. The unencrypted event name and arguments will be publicly visible.
* Sending L2->L1 messages from a private function. The entire message, and the resulting L1 function execution will all be publicly visible.

## Crossing the public -> private boundaryIf a public function sends a message to be consumed by a private function, the act of consuming that message might be leaked if not following recommended patterns.

## Timing of transactionsInformation about the nature of a transaction can be leaked based on the timing of that transaction.

If a transaction is executed at 8am GMT, it's much less likely to have been made by someone in the USA.

If there's a spike in transactions on the last day of every month, those might be salaries.

These minor details are information that can disclose much more information about a user than the user might otherwise expect.

Suppose that every time Alice sends Bob a private token, 1 minute later a transaction is always submitted to the tx pool with the same kind of 'fingerprint'. Alice might deduce that these transactions are automated reactions by Bob. (Here, 'fingerprint' is an intentionally vague term. It could be a public function call, or a private tx proof with a particular number of nonzero public inputs, or some other discernible pattern that Alice sees).

TL;DR: app developers should think about the *timing* of user transactions, and how this might leak information.

## Function Fingerprints and Tx FingerprintsA 'Function Fingerprint' is any data which is exposed by a function to the outside world. A 'Tx Fingerprint' is any data which is exposed by a tx to the outside world. We're interested in minimizing leakages of information from private txs. The leakiness of a Tx Fingerprint depends on the leakiness of its constituent functions' Function Fingerprints *and* on the appearance of the tx's Tx Fingerprint as a whole. For a private function (and by extension, for a private tx), the following information *could* be leaked (depending on the function, of course):

* All calls to public functions.
  + The contract address of the private function (if it calls an internal public function).
    - This could be the address of the transactor themselves, if the calling contract is an account contract.
  + All arguments which are passed to public functions.
* All calls to L1 functions (in the form of L2 -> L1 messages).
  + The contents of L2 -> L1 messages.
* All public logs (topics and arguments).
* The roots of all trees which have been read from.
* The *number* of ['side effects'](https://en.wikipedia.org/wiki/Side_effect_(computer_science)):
  + # new note hashes
  + # new nullifiers
  + # bytes of encrypted logs
  + # public function calls
  + # L2->L1 messages
  + # nonzero roots[^1]

> Note: many of these were mentioned in the ["Crossing the private -> public boundary"](#crossing-the-private---public-boundary) section.

> Note: the transaction effects submitted to L1 is [encoded (GitHub link)](https://github.com/AztecProtocol/aztec-packages/blob/master/l1-contracts/src/core/libraries/Decoder.sol) but not garbled with other transactions: the distinct Tx Fingerprint of each tx can is publicly visible when a tx is submitted to the L2 tx pool.

## Standardizing FingerprintsIf each private function were to have a unique Fingerprint, then all private functions would be distinguishable from each-other, and all of the efforts of the Aztec protocol to enable 'private function execution' would have been pointless. Standards need to be developed, to encourage smart contract developers to adhere to a restricted set of Tx Fingerprints. For example, a standard might propose that the number of new note hashes, nullifiers, logs, etc. must always be equal, and must always equal a power of two. Such a standard would effectively group private functions/txs into 'privacy sets', where all functions/txs in a particular 'privacy set' would look indistinguishable from each-other, when executed.

## Data queriesIt's not just the broadcasting of transactions to the network that can leak data.

Ethereum has a notion of a 'full node' which keeps-up with the blockchain and stores the full chain state. Many users don't wish to run full nodes, so rely on 3rd-party 'full-node-as-a-service' infrastructure providers, who service blockchain queries from their users.

This pattern is likely to develop in Aztec as well, except there's a problem: privacy. If a privacy-seeking user makes a query to a 3rd-party 'full node', that user might leak data about who they are; about their historical network activity; or about their future intentions. One solution to this problem is "always run a full node", but pragmatically, not everyone will. To protect less-advanced users' privacy, research is underway to explore how a privacy-seeking user may request and receive data from a 3rd-party node without revealing what that data is, nor who is making the request.

App developers should be aware of this avenue for private data leakage. **Whenever an app requests information from a node, the entity running that node is unlikely to be your user!**

## What kind of queries can be leaky?## Querying for up-to-date note sibling pathsTo read a private state is to read a note from the note hash tree. To read a note is to prove existence of that note in the note hash tree. And to prove existence is to re-compute the root of the note hash tree using the leaf value, the leaf index, and the sibling path of that leaf. This computed root is then exposed to the world, as a way of saying "This note exists", or more precisely "This note has existed at least since this historical snapshot time".

If an old historical snapshot is used, then that old historical root will be exposed, and this leaks some information about the nature of your transaction: it leaks that your note was created before the snapshot date. It shrinks the 'privacy set' of the transaction to a smaller window of time than the entire history of the network.

So for maximal privacy, it's in a user's best interest to read from the very-latest snapshot of the data tree.

Naturally, the note hash tree is continuously changing as new transactions take place and their new notes are appended. Most notably, the sibling path for every leaf in the tree changes every time a new leaf is appended.

If a user runs their own node, there's no problem: they can query the latest sibling path for their note(s) from their own machine without leaking any information to the outside world.

But if a user is not running their own node, they would need to query the very-latest sibling path of their note(s) from some 3rd-party node. In order to query the sibling path of a leaf, the leaf's index needs to be provided as an argument. Revealing the leaf's index to a 3rd-party trivially reveals exactly the note(s) you're about to read. And since those notes were created in some prior transaction, the 3rd-party will be able to link you with that prior transaction. Suppose then that the 3rd-party also serviced the creator of said prior transaction: the 3rd-party will slowly be able to link more and more transactions, and gain more and more insight into a network which is meant to be private!

We're researching cryptographic ways to enable users to retrieve sibling paths from 3rd-parties without revealing leaf indices.

> \* Note: due to the non-uniformity of Aztec transactions, the 'privacy set' of a transaction might not be the entire set of transactions that came before.

## Any queryAny query to a node leaks information to that node.

We're researching cryptographic ways to enable users to query any data privately.

---


# Limitations

Source: https://docs.aztec.network/developers/docs/resources/considerations/limitations

Version: Devnet (v3.0.0-devnet.20251212)

On this page

The Aztec stack is a work in progress. Packages have been released early, to gather feedback on the capabilities of the protocol and user experiences.

## What to expect?* Regular Breaking Changes;
* Missing features;
* Bugs;
* An 'unpolished' UX;
* Missing information.

## Why participate?Front-run the future!

Help shape and define:

* Previously-impossible smart contracts and applications
* Network tooling;
* Network standards;
* Smart contract syntax;
* Educational content;
* Core protocol improvements;

## Limitations developers need to know about* It is a testing environment, it is insecure, and unaudited. It is only for testing purposes.
* `msg_sender` is currently leaking when doing private -> public calls
  + The `msg_sender` will always be set, if you call a public function from the private world, the `msg_sender` will be set to the private caller's address.
  + There are patterns that can mitigate this.
* The initial `msg_sender` is `-1`, which can be problematic for some contracts.
* The number of side-effects attached to a tx (when sending the tx to the mempool) is leaky. At this stage of development, this is *intentional*, so that we can gauge appropriate choices for privacy sets. We have always had clear plans to implement privacy sets so that side effects are much less leaky, and these will be in place come mainnet.
* A transaction can only emit a limited number of side-effects (notes, nullifiers, logs, l2->l1 messages), see [circuit limitations](#circuit-limitations).
  + We haven't settled on the final constants, since we're still in a testing phase. But users could find that certain compositions of nested private function calls (e.g. call stacks that are dynamic in size, based on runtime data) could accumulate so many side-effects as to exceed tx limits. Such txs would then be unprovable. We would love for you to open an issue if you encounter this, as it will help us decide on adequate sizes for our constants.
* There are lots of features that we still want to implement. Checkout github and the forum for details. If you would like a feature, please open an issue on github!

## WARNINGDo not use real, meaningful secrets in Aztec's testnets. Some privacy features are still being worked on, including ensuring a secure "zk" property. Since the Aztec stack is still being worked on, there are no guarantees that real secrets will remain secret.

## LimitationsThere are plans to resolve all of the below.

## It is not auditedNone of the Aztec stack is audited. It's being iterated-on every day. It will not be audited for quite some time.

## Under-constrainedSome of our more-complex circuits are still being worked on, so they will still be be underconstrained.

## What are the consequences?Sound proofs are really only needed as a protection against malicious behavior, which we're not testing for at this stage.

## Keys and Addresses are subject to changeThe way in which keypairs and addresses are derived is still being iterated on as we receive feedback.

## What are the consequences?This will impact the kinds of apps that you can build with the Local Network, as it is today:

Please open new discussions on [discourse](http://discourse.aztec.network) or open issues on [github](http://github.com/AztecProtocol/aztec-packages), if you have requirements that aren't-yet being met by the local network's current key derivation scheme.

## No privacy-preserving queries to nodesEthereum has a notion of a 'full node' which keeps-up with the blockchain and stores the full chain state. Many users don't wish to run full nodes, so rely on 3rd-party 'full-node-as-a-service' infrastructure providers, who service blockchain queries from their users.

This pattern is likely to develop in Aztec as well, except there's a problem: privacy. If a privacy-seeking user makes a query to a 3rd-party 'full node', that user might leak data about who they are, or about their historical network activity, or about their future intentions. One solution to this problem is "always run a full node", but pragmatically, not everyone will. To protect less-advanced users' privacy, research is underway to explore how a privacy-seeking user may request and receive data from a 3rd-party node without revealing what that data is, nor who is making the request.

## No private data authenticationPrivate data should not be returned to an app, unless the user authorizes such access to the app. An authorization layer is not-yet in place.

## What are the consequences?Any app can request and receive any private user data relating to any other private app. Obviously this sounds bad. But the local network is a sandbox, and no meaningful value or credentials should be stored there; only test values and test credentials.

An auth layer will be added in due course.

## No bytecode validationFor safety reasons, bytecode should not be executed unless the PXE/Wallet has validated that the user's intentions (the function signature and contract address) match the bytecode.

## What are the consequences?Without such 'bytecode validation', if the incorrect bytecode is executed, and that bytecode is malicious, it could read private data from some other contract and emit that private data to the world. Obviously this would be bad in production. But the local network is a sandbox, and no meaningful value or credentials should be stored there; only test values and test credentials.

There are plans to add bytecode validation soon.

## Insecure hashesWe are planning a full assessment of the protocol's hashes, including rigorous domain separation.

## What are the consequences?Collisions and other hash-related attacks might be possible in the local network. Obviously that would be bad in production. But it's unlikely to cause problems at this early stage of testing.

## `msg_sender` is leaked when making a private -> public callThere are ongoing discussions [here](<https://forum.aztec.network/t/what-is-msg-sender-when-calling-private-public-plus-a-big-foray-into-stealth-addresses/7527> (and some more recent discussions that need to be documented) around how to address this.

## New Privacy Standards are requiredThere are many [patterns](/developers/docs/resources/considerations/privacy_considerations) which can leak privacy, even on Aztec. Standards haven't been developed yet, to encourage best practices when designing private smart contracts.

## What are the consequences?For example, until community standards are developed to reduce the uniqueness of ['Tx Fingerprints'](/developers/docs/resources/considerations/privacy_considerations#function-fingerprints-and-tx-fingerprints) app developers might accidentally forfeit some function privacy.

## Smart Contract limitationsWe will never be done with all the yummy features we want to add to aztec.nr. We have lots of features that we still want to implement. Please check out github, and please open new issues with any feature requests you might have.

## Circuit limitations## Upper limits on function outputs and tx outputsDue to the rigidity of zk-SNARK circuits, there are upper bounds on the amount of computation a circuit can perform, and on the amount of data that can be passed into and out of a function.

> Blockchain developers are no stranger to restrictive computational environments. Ethereum has gas limits, local variable stack limits, call stack limits, contract deployment size limits, log size limits, etc.

Here are the current constants:

constants

```
// TREES RELATED CONSTANTS  
pub global ARCHIVE_HEIGHT: u32 = 30; // 4-second blocks for 100 years.  
pub global VK_TREE_HEIGHT: u32 = 7;  
pub global FUNCTION_TREE_HEIGHT: u32 = 7; // The number of private functions in a contract.  
pub global NOTE_HASH_TREE_HEIGHT: u32 = 42; // 64 notes/tx (static because of base rollup insertion), 15tps, for 100 years.  
pub global PUBLIC_DATA_TREE_HEIGHT: u32 = 40; // Average of 16 updates/tx (guess), 15tps, 100 years.  
pub global NULLIFIER_TREE_HEIGHT: u32 = NOTE_HASH_TREE_HEIGHT;  
pub global L1_TO_L2_MSG_TREE_HEIGHT: u32 = 36; // 1024 messages per checkpoint, with 72 seconds per checkpoint, for 100 years.  
pub global ARTIFACT_FUNCTION_TREE_MAX_HEIGHT: u32 = FUNCTION_TREE_HEIGHT; // The number of unconstrained functions in a contract. Set to equal the number of private functions in a contract.  
  
pub global NULLIFIER_TREE_ID: Field = 0;  
pub global NOTE_HASH_TREE_ID: Field = 1;  
pub global PUBLIC_DATA_TREE_ID: Field = 2;  
pub global L1_TO_L2_MESSAGE_TREE_ID: Field = 3;  
pub global ARCHIVE_TREE_ID: Field = 4;  
pub global NOTE_HASH_TREE_LEAF_COUNT: u64 = 1 << (NOTE_HASH_TREE_HEIGHT as u64);  
pub global L1_TO_L2_MSG_TREE_LEAF_COUNT: u64 = 1 << (L1_TO_L2_MSG_TREE_HEIGHT as u64);  
  
// SUB-TREES RELATED CONSTANTS  
pub global NOTE_HASH_SUBTREE_HEIGHT: u32 = 6;  
pub global NULLIFIER_SUBTREE_HEIGHT: u32 = 6;  
pub global PUBLIC_DATA_SUBTREE_HEIGHT: u32 = 6;  
pub global L1_TO_L2_MSG_SUBTREE_HEIGHT: u32 = 10;  
pub global NOTE_HASH_SUBTREE_ROOT_SIBLING_PATH_LENGTH: u32 =  
    NOTE_HASH_TREE_HEIGHT - NOTE_HASH_SUBTREE_HEIGHT;  
pub global NULLIFIER_SUBTREE_ROOT_SIBLING_PATH_LENGTH: u32 =  
    NULLIFIER_TREE_HEIGHT - NULLIFIER_SUBTREE_HEIGHT;  
pub global L1_TO_L2_MSG_SUBTREE_ROOT_SIBLING_PATH_LENGTH: u32 =  
    L1_TO_L2_MSG_TREE_HEIGHT - L1_TO_L2_MSG_SUBTREE_HEIGHT;  
// Maximum number of subtrees a L2ToL1Msg wonky tree can have. Used when calculating the out hash of a tx.  
pub global MAX_L2_TO_L1_MSG_SUBTREES_PER_TX: u32 = 3; // ceil(log2(MAX_L2_TO_L1_MSGS_PER_TX))  
  
// "PER TRANSACTION" CONSTANTS  
pub global MAX_NOTE_HASHES_PER_TX: u32 = 1 << NOTE_HASH_SUBTREE_HEIGHT;  
pub global MAX_NULLIFIERS_PER_TX: u32 = 1 << NULLIFIER_SUBTREE_HEIGHT;  
pub global MAX_PRIVATE_CALL_STACK_LENGTH_PER_TX: u32 = 16;  
pub global MAX_ENQUEUED_CALLS_PER_TX: u32 = 32;  
pub global PROTOCOL_PUBLIC_DATA_UPDATE_REQUESTS_PER_TX: u32 = 1; // This is the fee_payer's fee juice balance.  
pub global MAX_TOTAL_PUBLIC_DATA_UPDATE_REQUESTS_PER_TX: u32 =  
    (1 as u8 << PUBLIC_DATA_SUBTREE_HEIGHT as u8) as u32;  
pub global MAX_PUBLIC_DATA_UPDATE_REQUESTS_PER_TX: u32 =  
    MAX_TOTAL_PUBLIC_DATA_UPDATE_REQUESTS_PER_TX - PROTOCOL_PUBLIC_DATA_UPDATE_REQUESTS_PER_TX;  
pub global MAX_PUBLIC_DATA_READS_PER_TX: u32 = 64;  
pub global MAX_L2_TO_L1_MSGS_PER_TX: u32 = 8; // Leave at 8, because it results in sha256 hashing in the Tx Base Rollup  
pub global MAX_NOTE_HASH_READ_REQUESTS_PER_TX: u32 = 64;  
pub global MAX_NULLIFIER_READ_REQUESTS_PER_TX: u32 = 64;  
pub global MAX_KEY_VALIDATION_REQUESTS_PER_TX: u32 = 64;  
pub global MAX_PRIVATE_LOGS_PER_TX: u32 = 64;  
pub global MAX_CONTRACT_CLASS_LOGS_PER_TX: u32 = 1;  
  
// "PER CALL" CONSTANTS  
pub global MAX_NOTE_HASHES_PER_CALL: u32 = 16;  
pub global MAX_NULLIFIERS_PER_CALL: u32 = 16;  
pub global MAX_PRIVATE_CALL_STACK_LENGTH_PER_CALL: u32 = 8;  
pub global MAX_ENQUEUED_CALLS_PER_CALL: u32 = MAX_ENQUEUED_CALLS_PER_TX;  
pub global MAX_L2_TO_L1_MSGS_PER_CALL: u32 = MAX_L2_TO_L1_MSGS_PER_TX;  
pub global MAX_NOTE_HASH_READ_REQUESTS_PER_CALL: u32 = 16;  
pub global MAX_NULLIFIER_READ_REQUESTS_PER_CALL: u32 = 16;  
pub global MAX_KEY_VALIDATION_REQUESTS_PER_CALL: u32 = 16;  
pub global MAX_PRIVATE_LOGS_PER_CALL: u32 = 16;  
pub global MAX_CONTRACT_CLASS_LOGS_PER_CALL: u32 = 1;
```

> [Source code: noir-projects/noir-protocol-circuits/crates/types/src/constants.nr#L31-L93](https://github.com/AztecProtocol/aztec-packages/blob/v3.0.0-devnet.20251212/noir-projects/noir-protocol-circuits/crates/types/src/constants.nr#L31-L93)

## What are the consequences?When you write an Aztec.nr function, there will be upper bounds on the following:

* The number of public state reads and writes;
* The number of note reads and nullifications;
* The number of new notes that may be created;
* The number of encrypted logs that may be emitted;
* The number of unencrypted logs that may be emitted;
* The number of L1->L2 messages that may be consumed;
* The number of L2->L1 messages that may be submitted to L1;
* The number of private function calls;
* The number of public function calls that may be enqueued;

Not only are there limits on a *per function* basis, there are also limits on a *per transaction* basis.

**In particular, these *per-transaction* limits will limit transaction call stack depths**. That means if a function call results in a cascade of nested function calls, and each of those function calls outputs lots of state reads and writes, or logs (etc.), then all of that accumulated output data might exceed the per-transaction limits that we currently have. This would cause such transactions to fail.

There are plans to relax some of this rigidity, by providing many 'sizes' of circuit.

> **In the mean time**, if you encounter a per-transaction limit when testing, please do open an issue to explain what you were trying to do; we'd love to hear about it. And if you're feeling adventurous, you could 'hack' the PXE to increase the limits. **However**, the limits cannot be increased indefinitely. So although we do anticipate that we'll be able to increase them a little bit, don't go mad and provide yourself with 1 million state transitions per transaction. That would be as unrealistic as artificially increasing Ethereum gas limits to 1 trillion.

## There's moreSee the [GitHub issues (GitHub link)](https://github.com/AztecProtocol/aztec-packages/issues) for all known bugs fixes and features currently being worked on.

---


# Migration notes

Source: https://docs.aztec.network/developers/docs/resources/migration_notes

Version: Devnet (v3.0.0-devnet.20251212)

On this page

Aztec is in full-speed development. Literally every version breaks compatibility with the previous ones. This page attempts to target errors and difficulties you might encounter when upgrading, and how to resolve them.

## TBD## [Aztec node, archiver] Deprecated `getPrivateLogs`Aztec node no longer offers a `getPrivateLogs` method. If you need to process the logs of a block, you can instead use `getBlock` and call `getPrivateLogs` on an `L2BlockNew` instance. See the diff below for before/after equivalent code samples.

```
-  const logs = await aztecNode.getPrivateLogs(blockNumber, 1);  
+  const logs = (await aztecNode.getBlock(blockNumber))?.toL2Block().getPrivateLogs();  
## [Aztec.nr] Private event emission API changes  
  
Private events are still emitted via the `emit` function, but this now returns an `EventMessage` type that must have `deliver_to` called on it in order to deliver the event message to the intended recipients. This allows for multiple recipients to receive the same event.  
  
```diff  
- self.emit(event, recipient, delivery_method)  
+ self.emit(event).delivery(recipient, delivery_method)
```

## [Aztec.nr] History proof functions no longer require `storage_slot` parameterThe `RetrievedNote` struct now includes a `storage_slot` field, making it self-contained for proving note inclusion and validity. As a result, the history proof functions in the `aztec::history` module no longer require a separate `storage_slot` parameter.

**Affected functions:**

* `BlockHeader::prove_note_inclusion` - removed `storage_slot: Field` parameter
* `BlockHeader::prove_note_validity` - removed `storage_slot: Field` parameter
* `BlockHeader::prove_note_is_nullified` - removed `storage_slot: Field` parameter
* `BlockHeader::prove_note_not_nullified` - removed `storage_slot: Field` parameter

**Migration:**

The `storage_slot` is now read from `retrieved_note.storage_slot` internally. Simply remove the `storage_slot` argument from all calls to these functions:

```
  let header = context.get_anchor_block_header();  
- header.prove_note_inclusion(retrieved_note, storage_slot);  
+ header.prove_note_inclusion(retrieved_note);  
  
  let header = context.get_anchor_block_header();  
- header.prove_note_validity(retrieved_note, storage_slot, context);  
+ header.prove_note_validity(retrieved_note, context);  
  
  let header = context.get_anchor_block_header();  
- header.prove_note_is_nullified(retrieved_note, storage_slot, context);  
+ header.prove_note_is_nullified(retrieved_note, context);  
  
  let header = context.get_anchor_block_header();  
- header.prove_note_not_nullified(retrieved_note, storage_slot, context);  
+ header.prove_note_not_nullified(retrieved_note, context);
```

## [Aztec.nr] Note fields are now publicAll note struct fields are now public, and the `new()` constructor methods and getter methods have been removed. Notes should be instantiated using struct literal syntax, and fields should be accessed directly.

The motivation for this change has been enshrining of randomness which lead to the `new` method being unnecessary boilerplate.

**Affected notes:**

* `UintNote` - `value` is now public, `new()` and `get_value()` removed
* `AddressNote` - `address` is now public, `new()` and `get_address()` removed
* `FieldNote` - `value` is now public, `new()` and `value()` removed

**Migration:**

```
- let note = UintNote::new(100);  
+ let note = UintNote { value: 100 };  
  
- let value = note.get_value();  
+ let value = note.value;  
  
- let address_note = AddressNote::new(owner);  
+ let address_note = AddressNote { address: owner };  
  
- let address = address_note.get_address();  
+ let address = address_note.address;  
  
- let field_note = FieldNote::new(42);  
+ let field_note = FieldNote { value: 42 };  
  
- let value = field_note.value();  
+ let value = field_note.value;
```

## [Aztec.nr] `emit` renamed to `deliver`Private state variable functions that created notes and returned their messages no longer return a `NoteEmission` but instead a `NoteMessage`. These messages are delivered to their owner via `deliver` instead of `emit`. The verb 'emit' remains for things like emitting events.

```
- self.storage.balances.at(owner).add(5).emit(owner);  
+ self.storage.balances.at(owner).add(5).deliver();
```

To deliver a message to a different recipient, use `deliver_to`:

```
- self.storage.balances.at(owner).add(5).emit(other);  
+ self.storage.balances.at(owner).add(5).deliver_to(other);
```

## [Aztec.nr] `ValueNote` renamed to `FieldNote` and `value-note` crate renamed to `field-note`The `ValueNote` struct has been renamed to `FieldNote` to better reflect that it stores a `Field` value. The crate has also been renamed from `value-note` to `field-note`.

**Migration:**

* Update your `Nargo.toml` dependencies: `value_note = { path = "..." }` → `field_note = { path = "..." }`
* Update imports: `use value_note::value_note::ValueNote` → `use field_note::field_note::FieldNote`
* Update type references: `ValueNote` → `FieldNote`
* Update generic parameters: `PrivateSet<ValueNote, ...>` → `PrivateSet<FieldNote, ...>`

## [Aztec.nr] New `balance-set` library for managing token balancesA new `balance-set` library has been created that provides `BalanceSet<Context>` for managing u128 token balances with `UintNote`. This consolidates balance management functionality that was previously duplicated across contracts.

**Features:**

* `add(amount: u128)` - Add to balance
* `sub(amount: u128)` - Subtract from balance (with change note)
* `try_sub(amount: u128, max_notes: u32)` - Attempt to subtract with configurable note limit
* `balance_of()` - Get total balance (unconstrained)

**Usage:**

```
use balance_set::BalanceSet;  
  
#[storage]  
struct Storage<Context> {  
    balances: Owned<BalanceSet<Context>, Context>,  
}  
  
// In a private function:  
self.storage.balances.at(owner).add(amount).deliver(owner, MessageDelivery.CONSTRAINED_ONCHAIN);  
self.storage.balances.at(owner).sub(amount).deliver(owner, MessageDelivery.CONSTRAINED_ONCHAIN);  
  
// In an unconstrained function:  
let balance = self.storage.balances.at(owner).balance_of();
```

## [Aztec.nr] `EasyPrivateUint` deprecated and removedThe `EasyPrivateUint` type and `easy-private-state` crate have been deprecated and removed. Use `BalanceSet` from the `balance-set` crate instead.

**Migration:**

* Remove `easy_private_state` dependency from `Nargo.toml`
* Add `balance_set = { path = "../../../../aztec-nr/balance-set" }` to `Nargo.toml`
* Update storage: `EasyPrivateUint<Context>` → `Owned<BalanceSet<Context>, Context>`
* Update method calls:
  + `add(amount, owner)` → `at(owner).add(amount).deliver(owner, MessageDelivery.CONSTRAINED_ONCHAIN)`
  + `sub(amount, owner)` → `at(owner).sub(amount).deliver(owner, MessageDelivery.CONSTRAINED_ONCHAIN)`
  + `get_value(owner)` → `at(owner).balance_of()` (returns `u128` instead of `Field`)

## [Aztec.nr] `balance_utils` removed from `value-note` (now `field-note`)The `balance_utils` module has been removed from the `field-note` crate (formerly `value-note`). If you need similar functionality, implement it locally in your contract or use `BalanceSet` for u128 balances.

## [Aztec.nr] `filter_notes_min_sum` removed from `value-note` (now `field-note`)The `filter_notes_min_sum` function has been removed from the `field-note` crate (formerly in `value-note`). If you need this functionality, copy it to your contract locally. This function was only used in specific test contracts and doesn't belong in the general-purpose note library.

## [Aztec.nr] `derive_ecdh_shared_secret_using_aztec_address` removedThis function made it annoying to deal with invalid addresses in circuits. If you were using it, replace it with `derive_ecdh_shared_secret` instead:

```
-let shared_secret = derive_ecdh_shared_secret_using_aztec_address(secret, address).unwrap();  
+let shared_secret = derive_ecdh_shared_secret(secret, address.to_address_point().unwrap().inner);
```

## [Aztec.nr] Note owner is now enshrinedIt turns out that in all the cases a note always have a logical owner.
For this reason we have decided to enshrine the concept of a note owner and you should drop the field from your note:

```
#[derive(Deserialize, Eq, Packable, Serialize)]  
#[note]  
pub struct ValueNote {  
    value: Field,  
-    owner: AztecAddress,  
}
```

The owner being enshrined means that our API explicitly expects it on the input.
The `NoteHash` trait got modified as follows:

```
pub trait NoteHash {  
    fn compute_note_hash(  
        self,  
+        owner: AztecAddress,  
        storage_slot: Field,  
        randomness: Field,  
    ) -> Field;  
  
    fn compute_nullifier(  
        self,  
        context: &mut PrivateContext,  
+        owner: AztecAddress,  
        note_hash_for_nullification: Field,  
    ) -> Field;  
  
    unconstrained fn compute_nullifier_unconstrained(  
        self,  
+        owner: AztecAddress,  
        note_hash_for_nullification: Field,  
    ) -> Field;  
}
```

Our low-level note utilities now also accept owner as a parameter:

```
pub fn create_note<Note>(  
    context: &mut PrivateContext,  
+    owner: AztecAddress,  
    storage_slot: Field,  
    note: Note,  
) -> NoteEmission<Note>  
where  
    Note: NoteType + NoteHash + Packable,  
{  
...  
}
```

Signature of some functions like `destroy_note_unsafe` is unchanged:

```
pub fn destroy_note_unsafe<Note>(  
    context: &mut PrivateContext,  
    retrieved_note: RetrievedNote<Note>,  
    note_hash_read: NoteHashRead,  
)  
where  
    Note: NoteHash,  
{  
...  
}
```

because `RetrievedNote` now contains owner.

`PrivateImmutable`, `PrivateMutable` and `PrivateSet` got modified to directly contain the owner instead of implicitly "containing it" by including it in the storage slot via a `Map`.
These state variables now implement a newly introduced `OwnedStateVariable` trait (see docs of `OwnedStateVariable` for explanation of what it is).
These changes make the state variables incompatible with `Map` and now instead these should be wrapped in new `Owned` state variable:

```
#[storage]  
struct Storage<Context> {  
-    private_nfts: Map<AztecAddress, PrivateSet<NFTNote, Context>, Context>,  
+    private_nfts: Owned<PrivateSet<NFTNote, Context>, Context>,  
}
```

Note that even though the types of your state variables are changing from `Map<AztecAddress, T, Context>` to `Owned<T, Context>`, usage remains unchanged:

```
let nft_notes = self.storage.private_nfts.at(from).pop_notes(NoteGetterOptions::new().select(NFTNote::properties().token_id, Comparator.EQ, token_id).set_limit(1));
```

With this change the underlying notes will inherit the storage slot of the `Owned` state variable.
This is unlike `Map` where the nested state variable got the storage slot computed as `hash([map_storage_slot, key])`.

if you had `PrivateImmutable` or `PrivateMutable` defined out of a `Map`, e.g.:

```
#[storage]  
struct Storage<Context> {  
    signing_public_key: PrivateImmutable<PublicKeyNote, Context>,  
}
```

you were most likely dealing with some kind of admin flow where only the admin can modify the state variable.
Now, unfortunately, there is a bit of a regression and you will need to wrap the state variable in `Owned` and call `at` on the state var:

```
+ use aztec::state_vars::Owned;  
  
#[storage]  
struct Storage<Context> {  
-   signing_public_key: PrivateImmutable<PublicKeyNote, Context>,  
+   signing_public_key: Owned<PrivateImmutable<PublicKeyNote, Context>, Context>,  
}  
  
#[external("private")]  
fn my_external_function() {  
-   self.storage.signing_public_key.initialize(pub_key_note)  
+   self.storage.signing_public_key.at(self.address).initialize(pub_key_note)  
        .emit(self.address, MessageDelivery.CONSTRAINED_ONCHAIN);  
}
```

We are likely to come up with a concept of admin state variables in the future.

None of the reference notes now contain the owner so if you manually construct `AddressNote`, `UintNote` or `ValueNote` you need to update the call to `new` method:

```
- let note = UintNote::new(156, owner);  
+ let note = UintNote::new(156);
```

## [Aztec.nr] Note randomness is now handled internallyIn order to prevent pre-image attacks, it is necessary to inject randomness to notes. Aztec.nr users were previously expected to add said randomness to their custom note types. From now on, Aztec.nr takes care of handling randomness as built-in note metadata, making it impossible to miss for library users. This change breaks backwards compatibility as we'll discuss below.

## Changes to Aztec.nr note typesIf you're using any of the following note types, please be aware that `randomness` no longer is an explicit attribute in them.

* ValueNote
* UintNote
* NFTNote
* AddressNote

## Migrating your custom note types: refer to UintNote as an example of how to migrateWe show the changes to `UintNote` below since it serves as a good example of the adjustments you will need to make to your own custom note types, including those that need to support partial notes.

## Remove `randomness` from note struct```
pub struct UintNote {  
    /// The owner of the note, i.e. the account whose nullifier secret key is required to compute the nullifier.  
    owner: AztecAddress,  
-   /// Random value, protects against note hash preimage attacks.  
-   randomness: Field,  
    /// The number stored in the note.  
    value: u128,  
}  
  
impl UintNote {  
    pub fn new(value: u128, owner: AztecAddress) -> Self {  
-       let randomness = unsafe { random() };  
-       Self { value, owner, randomness }  
+       Self { value, owner }  
    }
```

## Add `randomness` to `compute_note_hash` implementationThe `NoteHash` trait now requires `compute_note_hash` to receive a `randomness` field. This impacts

```
pub trait NoteHash {  
    /// ...  
-   fn compute_note_hash(self, storage_slot: Field) -> Field;  
+   fn compute_note_hash(self, storage_slot: Field, randomness: Field) -> Field;
```

Then in trait implementations:

```
impl NoteHash for UintNote {  
-   fn compute_note_hash(self, storage_slot: Field) -> Field {  
+   fn compute_note_hash(self, storage_slot: Field, randomness: Field) -> Field {  
    /// ...  
-   let private_content =  
-       UintPartialNotePrivateContent { owner: self.owner, randomness: self.randomness };  
-   let partial_note = PartialUintNote {  
-        commitment: private_content.compute_partial_commitment(storage_slot),  
-   };  
  
+   let private_content =  
+       UintPartialNotePrivateContent { owner: self.owner };  
+   let partial_note = PartialUintNote {  
+        commitment: private_content.compute_partial_commitment(storage_slot, randomness),  
+   };
```

It's worth noting that this change also affects how partial notes are structured and handled.

```
pub fn partial(  
        owner: AztecAddress,  
        storage_slot: Field,  
        randomness: Field,  
        context: &mut PrivateContext,  
        recipient: AztecAddress,  
        completer: AztecAddress,  
    ) -> PartialUintNote {  
-   let commitment = UintPartialNotePrivateContent { owner, randomness }  
-       .compute_partial_commitment(storage_slot);  
+   let commitment = UintPartialNotePrivateContent { owner }  
+       .compute_partial_commitment(storage_slot, randomness);  
  
    let private_log_content =  
-       UintPartialNotePrivateLogContent { owner, randomness, public_log_tag: commitment };  
+       UintPartialNotePrivateLogContent { owner, public_log_tag: commitment };  
        let encrypted_log = note::compute_partial_note_private_content_log(  
            private_log_content,  
            storage_slot,  
+           randomness,  
            recipient,  
        );  
    /// ...  
}  
  
struct UintPartialNotePrivateContent {  
    owner: AztecAddress,  
-   randomness: Field,  
}  
  
impl UintPartialNotePrivateContent {  
-   fn compute_partial_commitment(self, storage_slot: Field) -> Field {  
+   fn compute_partial_commitment(self, storage_slot: Field, randomness: Field) -> Field {  
        poseidon2_hash_with_separator(  
-           self.pack().concat([storage_slot]),  
+           self.pack().concat([storage_slot, randomness]),  
            GENERATOR_INDEX__NOTE_HASH,  
        )  
    }  
}  
  
struct UintPartialNotePrivateLogContent {  
    public_log_tag: Field,  
    owner: AztecAddress,  
-   randomness: Field,  
}
```

## Note sizeAs a result of this change, the maximum packed length of the content of a note is 11 fields, down from 12. This is a direct consequence of moving the randomness field from the note content structure to the note's metadata.

## RetrievedNote now includes randomness field```
pub struct RetrievedNote<Note> {  
    pub note: Note,  
    pub contract_address: AztecAddress,  
+   pub randomness: Field,  
    pub metadata: NoteMetadata,  
}
```

## [L1 Contracts] `Block` is now `Checkpoint`A `checkpoint` is now the primary unit handled by the L1 contracts.

A checkpoint may contain one or more L2 blocks. The protocol circuits already support producing multiple blocks per checkpoint. Updating the L1 contracts to operate on checkpoints allow L2 blockchain to advance faster.

Below are the API and event renames reflecting this change:

```
- event L2BlockProposed  
+ event CheckpointProposed
```

```
- event BlockInvalidated  
+ event CheckpointInvalidated
```

```
- function getEpochForBlock(uint256 _blockNumber) external view returns (Epoch);  
+ function getEpochForCheckpoint(uint256 _checkpointNumber) external view returns (Epoch);
```

```
- function getProvenBlockNumber() external view returns (uint256);  
+ function getProvenCheckpointNumber() external view returns (uint256);
```

```
- function getPendingBlockNumber() external view returns (uint256);  
+ function getPendingCheckpointNumber() external view returns (uint256);
```

```
- function getBlock(uint256 _blockNumber) external view returns (BlockLog memory);  
+ function getCheckpoint(uint256 _checkpointNumber) external view returns (CheckpointLog memory);
```

```
- function getBlockReward() external view returns (uint256);  
+ function getCheckpointReward() external view returns (uint256);
```

Additionally, any function or struct that previously referenced an L2 block number now uses a checkpoint number instead:

```
- function status(uint256 _blockNumber) external view returns (  
+ function status(uint256 _checkpointNumber) external view returns (  
-    uint256 provenBlockNumber,  
+    uint256 provenCheckpointNumber,  
     bytes32 provenArchive,  
-    uint256 pendingBlockNumber,  
+    uint256 pendingCheckpointNumber,  
     bytes32 pendingArchive,  
     bytes32 archiveOfMyBlock,  
     Epoch provenEpochNumber  
);
```

Note: current node softwares still produce exactly one L2 block per checkpoint, so for now checkpoint numbers and L2 block numbers remain equal. This may change once multi-block checkpoints are enabled.

## [Aztec.js] Wallet interface changes## `simulateTx` is now batchableThe `simulateTx` method on the `Wallet` interface is now batchable, meaning it can be called as part of a batch operation using `wallet.batch()`. This allows you to batch simulations together with other wallet operations like `registerContract`, `sendTx`, and `registerSender`.

```
- // Could not batch simulations  
- const simulationResult = await wallet.simulateTx(executionPayload, options);  
+ // Can now batch simulations with other operations  
+ const results = await wallet.batch([  
+   { name: 'registerContract', args: [instance, artifact] },  
+   { name: 'simulateTx', args: [executionPayload, options] },  
+   { name: 'sendTx', args: [anotherPayload, sendOptions] },  
+ ]);
```

## `ExecutionPayload` moved to `@aztec/stdlib/tx`The `ExecutionPayload` type has been moved from `@aztec/aztec.js` to `@aztec/stdlib/tx`. Update your imports accordingly.

```
- import { ExecutionPayload } from '@aztec/aztec.js';  
+ import { ExecutionPayload } from '@aztec/stdlib/tx';  
+ // Or import from the re-export in aztec.js/tx:  
+ import { ExecutionPayload } from '@aztec/aztec.js/tx';
```

## `ExecutionPayload` now includes `feePayer` propertyThe `ExecutionPayload` class now includes an optional `feePayer` property that specifies which address is paying for the fee in the execution payload (if any)

```
  const payload = new ExecutionPayload(  
    calls,  
    authWitnesses,  
    capsules,  
    extraHashedArgs,  
+   feePayer // optional AztecAddress  
  );
```

This was previously provided as part of the `SendOptions` (and others) in the wallet interface, which could cause problems if a payload was assembled with a payment method and the parameter was later omitted. This means `SendOptions` now loses `embeddedPaymentMethodFeePayer`

```
-wallet.simulateTx(executionPayload, { from: address, embeddedFeePaymentMethodFeePayer: feePayer });  
+wallet.simulateTx(executionPayload, { from: address });
```

## `simulateUtility` signature and return type changedThe `simulateUtility` method signature has changed to accept a `FunctionCall` object instead of separate `functionName`, `args`, and `to` parameters. Additionally, the return type has changed from `AbiDecoded` to `Fr[]`.

```
- const result: AbiDecoded = await wallet.simulateUtility(functionName, args, to, authWitnesses);  
+ const result: UtilitySimulationResult = await wallet.simulateUtility(functionCall, authWitnesses?);  
+ // result.result is now Fr[] instead of AbiDecoded
```

The new signature takes:

* `functionCall`: A `FunctionCall` object containing `name`, `args`, `to`, `selector`, `type`, `isStatic`, `hideMsgSender`, and `returnTypes`
* `authWitnesses` (optional): An array of `AuthWitness` objects

The first argument is exactly the same as what goes into `ExecutionPayload.calls`. As such, the data is already encoded. The return value is now `UtilitySimulationResult` with `result: Fr[]` instead of returning an `AbiDecoded` value directly. You'll need to decode the `Fr[]` array yourself if you need typed results.

## `Contract.at()` is now synchronous and no longer calls `registerContract`The `Contract.at()` method (and generated contract `.at()` methods) is now synchronous and no longer automatically registers the contract with the wallet. This reduces unnecessary artifact storage and RPC calls.

```
- const contract = await TokenContract.at(address, wallet);  
+ const contract = TokenContract.at(address, wallet);
```

**Important:** You now need to explicitly call `registerContract` if you want the wallet to store the contract instance and artifact. This is only necessary when:

* An app first registers a contract
* An app tries to update a contract's artifact

If you need to register the contract, do so explicitly:

```
// Get the instance from deployment  
const { contract, instance } = await TokenContract.deploy(wallet, ...args)  
  .send({ from: address })  
  .wait();  
  
// wallet already has it registered, since the deploy method does it by default  
// to avoid it, set skipContractRegistration: true in the send options.  
  
// Register it with another wallet  
await otherWallet.registerContract(instance, TokenContract.artifact);  
  
// Now you can use the contract  
const otherContract = TokenContract.at(instance.address, otherWallet);
```

Publicly deployed contract instances can be retrieved via `node.getContract(address)`. Otherwise and if deployment parameters are known, an instance can be computed via the `getContractInstanceFromInstantiationParams` from `@aztec/aztec.js/contracts`

## `registerContract` signature simplifiedThe `registerContract` method now takes a `ContractInstanceWithAddress` instead of a `Contract` object, and the `artifact` parameter is now optional. If the artifact is not provided, the wallet will attempt to look it up from its contract class storage.

```
- await wallet.registerContract(contract);  
+ await wallet.registerContract(instance, artifact?);
```

The method now only accepts:

* `instance`: A `ContractInstanceWithAddress` object
* `artifact` (optional): A `ContractArtifact` object
* `secretKey` (optional): A secret key for privacy keys registration

## Return value of `getNotes` no longer contains a recipient and it contains some other additional infoReturn value of `getNotes` used to be defined as `Promise<UniqueNote[]>` and now it's defined as `Promise<UniqueNote[]>`.
`NoteDao` is mostly a super-set of `UniqueNote` but it doesn't contain a `recipient`.
Having the recipient in the return value has been redundant as the same outcome can be achieved by populating the `scopes` array in `NoteFilter` with the `recipient` value.

## Changes to `getPrivateEvents`The signature of `getPrivateEvents` has changed for two reasons:

1. To align it with how other query methods that include filtering by block range work (for example, `AztecNode#getPublicLogs`)
2. To enrich the returned private events with metadata.

```
getPrivateEvents<T>(  
-    contractAddress: AztecAddress,  
-    eventMetadata: EventMetadataDefinition,  
-    from: number,  
-    numBlocks: number,  
-    recipients: AztecAddress[],  
-  ): Promise<T[]>;  
+    eventFilter: PrivateEventFilter,  
+  ): Promise<PrivateEvent<T>[]>;
```

`PrivateEvent<T>` bundles together an ABI decoded event of type `T`, with `metadata` of type `InTx`:

```
export type InBlock = {  
  l2BlockNumber: BlockNumber;  
  l2BlockHash: L2BlockHash;  
};  
  
export type InTx = InBlock & {  
  txHash: TxHash;  
};  
  
export type PrivateEvent<T> = {  
  event: T;  
  metadata: InTx;  
};
```

You will need to update any calls to `Wallet#getPrivateEvents` accordingly. See below for before/after comparison which conserves
semantics.

Pay special attention to the fact that the old method expects a `numBlocks` parameter that instructs it to
return `numBlocks` blocks after `fromBlock`, whereas the new version expects an (exclusive) `toBlock` block number.

Also note we're replacing *recipient* terminology with *scope*. While underlying data types are equivalent (they are Aztec addresses), they have different semantics. Messages have a recipient who will be able to receive and process them. As a result of processing messages for a given recipient address, PXE might discover events. Those events are then said to be *in scope* for that address.

```
- const events = await context.client.getPrivateEvents(contractAddress, eventMetadata, 42, 10, [recipient]);  
- doSomethingWithAnEvent(events[0]);  
  
+ const events = await context.client.getPrivateEvents(eventMetadata, {  
+   contractAddress,  
+   fromBlock: BlockNumber(42),  
+   toBlock: BlockNumber(42 + 10),  
+   scopes: [scope],  
+ });  
+ doSomethingWithAnEvent(events[0].event);
```

Please refer to the wallet interface js-docs for further details.

## [CLI] Command refactorThe sandbox command has been renamed and remapped to "local network". We believe this conveys better what is actually being spun up when running it.

**REMOVED/RENAMED**:

* `aztec start --sandbox`: now `aztec start --local-network`

## [Aztec.nr] - Contract API redesignIn this release we decided to largely redesign our contract API. Most of the changes here are not a breaking change
(only renaming of original `#[internal]` to `#[only_self]` and `storage` now being available on the newly introduced
`self` struct are a breaking change).

## 1. Renaming of original #[internal] as #[only\_self]We want for internal to mean the same as in Solidity where internal function can be called only from the same contract
and is also inlined (EVM JUMP opcode and not EVM CALL). The original implementation of our `#[internal]` macro also
results in the function being callable only from the same contract but it results in a different call (hence it doesn't
map to EVM JUMP). This is very confusing for people that know Solidity hence we are doing the rename. A true
`#[internal]` will be introduced in the future.

To migrate your contracts simply rename all the occurrences of `#[internal]` with `#[only_self]` and update the imports:

```
- use aztec::macros::functions::internal;  
+ use aztec::macros::functions::only_self;
```

```
#[external("public")]  
- #[internal]  
+ #[only_self]  
fn _deduct_public_balance(owner: AztecAddress, amount: u64) {  
    ...  
}
```

## 2. Introducing of new #[internal]Same as in Solidity internal functions are functions that are callable from inside the contract. Unlike #[only\_self]
functions, internal functions are inlined (e.g. akin to EVM's JUMP and not EVM's CALL).

Internal function can be called using the following API which leverages the new `self` struct (see change 3 below for
details):

```
self.internal.my_internal_function(...)
```

Private internal functions can only be called from other private external or internal functions.
Public internal functions can only be called from other public external or internal functions.

## 3. Introducing `self` in contracts and a new call interfaceAztec contracts now automatically inject a `self` parameter into every contract function, providing a unified interface
for accessing the contract's address, storage, calling of function and an execution context.

## What is `self`?`self` is an instance of `ContractSelf<Context, Storage>` that provides:

* `self.address` - The contract's own address
* `self.storage` - Access to your contract's storage
* `self.context` - The execution context (private, public, or utility)
* `self.msg_sender()` - Get the address of the caller
* `self.emit(...)` - Emit events
* `self.call(...)` - Call an external function
* `self.view(...)` - Call an external function statically
* `self.enqueue(...)` - Enqueue a call to an external function
* `self.enqueue_view(...)` - Enqueue a call to an external function
* `self.enqueue_incognito(...)` - Enqueue a call to an external function but hides the `msg_sender`
* `self.enqueue_view_incognito(...)` - Enqueue a static call to an external function but hides the `msg_sender`
* `self.set_as_teardown(...)` - Enqueue a call to an external public function and sets the call as teardown
* `self.set_as_teardown_incognito(...)` - Enqueue a call to an external public function and sets the call as teardown
  and hides the `msg_sender`
* `self.internal.my_internal_fn(...)` - Call an internal function

`self` also provides you with convenience API to call and enqueue calls to external functions from within the same
contract (this is just a convenience API as `self.call(MyContract::at(self.address).my_external_fn(...))` would also
work):

* `self.call_self.my_external_fn(...)` - Call external function from within the same contract
* `self.enqueue_self.my_public_external_fn(...)`
* `self.call_self_static.my_static_external_fn(...)`
* `self.enqueue_self_static.my_static_external_public_fn(...)`

## How it worksThe `#[external(...)]` macro automatically injects `self` into your function. When you write:

```
#[external("private")]  
fn transfer(amount: u128, recipient: AztecAddress) {  
    let sender = self.msg_sender().unwrap();  
    self.storage.balances.at(sender).sub(amount);  
    self.storage.balances.at(recipient).add(amount);  
}
```

The macro transforms it to initialize `self` with the context and storage before your code executes.

## Migration guide**Before:** Access context and storage as separate parameters

```
#[external("private")]  
fn old_transfer(amount: u128, recipient: AztecAddress) {  
    let storage = Storage::init(context);  
    let sender = context.msg_sender().unwrap();  
    storage.balances.at(sender).sub(amount);  
}
```

**After:** Use `self` to access everything

```
#[external("private")]  
fn new_transfer(amount: u128, recipient: AztecAddress) {  
    let sender = self.msg_sender().unwrap();  
    self.storage.balances.at(sender).sub(amount);  
}
```

## Key changes1. **Storage and context access:**

Storage and context are no longer injected into the function as standalone variables and instead you need to access them via `self`:

```
- let balance = storage.balances.at(owner).read();  
+ let balance = self.storage.balances.at(owner).read();
```

```
- context.push_nullifier(nullifier);  
+ self.context.push_nullifier(nullifier);
```

Note that `context` is expected to be use only when needing to access a low-level API (like directly emitting a nullifier).

2. **Getting caller address:** Use `self.msg_sender()` instead of `context.msg_sender()`

   ```
   - let caller = context.msg_sender().unwrap();  
   + let caller = self.msg_sender().unwrap();
   ```
3. **Getting contract address:** Use `self.address` instead of `context.this_address()`

   ```
   - let this_contract = context.this_address();  
   + let this_contract = self.address;
   ```
4. **Emitting events:**

   In private functions:

   ```
   - emit_event_in_private(event, context, recipient, delivery_mode);  
   + self.emit(event, recipient, delivery_mode);
   ```

   In public functions:

   ```
   - emit_event_in_public(event, context);  
   + self.emit(event);
   ```
5. **Calling functions:**

   In private functions:

   ```
   - Token::at(stable_coin).mint_to_public(to, amount).call(&mut context);  
   + self.call(Token::at(stable_coin).mint_to_public(to, amount));
   ```

## Example: Full contract migration**Before:**

```
#[external("private")]  
fn withdraw(amount: u128, recipient: AztecAddress) {  
    let storage = Storage::init(context);  
    let sender = context.msg_sender().unwrap();  
    let token = storage.donation_token.get_note().get_address();  
  
    // ... withdrawal logic  
  
    emit_event_in_private(Withdraw { withdrawer, amount }, context, withdrawer, MessageDelivery.UNCONSTRAINED_ONCHAIN);  
}
```

**After:**

```
#[external("private")]  
fn withdraw(amount: u128, recipient: AztecAddress) {  
    let sender = self.msg_sender().unwrap();  
    let token = self.storage.donation_token.get_note().get_address();  
  
    // ... withdrawal logic  
  
    self.emit(Withdraw { withdrawer, amount }, withdrawer, MessageDelivery.UNCONSTRAINED_ONCHAIN);  
}
```

## No-longer allowing calling of non-view function statically via the old higher-level APIWe used to allow calling of non-view function statically as follows:

```
MyContract::at(address).my_non_view_function(...).view(context);  
MyContract::at(address).my_non_view_function(...).enqueue_view(context);
```

This is no-longer allowed and if you will want to call a function statically you will need to mark the function with
`#[view]`.

## Phase checksNow private external functions check by default that no phase change from non revertible to revertible happens during the execution of the function or any of its nested calls. If you're developing a function
that handles phase change (you call `context.end_setup()` or call a function that you expect will change phase) you need to opt out of the phase check using the `#[nophasecheck]` macro. Also, now it's possible to know if you're in the revertible phase of the transaction at any point using `self.context.in_revertible_phase()`.

## [`aztec` command] Moving functionality of `aztec-nargo` to `aztec` command`aztec-nargo` has been deprecated and all workflows should now migrate to the `aztec` command that fully replaces `aztec-nargo`:

* **For contract initialization:**

  ```
  aztec init
  ```

  (Behaves like `nargo init`, but defaults to a contract project.)
* **For testing:**

  ```
  aztec test
  ```

  (Starts the Aztec TXE and runs your tests.)
* **For compiling contracts:**

  ```
  aztec compile
  ```

  (Transpiles your contracts and generates verification keys.)

## 3.0.0-devnet.4## [aztec.js] Removal of barrel export`aztec.js` is now divided into granular exports, which improves loading performance in node.js and also makes the job of web bundlers easier:

```
-import { AztecAddress, Fr, getContractInstanceFromInstantiationParams, type Wallet } from '@aztec/aztec.js';  
+import { AztecAddress } from '@aztec/aztec.js/addresses';  
+import { getContractInstanceFromInstantiationParams } from '@aztec/aztec.js/contracts';  
+import { Fr } from '@aztec/aztec.js/fields';  
+import type { Wallet } from '@aztec/aztec.js/wallet';
```

Additionally, some general utilities reexported from `foundation` have been removed:

```
-export { toBigIntBE } from '@aztec/foundation/bigint-buffer';  
-export { sha256, Grumpkin, Schnorr } from '@aztec/foundation/crypto';  
-export { makeFetch } from '@aztec/foundation/json-rpc/client';  
-export { retry, retryUntil } from '@aztec/foundation/retry';  
-export { to2Fields, toBigInt } from '@aztec/foundation/serialize';  
-export { sleep } from '@aztec/foundation/sleep';  
-export { elapsed } from '@aztec/foundation/timer';  
-export { type FieldsOf } from '@aztec/foundation/types';  
-export { fileURLToPath } from '@aztec/foundation/url';
```

## `getSenders` renamed to `getAddressBook` in wallet interfaceAn app could request "contacts" from the wallet, which don't necessarily have to be senders in the wallet's PXE. This method has been renamed to reflect that fact:

```
-wallet.getSenders();  
+wallet.getAddressBook();
```

## Removal of `proveTx` from `Wallet` interfaceExposing this method on the interface opened the door for certain types of attacks, were an app could route proven transactions through malicious nodes (that stored them for later decryption, or collected user IPs for example). It also made transactions difficult to track for the wallet, since they could be sent without their knowledge at any time. This change also affects `ContractFunctionInteraction` and `DeployMethod`, which no longer expose a `prove()` method.

## `msg_sender` is now an `Option<AztecAddress>` type.Because Aztec has native account abstraction, the very first function call of a tx has no `msg_sender`. (Recall, the first function call of an Aztec transaction is always a *private* function call).

Previously (before this change) we'd been silently setting this first `msg_sender` to be `AztecAddress::from_field(-1);`, and enforcing this value in the protocol's kernel circuits. Now we're passing explicitness to smart contract developers by wrapping `msg_sender` in an `Option` type. We'll explain the syntax shortly.

We've also added a new protocol feature. Previously (before this change) whenever a public function call was enqueued by a private function (a so-called private->public call), the called public function (and hence the whole world) would be able to see `msg_sender`. For some use cases, visibility of `msg_sender` is important, to ensure the caller executed certain checks in private-land. For `#[only_self]` public functions, visibility of `msg_sender` is unavoidable (the caller of an `#[only_self]` function must be the same contract address by definition). But for *some* use cases, a visible `msg_sender` is an unnecessary privacy leakage.
We therefore have added a feature where `msg_sender` can be optionally set to `Option<AztecAddress>::none()` for enqueued public function calls (aka private->public calls). We've been colloquially referring to this as "setting msg\_sender to null".

## Aztec.nr diffs> Note: we'll be doing another pass at this aztec.nr syntax in the near future.

Given the above, the syntax for accessing `msg_sender` in Aztec.nr is slightly different:

For most public and private functions, to adjust to this change, you can make this change to your code:

```
- let sender: AztecAddress = context.msg_sender();  
+ let sender: AztecAddress = context.msg_sender().unwrap();
```

Recall that `Option::unwrap()` will throw if the Option is "none".

Indeed, most smart contract functions will require access to a proper contract address (instead of a "null" value), in order to do bookkeeping (allocation of state variables against user addresses), and so in such cases throwing is sensible behaviour.

If you want to output a useful error message when unwrapping fails, you can use `Option::expect`:

```
- let sender: AztecAddress = context.msg_sender();  
+ let sender: AztecAddress = context.msg_sender().expect(f"Sender must not be none!");
```

For a minority of functions, a "null" msg\_sender will be acceptable:

* A private entrypoint function.
* A public function which doesn't seek to do bookkeeping against `msg_sender`.

Some apps might even want to *assert* that the `msg_sender` is "null" to force their users into strong privacy practices:

```
let sender: Option<AztecAddress> = context.msg_sender();  
assert(sender.is_none());
```

## Enqueueing public function calls## Auto-generated contract interfacesWhen you use the `#[aztec]` macro, it will generate a noir contract interface for your contract, behind the scenes.

This provides pretty syntax when you come to call functions of that contract. E.g.:

```
Token::at(context.this_address())._increase_public_balance(to, amount).enqueue(&mut context);
```

In keeping with this new feature of being able to enqueue public function calls with a hidden `msg_sender`, there are some new methods that can be chained instead of `.enqueue(...)`:

* `enqueue_incognito` -- akin to `enqueue`, but `msg_sender` is set "null".
* `enqueue_view_incognito` -- akin to `enqueue_view`, but `msg_sender` is "null".
* `set_as_teardown_incognito` -- akin to `set_as_teardown`, but `msg_sender` is "null".

> The name "incognito" has been chosen to imply "msg\_sender will not be visible to observers".

These new functions enable the *calling* contract to specify that it wants its address to not be visible to the called public function. This is worth re-iterating: it is the *caller's* choice. A smart contract developer who uses these functions must be sure that the target public function will accept a "null" `msg_sender`. It would not be good (for example) if the called public function did `context.msg_sender().unwrap()`, because then a public function that is called via `enqueue_incognito` would *always fail*! Hopefully smart contract developers will write sufficient tests to catch such problems during development!

## Making lower-level public function calls from the private contextThis is discouraged vs using the auto-generated contract interfaces described directly above.

If you do use any of these low-level methods of the `PrivateContext` in your contract:

* `call_public_function`
* `static_call_public_function`
* `call_public_function_no_args`
* `static_call_public_function_no_args`
* `call_public_function_with_calldata_hash`
* `set_public_teardown_function`
* `set_public_teardown_function_with_calldata_hash`

... there is a new `hide_msg_sender: bool` parameter that you will need to specify.

## Aztec.js diffs> Note: we'll be doing another pass at this aztec.js syntax in the near future.

When lining up a new tx, the `FunctionCall` struct has been extended to include a `hide_msg_sender: bool` field.

* `is_public & hide_msg_sender` -- will make a public call with `msg_sender` set to "null".
* `is_public & !hide_msg_sender` -- will make a public call with a visible `msg_sender`, as was the case before this new feature.
* `!is_public & hide_msg_sender` -- Incompatible flags.
* `!is_public & !hide_msg_sender` -- will make a private call with a visible `msg_sender` (noting that since it's a private function call, the `msg_sender` will only be visible to the called private function, but not to the rest of the world).

## [cli-wallet]The `deploy-account` command now requires the address (or alias) of the account to deploy as an argument, not a parameter

```
+aztec-wallet deploy-account main  
-aztec-wallet deploy-account -f main
```

This release includes a major architectural change to the system.
The PXE JSON RPC Server has been removed, and PXE is now available only as a library to be used by wallets.

## [Aztec node]Network config. The node now pulls default configuration from the public repository [AztecProtocol/networks](https://github.com/AztecProtocol/networks) after it applies the configuration it takes from the running environment and the configuration values baked into the source code. See associated [Design document](https://github.com/AztecProtocol/engineering-designs/blob/15415a62a7c8e901acb8e523625e91fc6f71dce4/docs/network-config/dd.md)

## [Aztec.js]## Removing Aztec cheatcodesThe Aztec cheatcodes class has been removed. Its functionality can be replaced by using the `getNotes(...)` function directly available on our `TestWallet`, along with the relevant functions available on the Aztec Node interface (note that the cheatcodes were generally just a thin wrapper around the Aztec Node interface).

## CLI Wallet commands dropped from `aztec` commandThe following commands used to be exposed by both the `aztec` and the `aztec-wallet` commands:

* import-test-accounts
* create-account
* deploy-account
* deploy
* send
* simulate
* profile
* bridge-fee-juice
* create-authwit
* authorize-action
* get-tx
* cancel-tx
* register-sender
* register-contract

These were dropped from `aztec` and now are exposed only by the `cli-wallet` command exposed by the `@aztec/cli-wallet` package.

## PXE commands dropped from `aztec` commandThe following commands were dropped from the `aztec` command:

* `add-contract`: use can be replaced with `register-contract` on our `cli-wallet`
* `get-contract-data`: debug-only and not considered important enough to need a replacement
* `get-accounts`: debug-only and can be replaced by loading aliases from `cli-wallet`
* `get-account`: debug-only and can be replaced by loading aliases from `cli-wallet`
* `get-pxe-info`: debug-only and not considered important enough to need a replacement

## [Aztec.nr]## Replacing #[private], #[public], #[utility] with #[external(...)] macro] macro")

The original naming was not great in that it did not sufficiently communicate what the given macro did.
We decided to rename `#[private]` as `#[external("private")]`, `#[public]` as `#[external("public")]`, and `#[utility]` as `#[external("utility")]` to better communicate that these functions are externally callable and to specify their execution context. In this sense, `external` now means the exact same thing as in Solidity, i.e. a function that can be called from other contracts, and that can only be invoked via a contract call (i.e. the `CALL` opcode in the EVM, and a kernel call/AVM `CALL` opcode in Aztec).

You have to do the following changes in your contracts:

Update import:

```
- use aztec::macros::functions::private;  
- use aztec::macros::functions::public;  
- use aztec::macros::functions::utility;  
+ use aztec::macros::functions::external;
```

Update attributes of your functions:

```
-    #[private]  
+    #[external("private")]  
    fn my_private_func() {
```

```
-    #[public]  
+    #[external("public")]  
    fn my_public_func() {
```

```
-    #[utility]  
+    #[external("utility")]  
    fn my_utility_func() {
```

## Dropping remote mutable references to public context`PrivateContext` generally needs to be passed as a mutable reference to functions because it does actually hold state
we're mutating. This is not the case for `PublicContext`, or `UtilityContext` - these are just marker objects that
indicate the current execution mode and make available the correct subset of the API. For this reason we have dropped
the mutable reference from the API.

If you've passed the context as an argument to custom functions you will need to do the following migration (example
from our token contract):

```
#[contract_library_method]  
fn _finalize_transfer_to_private(  
    from_and_completer: AztecAddress,  
    amount: u128,  
    partial_note: PartialUintNote,  
-    context: &mut PublicContext,  
-    storage: Storage<&mut PublicContext>,  
+    context: PublicContext,  
+    storage: Storage<PublicContext>,  
) {  
    ...  
}
```

## Authwit Test Helper now takes `env`The `add_private_authwit_from_call_interface` test helper available in `test::helpers::authwit` now takes a `TestEnvironment` parameter, mirroring `add_public_authwit_from_call_interface`. This adds some unfortunate verbosity, but there are bigger plans to improve authwit usage in Noir tests in the near future.

```
add_private_authwit_from_call_interface(  
+   env,  
    on_behalf_of,  
    caller,  
    call_interface,  
);
```

## Historical block renamed as anchor blockA historical block term has been used as a term that denotes the block against which a private part of a tx has been executed.
This name is ambiguous and for this reason we've introduce "anchor block".
This naming change resulted in quite a few changes and if you've access private context's or utility context's block header you will need to update your code:

```
- let header = context.get_block_header();  
+ let header = context.get_anchor_block_header();
```

## Removed ValueNote utilsThe `value_note::utils` module has been removed because it was incorrect to have those in the value note package.

For the increment function you can easily just insert the note:

```
- use value_note::utils;  
- utils::increment(storage.notes.at(owner), value, owner, sender);  
+ let note = ValueNote::new(value, owner);  
+ storage.notes.at(owner).insert(note).emit(&mut context, owner, MessageDelivery.CONSTRAINED_ONCHAIN);
```

## PrivateMutable: replace / initialize\_or\_replace behaviour change**Motivation:**
Updating a note used to require reading it first (via `get_note`, which nullifies and recreates it) and then calling `replace` — effectively proving a note twice. Now, `replace` accepts a callback that transforms the current note directly, and `initialize_or_replace` simply uses this updated `replace` internally. This reduces circuit cost while maintaining exactly one current note.

**Key points:**

1. `replace(self, new_note)` (old) → `replace(self, f)` (new), where `f` takes the current note and returns a transformed note.
2. `initialize_or_replace(self, note)` (old) → `initialize_or_replace(self, f)` (new), where `f` takes an `Option` with the current none, or `none` if uninitialized.
3. Previous note is automatically nullified before the new note is inserted.
4. `NoteEmission<Note>` still requires `.emit()` or `.discard()`.

**Example Migration:**

```
- let current_note = storage.my_var.get_note();  
- let new_note = f(current_note);  
- storage.my_var.replace(new_note);  
+ storage.my_var.replace(|current_note| f(current_note));
```

```
- storage.my_var.initialize_or_replace(new_note);  
+ storage.my_var.initialize_or_replace(|_| new_note);
```

This makes it easy and efficient to handle both initialization and current value mutation via `initialize_or_replace`, e.g. if implementing a note that simply counts how many times it has been read:

```
+ storage.my_var.initialize_or_replace(|opt_current: Option<Note>| opt_current.unwrap_or(0 /* initial value */) + 1);
```

* The callback can be a closure (inline) or a named function.
* Any previous assumptions that replace simply inserts a new\_note directly must be updated.

## Unified oracles into single get\_utility\_context oracleThe following oracles:

1. get\_contract\_address,
2. get\_block\_number,
3. get\_timestamp,
4. get\_chain\_id,
5. get\_version

were replaced with a single `get_utility_context` oracle whose return value contains all the values returned from the removed oracles.

If you have used one of these removed oracles before, update the import, e.g.:

```
- aztec::oracle::execution::get_chain_id;  
+ aztec::oracle::execution::get_utility_context
```

and get the value out of the returned utility context:

```
- let chain_id = get_chain_id();  
+ let chain_id = get_utility_context().chain_id();
```

## Note emission API changesThe note emission API has been significantly reworked to provide clearer semantics around message delivery guarantees. The key changes are:

1. `encode_and_encrypt_note` has been removed in favor of calling `emit` directly with `MessageDelivery.CONSTRAINED_ONCHAIN`
2. `encode_and_encrypt_note_unconstrained` has been removed in favor of calling `emit` directly with `MessageDelivery.UNCONSTRAINED_ONCHAIN`
3. `encode_and_encrypt_note_and_emit_as_offchain_message` has been removed in favor of using `emit` with `MessageDelivery.UNCONSTRAINED_OFFCHAIN`
4. Note emission now takes a `delivery_mode` parameter with the following values:
   * `CONSTRAINED_ONCHAIN`: For onchain delivery with cryptographic guarantees that recipients can discover and decrypt messages. Uses constrained encryption but is slower to prove. Best for critical messages that contracts need to verify.
   * `UNCONSTRAINED_ONCHAIN`: For onchain delivery without encryption constraints. Faster proving but trusts the sender. Good when the sender is incentivized to perform encryption correctly (e.g. they are buying something and will only get it if the recipient sees the note). No guarantees that recipients will be able to find or decrypt messages.
   * `UNCONSTRAINED_OFFCHAIN`: For offchain delivery (e.g. cloud storage) without constraints. Lowest cost since no onchain storage needed. Requires custom infrastructure for delivery. No guarantees that messages will be delivered or that recipients will ever find them.
5. The `context` object no longer needs to be passed to these functions

Example migration:

First you need to update imports in your contract:

```
- aztec::messages::logs::note::encode_and_encrypt_note;  
- aztec::messages::logs::note::encode_and_encrypt_note_unconstrained;  
- aztec::messages::logs::note::encode_and_encrypt_note_and_emit_as_offchain_message;  
+ aztec::messages::message_delivery::MessageDelivery;
```

Then update the emissions:

```
- storage.balances.at(from).sub(from, amount).emit(encode_and_encrypt_note(&mut context, from));  
+ storage.balances.at(from).sub(from, amount).emit(&mut context, from, MessageDelivery.CONSTRAINED_ONCHAIN);
```

```
- storage.balances.at(from).add(from, change).emit(encode_and_encrypt_note_unconstrained(&mut context, from));  
+ storage.balances.at(from).add(from, change).emit(&mut context, from, MessageDelivery.UNCONSTRAINED_ONCHAIN);
```

```
- storage.balances.at(owner).insert(note).emit(encode_and_encrypt_note_and_emit_as_offchain_message(&mut context, context.msg_sender());  
+ storage.balances.at(owner).insert(note).emit(&mut context, context.msg_sender(), MessageDelivery.UNCONSTRAINED_OFFCHAIN);
```

## 2.0.2## [Public functions]The L2 gas cost of the different AVM opcodes have been updated to reflect more realistic proving costs. Developers should review the L2 gas costs of executing public functions and reevaluate any hardcoded L2 gas limits.

## [Aztec Tools]## Contract compilation now requires two stepsThe `aztec-nargo` command is now a direct pass-through to vanilla nargo, without any special compilation flags or postprocessing. Contract compilation for Aztec now requires two explicit steps:

1. Compile your contracts with `aztec-nargo compile`
2. Run postprocessing with the new `aztec-postprocess-contract` command

The postprocessing step includes:

* Transpiling functions for the Aztec VM
* Generating verification keys for private functions
* Caching verification keys for faster subsequent compilations

Update your build scripts accordingly:

```
- aztec-nargo compile  
+ aztec-nargo compile  
+ aztec-postprocess-contract
```

If you're using the `aztec-up` installer, the `aztec-postprocess-contract` command will be automatically installed alongside `aztec-nargo`.

## [Aztec.js] Mandatory `from`As we prepare for a bigger `Wallet` interface refactor and the upcoming `WalletSDK`, a new parameter has been added to contract interactions, which now should indicate *explicitly* the address of the entrypoint (usually the account contract) that will be used to authenticate the request. This will be checked in runtime against the current `this.wallet.getAddress()` value, to ensure consistent behavior while the rest of the API is reworked.

```
- await contract.methods.my_func(arg).send().wait();  
+ await contract.methods.my_func(arg).send({ from: account1Address }).wait();
```

## [Aztec.nr]## `emit_event_in_public_log` function renamed as `emit_event_in_public`This change was done to make the naming consistent with the private counterpart (`emit_event_in_private`).

## Private event emission API changesThe private event emission API has been significantly reworked to provide clearer semantics around message delivery guarantees. The key changes are:

1. `emit_event_in_private_log` has been renamed to `emit_event_in_private` and now takes a `delivery_mode` parameter instead of `constraints`
2. `emit_event_as_offchain_message` has been removed in favor of using `emit_event_in_private` with `MessageDelivery.UNCONSTRAINED_OFFCHAIN`
3. `PrivateLogContent` enum has been replaced with `MessageDelivery` enum with the following values:
   * `CONSTRAINED_ONCHAIN`: For onchain delivery with cryptographic guarantees that recipients can discover and decrypt messages. Uses constrained encryption but is slower to prove. Best for critical messages that contracts need to verify.
   * `UNCONSTRAINED_ONCHAIN`: For onchain delivery without encryption constraints. Faster proving but trusts the sender. Good when the sender is incentivized to perform encryption correctly (e.g. they are buying something and will only get it if the recipient sees the note). No guarantees that recipients will be able to find or decrypt messages.
   * `UNCONSTRAINED_OFFCHAIN`: For offchain delivery (e.g. cloud storage) without constraints. Lowest cost since no onchain storage needed. Requires custom infrastructure for delivery. No guarantees that messages will be delivered or that recipients will ever find them.

## Contract functions can no longer be `pub` or `pub(crate)`With the latest changes to `TestEnvironment`, making contract functions have public visibility is no longer required given the new `call_public` and `simulate_utility` functions. To avoid accidental direct invocation, and to reduce confusion with the autogenerated interfaces, we're forbidding them being public.

```
- pub(crate) fn balance_of_private(account: AztecAddress) -> 128 {  
+ fn balance_of_private(account: AztecAddress) -> 128 {
```

## Notes require you to manually implement or derive PackableWe have decided to drop auto-derivation of `Packable` from the `#[note]` macro because we want to make the macros less magical.
With this change you will be forced to either apply `#[derive(Packable)` on your notes:

```
+use aztec::protocol_types::traits::Packable;  
  
+#[derive(Packable)]  
#[note]  
pub struct UintNote {  
    owner: AztecAddress,  
    randomness: Field,  
    value: u128,  
}
```

or to implement it manually yourself:

```
impl Packable for UintNote {  
    let N: u32 = 3;  
  
    fn pack(self) -> [Field; Self::N] {  
        [self.owner.to_field(), randomness, value as Field]  
    }  
  
    fn unpack(fields: [Field; Self::N]) -> Self {  
        let owner = AztecAddress::from_field(fields[0]);  
        let randomness = fields[1];  
        let value = fields[2] as u128;  
        UintNote { owner, randomness, value }  
    }  
}
```

## Tagging sender now managed via oracle functionsNow, instead of manually needing to pass a tagging sender as an argument to log emission functions (e.g. `encode_and_encrypt_note`, `encode_and_encrypt_note_unconstrained`, `emit_event_in_private_log`, ...) we automatically load the sender via the `get_sender_for_tags()` oracle.
This value is expected to be populated by account contracts that should call `set_sender_for_tags()` in their entry point functions.

The changes you need to do in your contracts are quite straightforward.
You simply need to drop the `sender` arg from the callsites of the log emission functions.
E.g. note emission:

```
storage.balances.at(from).sub(from, amount).emit(encode_and_encrypt_note(  
    &mut context,  
    from,  
-    tagging_sender,  
));
```

E.g. private event emission:

```
emit_event_in_private_log(  
    Transfer { from, to, amount },  
    &mut context,  
-    tagging_sender,  
    to,  
    PrivateLogContent.NO_CONSTRAINTS,  
);
```

This change affected arguments `prepare_private_balance_increase` and `mint_to_private` functions on the `Token` contract.
Drop the `from` argument when calling these.

Example n TypeScript test:

```
- await token.methods.mint_to_private(fundedWallet.getAddress(), alice, mintAmount).send().wait();  
+ await token.methods.mint_to_private(alice, mintAmount).send().wait();
```

Example when

```
let token_out_partial_note = Token::at(token_out).prepare_private_balance_increase(  
    sender,  
-    tagging_sender  
).call(&mut context);
```

## SharedMutable -> DelayedPublicMutableThe `SharedMutable` state variable has been renamed to `DelayedPublicMutable`. It is a public mutable with a delay before state changes take effect. It can be read in private during the delay period. The name "shared" confuses developers who actually wish to work with so-called "shared private state". Also, we're working on a `DelayedPrivateMutable` which will have similar properties, except writes will be scheduled from private instead. With this new state variable in mind, the new name works nicely.

## [TXE] - Testing Aztec Contracts using Noir## Full `TestEnvironment` API overhaulAs part of a broader effort to make Noir tests that leverage TXE easier to use and reason about, large parts of it were changed or adapted, resulting in the API now being quite different. No functionality was lost, so it should be possible to migrate any older Noir test to use the new API.

## Network State Manipulation* `committed_timestamp` removed: this function did not work correctly
* `private_at_timestamp`: this function was not really meaningful: private contexts are built from block numbers, not timestamps
* `pending_block_number` was renamed to `next_block_number`. `pending_timestamp` was removed since it was confusing and not useful
* `committed_block_number` was renamed to `last_block_number`
* `advance_timestamp_to` and `advance_timestamp_by` were renamed to `set_next_block_timestamp` and `advance_next_block_timestamp_by` respectively
* `advance_block_to` was renamed to `mine_block_at`, which takes a timestamp instead of a target block number
* `advance_block_by` was renamed to `mine_block`, which now mines a single block

## Account Management* `create_account` was renamed to `create_light_account`
* `create_account_contract` was renamed to `create_contract_account`

## Contract Deployment* `deploy_self` removed: merged into `deploy`
* `deploy` now accepts both local and external contracts

## Contract InteractionsThe old way of calling contract functions is gone. Contract functions are now invoked via the `call_private`, `view_private`, `call_public`, `view_public` and `simulate_utility` `TestEnvironment` methods. These take a `CallInterface`, like their old counterparts, but now also take an explicit `from` parameter (for the `call` variants - this is left out of the `view` and `simulate` methods for simplicity).

## Raw Context AccessThe `private` and `public` methods are gone. Private, public and utility contexts can now be crated with the `private_context`, `public_context` and `utility_context` functions, all of which takes a callback function that is called with the corresponding context. This functions are expected to be defined in-line as lambdas, and contain the user-defined test logic. This helps delineate where contexts begin and end. Contexts automatically mine blocks on closing, when appropriate.

## Error-expecting Functions`assert_public_call_revert` and variants have been removed. Use `#[test(should_fail_with = "message")]` instead.

## Example MigrationThe following are two tests using the older version of `TestEnvironment`:

```
#[test]  
unconstrained fn initial_empty_value() {  
    let mut env = TestEnvironment::new();  
  
    // Setup without account contracts. We are not using authwits here, so dummy accounts are enough  
    let admin = env.create_account(1);  
  
    let initializer_call_interface = Auth::interface().constructor(admin);  
  
    let auth_contract =  
        env.deploy_self("Auth").with_public_void_initializer(admin, initializer_call_interface);  
    let auth_contract_address = auth_contract.to_address();  
  
    env.impersonate(admin);  
    let authorized = Auth::at(auth_contract_address).get_authorized().view(&mut env.public());  
    assert_eq(authorized, AztecAddress::from_field(0));  
}  
  
#[test]  
unconstrained fn non_admin_cannot_set_authorized() {  
    let mut env = TestEnvironment::new();  
  
    // Setup without account contracts. We are not using authwits here, so dummy accounts are enough  
    let admin = env.create_account(1);  
    let other = env.create_account(2);  
  
    let initializer_call_interface = Auth::interface().constructor(admin);  
  
    let auth_contract =  
        env.deploy_self("Auth").with_public_void_initializer(admin, initializer_call_interface);  
    let auth_contract_address = auth_contract.to_address();  
  
    env.impersonate(other);  
    env.assert_public_call_fails(Auth::at(auth_contract_address).set_authorized(to_authorize));  
}
```

These now look like this:

```
#[test]  
unconstrained fn authorized_initially_unset() {  
    let mut env = TestEnvironment::new();  
  
    let admin = env.create_light_account(); // Manual secret management gone  
  
    let auth_contract_address =  
        env.deploy("Auth").with_public_initializer(admin, Auth::interface().constructor(admin)); // deploy_self replaced  
    let auth = Auth::at(auth_contract_address);  
  
    assert_eq(env.view_public(auth.get_authorized()), AztecAddress::zero()); // .view_public() instead of .public()  
}  
  
#[test(should_fail_with = "caller is not admin")]  
unconstrained fn non_admin_cannot_set_unauthorized() {  
    let mut env = TestEnvironment::new();  
  
    let admin = env.create_light_account();  
    let other = env.create_light_account();  
  
    let auth_contract_address =  
        env.deploy("Auth").with_public_initializer(admin, Auth::interface().constructor(admin)); // deploy_self replaced  
    let auth = Auth::at(auth_contract_address);  
  
    env.call_public(other, auth.set_authorized(other)); // .call_public(), should_fail_with  
}
```

## [Aztec.js]## CheatcodesCheatcodes where moved out of the `@aztec/aztec.js` package to `@aztec/ethereum` and `@aztec/aztec` packages.
While all of the cheatcodes can be imported from the `@aztec/aztec` package `EthCheatCodes` and `RollupCheatCodes` reside in `@aztec/ethereum` package and if you need only those importing only that package should result in a lighter build.

## Note exports dropped from artifactNotes are no longer exported in the contract artifact.
Exporting notes was technical debt from when we needed to interpret notes in TypeScript.

The following code will no longer work since `notes` is no longer available on the artifact:

```
const valueNoteTypeId = StatefulTestContractArtifact.notes['ValueNote'].id;
```

## [core protocol, Aztec.nr, Aztec.js] Max block number property changed to be seconds based## `max_block_number` -> `include_by_timestamp`The transaction expiration mechanism has been updated to use seconds rather than number of blocks.
As part of this change, the transaction property `max_block_number` has been renamed to `include_by_timestamp`.

This change significantly impacts the `SharedMutable` state variable in `Aztec.nr`, which now operates on a seconds instead of number of blocks.
If your contract uses `SharedMutable`, you'll need to:

1. Update the `INITIAL_DELAY` numeric generic to use seconds instead of blocks
2. Modify any related logic to account for timestamp-based timing
3. Note that timestamps use `u64` values while block numbers use `u32`

## Removed `prelude`, so your `dep::aztec::prelude::...` imports will need to be amended.Instead of importing common types from `dep::aztec::prelude...`, you'll now need to import them from their lower-level locations.
The Noir Language Server vscode extension is now capable of autocompleting imports: just type some of the import and press 'tab' when it pops up with the correct item, and the import will be inserted at the top of the file.

As a quick reference, here are the paths to the types that were previously in the `prelude`.
So, for example, if you were previously using `dep::aztec::prelude::AztecAddress`, you'll need to replace it with `dep::aztec::protocol_types::address::AztecAddress`.
Apologies for any pain this brings. The reasoning is that these types were somewhat arbitrary, and it was unclear which types were worthy enough to be included here.

```
use dep::aztec::{  
    context::{PrivateCallInterface, PrivateContext, PublicContext, UtilityContext, ReturnsHash},  
    note::{  
        note_getter_options::NoteGetterOptions,  
        note_interface::{NoteHash, NoteType},  
        note_viewer_options::NoteViewerOptions,  
        retrieved_note::RetrievedNote,  
    },  
    state_vars::{  
        map::Map, private_immutable::PrivateImmutable, private_mutable::PrivateMutable,  
        private_set::PrivateSet, public_immutable::PublicImmutable, public_mutable::PublicMutable,  
        shared_mutable::SharedMutable,  
    },  
};  
  
use dep::aztec::protocol_types::{  
    abis::function_selector::FunctionSelector,  
    address::{AztecAddress, EthAddress},  
    point::Point,  
    traits::{Deserialize, Serialize},  
};
```

## `include_by_timestamp` is now mandatoryEach transaction must now include a valid `include_by_timestamp` that satisfies the following conditions:

* It must be greater than the historical block ’s timestamp.
* The duration between the `include_by_timestamp` and the historical block’s timestamp must not exceed the maximum allowed (currently 24 hours).
* It must be greater than or equal to the timestamp of the block in which the transaction is included.

The protocol circuits compute the `include_by_timestamp` for contract updates during each private function iteration. If a contract does not explicitly specify a value, the default will be the maximum allowed duration. This ensures that `include_by_timestamp` is never left unset.

No client-side changes are required. However, please note that transactions now have a maximum lifespan of 24 hours and will be removed from the transaction pool once expired.

## 0.88.0## [Aztec.nr] Deprecation of the `authwit` libraryIt is now included in `aztec-nr`, so imports must be updated:

```
-dep::authwit::...  
+dep::aztec::authwit...
```

and stale dependencies removed from `Nargo.toml`

```
-authwit = { path = "../../../../aztec-nr/authwit" }
```

## 0.87.0## [Aztec.js/TS libraries]We've bumped our minimum supported node version to v20, as v18 is now EOL. As a consequence, the deprecated type assertion syntax has been replaced with modern import attributes whenever contract artifact JSONs are loaded:

```
-import ArtifactJson from '../artifacts/contract-Contract.json' assert { type: 'json' };  
+import ArtifactJson from '../artifacts/contract-Contract.json' with { type: 'json' };
```

## [Aztec.js/PXE] `simulateUtility` return type`pxe.simulateUtility()` now returns a complex object (much like `.simulateTx()`) so extra information can be provided such as simulation timings.

This information can be accessed setting the `includeMetadata` flag in `SimulateMethodOptions` to `true`, but not providing it (which is the default) will NOT change the behavior of the current code.

```
-const result = await pxe.simulateUtility(...);  
+const { meta, result } = await pxe.simulateUtility(...);  
  
const result = await Contract.methods.myFunction(...).simulate();  
const { result, meta} = await Contract.methods.myFunction(...).simulate({ includeMetadata: true });
```

## [Aztec.js] Removed mandatory simulation before proving in contract interfacesPreviously, our autogenerated contract classes would perform a simulation when calling `.prove` or `.send` on them. This could potentially catch errors earlier, but took away control from the app/wallets on how to handle network interactions. Now this process has to be triggered manually, which means just proving an interaction (or proving and sending it to the network in one go via `.send`) is much faster.

*WARNING:* This means users can incurr in network fees if a transaction that would otherwise be invalid is sent without sanity checks. To ensure this, it is recommended to do:

```
+await Contract.method.simulate();  
await Contract.method.send().wait();
```

## 0.86.0## [PXE] Removed PXE\_L2\_STARTING\_BLOCK environment variablePXE now fast-syncs by skipping finalized blocks and never downloads all blocks, so there is no longer a need to specify a starting block.

## [Aztec.nr] Logs and messages renamingThe following renamings have taken place:

* `encrypted_logs` to `messages`: this module now handles much more than just encrypted logs (including unconstrained message delivery, message encoding, etc.)
* `log_assembly_strategies` to `logs`
* `discovery` moved to `messages`: given that what is discovered are messages
* `default_aes128` removed

Most contracts barely used these modules, the only frequent imports are the `encode_and_encrypt` functions:

```
- use dep::aztec::messages::logs::note::encode_and_encrypt_note;  
+ use dep::aztec::messages::logs::note::encode_and_encrypt_note;
```

## [noir-contracts] Reference Noir contracts directory structure change`noir-projects/noir-contracts/contracts` directory became too cluttered so we grouped contracts into `account`, `app`, `docs`, `fees`, `libs`, `protocol` and `test` dirs.
If you import contract from the directory make sure to update the paths accordingly.
E.g. for a token contract:

```
#[dependencies]  
-token = { git = "https://github.com/AztecProtocol/aztec-packages/", tag = "v0.83.0", directory = "noir-projects/noir-contracts/contracts/src/token_contract" }  
+token = { git = "https://github.com/AztecProtocol/aztec-packages/", tag = "v0.83.0", directory = "noir-projects/noir-contracts/contracts/app/src/token_contract" }
```

## [Aztec.nr] #[utility] contract functionsAztec contracts have three kinds of functions: `#[private]`, `#[public]` and what was sometimes called 'top-level unconstrained': an unmarked unconstrained function in the contract module. These are now called `[#utility]` functions, and must be explicitly marked as such:

```
+    #[utility]  
    unconstrained fn balance_of_private(owner: AztecAddress) -> u128 {  
        storage.balances.at(owner).balance_of()  
    }
```

Utility functions are standalone unconstrained functions that cannot be called from private or public functions: they are meant to be called by *applications* to perform auxiliary tasks: query contract state (e.g. a token balance), process messages received offchain, etc.

All functions in a `contract` block must now be marked as one of either `#[private]`, `#[public]`, `#[utility]`, `#[contract_library_method]`, or `#[test]`.

Additionally, the `UnconstrainedContext` type has been renamed to `UtilityContext`. This led us to rename the `unkonstrained` method on `TestEnvironment` to `utility`, so any tests using it also need updating:

```
-     SharedMutable::new(env.unkonstrained(), storage_slot)  
+     SharedMutable::new(env.utility(), storage_slot)
```

## [AuthRegistry] function name changeAs part of the broader transition from "top-level unconstrained" to "utility" name (detailed in the note above), the `unconstrained_is_consumable` function in AuthRegistry has been renamed to `utility_is_consumable`. The function's signature and behavior remain unchanged - only the name has been updated to align with the new convention. If you're currently using this function, a simple rename in your code will suffice.

## 0.83.0## [aztec.js] AztecNode.getPrivateEvents API changeThe `getPrivateEvents` method signature has changed to require an address of a contract that emitted the event and use recipient addresses instead of viewing public keys:

```
- const events = await wallet.getPrivateEvents<Transfer>(TokenContract.events.Transfer, 1, 1, [recipient.getCompleteAddress().publicKeys.masterIncomingViewingPublicKey()]);  
+ const events = await wallet.getPrivateEvents<Transfer>(token.address, TokenContract.events.Transfer, 1, 1, [recipient.getAddress()]);
```

## [portal contracts] Versions and Non-following message boxesThe version number is no longer hard-coded to be `1` across all deployments (it not depends on where it is deployed to and with what genesis and logic).
This means that if your portal were hard-coding `1` it will now fail when inserting into the `inbox` or consuming from the `outbox` because of a version mismatch.
Instead you can get the real version (which don't change for a deployment) by reading the `VERSION` on inbox and outbox, or using `getVersion()` on the rollup.

New Deployments of the protocol do not preserve former state/across each other.
This means that after a new deployment, any "portal" following the registry would try to send messages into this empty rollup to non-existant contracts.
To solve, the portal should be linked to a specific deployment, e.g., a specific inbox.
This can be done by storing the inbox/outbox/version at the time of deployment or initialize and not update them.

Both of these issues were in the token portal and the uniswap portal, so if you used them as a template it is very likely that you will also have it.

## 0.82.0## [aztec.js] AztecNode.findLeavesIndexes returns indexes with block metadataIt's common that we need block metadata of a block in which leaves where inserted when querying indexes of these tree leaves.
For this reason we now return that information along with the indexes.
This allows us to reduce the number of individual AztecNode queries.

Along this change `findNullifiersIndexesWithBlock` and `findBlockNumbersForIndexes` functions wer removed as all its uses can now be replaced with the newly modified `findLeavesIndexes` function.

## [aztec.js] AztecNode.getPublicDataTreeWitness renamed as AztecNode.getPublicDataWitnessThis change was done to have consistent naming across codebase.

## [aztec.js] Wallet interface and Authwit managementThe `Wallet` interface in `aztec.js` is undergoing transformations, trying to be friendlier to wallet builders and reducing the surface of its API. This means `Wallet` no longer extends `PXE`, and instead just implements a subset of the methods of the former. This is NOT going to be its final form, but paves the way towards better interfaces and starts to clarify what the responsibilities of the wallet are:

```
/**  
 * The wallet interface.  
 */  
export type Wallet = AccountInterface &  
  Pick<  
    PXE,  
    // Simulation  
    | "simulateTx"  
    | "simulateUnconstrained"  
    | "profileTx"  
    // Sending  
    | "sendTx"  
    // Contract management (will probably be collapsed in the future to avoid instance and class versions)  
    | "getContractClassMetadata"  
    | "getContractMetadata"  
    | "registerContract"  
    | "registerContractClass"  
    // Likely to be removed  
    | "proveTx"  
    // Will probably be collapsed  
    | "getNodeInfo"  
    | "getPXEInfo"  
    // Fee info  
    | "getCurrentBaseFees"  
    // Still undecided, kept for the time being  
    | "updateContract"  
    // Sender management  
    | "registerSender"  
    | "getSenders"  
    | "removeSender"  
    // Tx status  
    | "getTxReceipt"  
    // Events. Kept since events are going to be reworked and changes will come when that's done  
    | "getPrivateEvents"  
    | "getPublicEvents"  
  > & {  
    createAuthWit(intent: IntentInnerHash | IntentAction): Promise<AuthWitness>;  
  };
```

As a side effect, a few debug only features have been removed

```
// Obtain tx effects  
const { txHash, debugInfo } = await contract.methods  
      .set_constant(value)  
      .send()  
--      .wait({ interval: 0.1, debug: true });  
++      .wait({ interval: 0.1 })  
  
--    // check that 1 note hash was created  
--    expect(debugInfo!.noteHashes.length).toBe(1);  
++    const txEffect = await aztecNode.getTxEffect(txHash);  
++    const noteHashes = txEffect?.data.noteHashes;  
++    // check that 1 note hash was created  
++    expect(noteHashes?.length).toBe(1);  
  
// Wait for a tx to be proven  
--      tx.wait({ timeout: 300, interval: 10, proven: true, provenTimeout: 3000 })));  
++      const receipt = await tx.wait({ timeout: 300, interval: 10 });  
++      await waitForProven(aztecNode, receipt, { provenTimeout: 3000 });
```

Authwit management has changed, and PXE no longer stores them. This is unnecessary because now they can be externally provided to simulations and transactions, making sure no stale authorizations are kept inside PXE's db.

```
const witness = await wallet.createAuthWit({ caller, action });  
--await callerWallet.addAuthWitness(witness);  
--await action.send().wait();  
++await action.send({ authWitnesses: [witness] }).wait();
```

Another side effect of this is that the interface of the `lookupValidity` method has changed, and now the authwitness has to be provided:

```
const witness = await wallet.createAuthWit({ caller, action });  
--await callerWallet.addAuthWitness(witness);  
--await wallet.lookupValidity(wallet.getAddress(), { caller, action });  
++await wallet.lookupValidity(wallet.getAddress(), { caller, action }, witness);
```

## 0.80.0## [PXE] Concurrent contract function simulation disabledPXE is no longer be able to execute contract functions concurrently (e.g. by collecting calls to `simulateTx` and then using `await Promise.all`). They will instead be put in a job queue and executed sequentially in order of arrival.

## 0.79.0## [aztec.js] Changes to `BatchCall` and `BaseContractInteraction`The constructor arguments of `BatchCall` have been updated to improve usability. Previously, it accepted an array of `FunctionCall`, requiring users to manually set additional data such as `authwit` and `capsules`. Now, `BatchCall` takes an array of `BaseContractInteraction`, which encapsulates all necessary information.

```
class BatchCall extends BaseContractInteraction {  
-    constructor(wallet: Wallet, protected calls: FunctionCall[]) {  
+    constructor(wallet: Wallet, protected calls: BaseContractInteraction[]) {  
        ...  
    }
```

The `request` method of `BaseContractInteraction` now returns `ExecutionPayload`. This object includes all the necessary data to execute one or more functions. `BatchCall` invokes this method on all interactions to aggregate the required information. It is also used internally in simulations for fee estimation.

Declaring a `BatchCall`:

```
new BatchCall(wallet, [  
-    await token.methods.transfer(alice, amount).request(),  
-    await token.methods.transfer_to_private(bob, amount).request(),  
+    token.methods.transfer(alice, amount),  
+    token.methods.transfer_to_private(bob, amount),  
])
```

## 0.77.0## [aztec-nr] `TestEnvironment::block_number()` refactoredThe `block_number` function from `TestEnvironment` has been expanded upon with two extra functions, the first being `pending_block_number`, and the second being `committed_block_number`. `pending_block_number` now returns what `block_number` does. In other words, it returns the block number of the block we are currently building. `committed_block_number` returns the block number of the last committed block, i.e. the block number that gets used to execute the private part of transactions when your PXE is successfully synced to the tip of the chain.

```
+    `TestEnvironment::pending_block_number()`  
+    `TestEnvironment::committed_block_number()`
```

## [aztec-nr] `compute_nullifier_without_context` renamedThe `compute_nullifier_without_context` function from `NoteHash` (ex `NoteInterface`) is now called `compute_nullifier_unconstrained`, and instead of taking storage slot, contract address and nonce it takes a note hash for nullification (same as `compute_note_hash`). This makes writing this
function simpler:

```
-    unconstrained fn compute_nullifier_without_context(self, storage_slot: Field, contract_address: AztecAddress, nonce: Field) -> Field {  
-       let note_hash_for_nullify = ...;  
+    unconstrained fn compute_nullifier_unconstrained(self, note_hash_for_nullify: Field) -> Field {  
        ...  
    }
```

## `U128` type replaced with native `u128`The `U128` type has been replaced with the native `u128` type. This means that you can no longer use the `U128` type in your code. Instead, you should use the `u128` type.
Doing the changes is as straightforward as:

```
    #[public]  
    #[view]  
-    fn balance_of_public(owner: AztecAddress) -> U128 {  
+    fn balance_of_public(owner: AztecAddress) -> u128 {  
        storage.public_balances.at(owner).read()  
    }
```

`UintNote` has also been updated to use the native `u128` type.

## [aztec-nr] Removed `compute_note_hash_and_optionally_a_nullifer`This function is no longer mandatory for contracts, and the `#[aztec]` macro no longer injects it.

## [PXE] Removed `addNote` and `addNullifiedNote`These functions have been removed from PXE and the base `Wallet` interface. If you need to deliver a note manually because its creation is not being broadcast in an encrypted log, then create an unconstrained contract function to process it and simulate execution of it. The `aztec::discovery::private_logs::do_process_log` function can be used to perform note discovery and add to it to PXE.

See an example of how to handle a `TransparentNote`:

```
    unconstrained fn deliver_transparent_note(  
        contract_address: AztecAddress,  
        amount: Field,  
        secret_hash: Field,  
        tx_hash: Field,  
        unique_note_hashes_in_tx: BoundedVec<Field, MAX_NOTE_HASHES_PER_TX>,  
        first_nullifier_in_tx: Field,  
        recipient: AztecAddress,  
    ) {  
        // do_process_log expects a standard aztec-nr encoded note, which has the following shape:  
        // [ storage_slot, note_type_id, ...packed_note ]  
        let note = TransparentNote::new(amount, secret_hash);  
        let log_plaintext = BoundedVec::from_array(array_concat(  
            [  
                MyContract::storage_layout().my_state_variable.slot,  
                TransparentNote::get_note_type_id(),  
            ],  
            note.pack(),  
        ));  
  
        do_process_log(  
            contract_address,  
            log_plaintext,  
            tx_hash,  
            unique_note_hashes_in_tx,  
            first_nullifier_in_tx,  
            recipient,  
            _compute_note_hash_and_nullifier,  
        );  
    }
```

The note is then processed by calling this function:

```
const txEffects = await wallet.getTxEffect(txHash);  
await contract.methods  
  .deliver_transparent_note(  
    contract.address,  
    new Fr(amount),  
    secretHash,  
    txHash.hash,  
    toBoundedVec(txEffects!.data.noteHashes, MAX_NOTE_HASHES_PER_TX),  
    txEffects!.data.nullifiers[0],  
    wallet.getAddress()  
  )  
  .simulate();
```

## Fee is mandatoryAll transactions must now pay fees. Previously, the default payment method was `NoFeePaymentMethod`; It has been changed to `FeeJuicePaymentMethod`, with the wallet owner as the fee payer.

For example, the following code will still work:

```
await TokenContract.at(address, wallet).methods.transfer(recipient, 100n).send().wait();
```

However, the wallet owner must have enough fee juice to cover the transaction fee. Otherwise, the transaction will be rejected.

The 3 test accounts deployed in the sandbox are pre-funded with 10 ^ 22 fee juice, allowing them to send transactions right away.

In addition to the native fee juice, users can pay the transaction fees using tokens that have a corresponding FPC contract. The sandbox now includes `BananaCoin` and `BananaFPC`. Users can use a funded test account to mint banana coin for a new account. The new account can then start sending transactions and pay fees with banana coin.

```
import { getDeployedTestAccountsWallets } from "@aztec/accounts/testing";  
import {  
  getDeployedBananaCoinAddress,  
  getDeployedBananaFPCAddress,  
} from "@aztec/aztec";  
  
// Fetch the funded test accounts.  
const [fundedWallet] = await getDeployedTestAccountsWallets(pxe);  
  
// Create a new account.  
const secret = Fr.random();  
const signingKey = GrumpkinScalar.random();  
const alice = await getSchnorrAccount(pxe, secret, signingKey);  
const aliceWallet = await alice.getWallet();  
const aliceAddress = alice.getAddress();  
  
// Deploy the new account using the pre-funded test account.  
await alice.deploy({ deployWallet: fundedWallet }).wait();  
  
// Mint banana coin for the new account.  
const bananaCoinAddress = await getDeployedBananaCoinAddress(pxe);  
const bananaCoin = await TokenContract.at(bananaCoinAddress, fundedWallet);  
const mintAmount = 10n ** 20n;  
await bananaCoin.methods  
  .mint_to_private(fundedWallet.getAddress(), aliceAddress, mintAmount)  
  .send()  
  .wait();  
  
// Use the new account to send a tx and pay with banana coin.  
const transferAmount = 100n;  
const bananaFPCAddress = await getDeployedBananaFPCAddress(pxe);  
const paymentMethod = new PrivateFeePaymentMethod(  
  bananaFPCAddress,  
  aliceWallet  
);  
const receipt = await bananaCoin  
  .withWallet(aliceWallet)  
  .methods.transfer(recipient, transferAmount)  
  .send({ fee: { paymentMethod } })  
  .wait();  
const transactionFee = receipt.transactionFee!;  
  
// Check the new account's balance.  
const aliceBalance = await bananaCoin.methods  
  .balance_of_private(aliceAddress)  
  .simulate();  
expect(aliceBalance).toEqual(mintAmount - transferAmount - transactionFee);
```

## The tree of protocol contract addresses is now an indexed treeThis is to allow for non-membership proofs for non-protocol contract addresses. As before, the canonical protocol contract addresses point to the index of the leaf of the 'real' computed protocol address.

For example, the canonical `DEPLOYER_CONTRACT_ADDRESS` is a constant `= 2`. This is used in the kernels as the `contract_address`. We calculate the `computed_address` (currently `0x1665c5fbc1e58ba19c82f64c0402d29e8bbf94b1fde1a056280d081c15b0dac1`) and check that this value exists in the indexed tree at index `2`. This check already existed and ensures that the call cannot do 'special' protocol contract things unless it is a real protocol contract.

The new check an indexed tree allows is non-membership of addresses of non protocol contracts. This ensures that if a call is from a protocol contract, it must use the canonical address. For example, before this check a call could be from the deployer contract and use `0x1665c5fbc1e58ba19c82f64c0402d29e8bbf94b1fde1a056280d081c15b0dac1` as the `contract_address`, but be incorrectly treated as a 'normal' call.

```
- let computed_protocol_contract_tree_root = if is_protocol_contract {  
-     0  
- } else {  
-     root_from_sibling_path(  
-         computed_address.to_field(),  
-         protocol_contract_index,  
-         private_call_data.protocol_contract_sibling_path,  
-     )  
- };  
  
+ conditionally_assert_check_membership(  
+     computed_address.to_field(),  
+     is_protocol_contract,  
+     private_call_data.protocol_contract_leaf,  
+     private_call_data.protocol_contract_membership_witness,  
+     protocol_contract_tree_root,  
+ );
```

## [Aztec.nr] Changes to note interfaces and note macrosIn this releases we decided to do a large refactor of notes which resulted in the following changes:

1. We removed `NoteHeader` and we've introduced a `RetrievedNote` struct that contains a note and the information originally stored in the `NoteHeader`.
2. We removed the `pack_content` and `unpack_content` functions from the `NoteInterface`and made notes implement the standard `Packable` trait.
3. We renamed the `NullifiableNote` trait to `NoteHash` and we've moved the `compute_note_hash` function to this trait from the `NoteInterface` trait.
4. We renamed `NoteInterface` trait as `NoteType` and `get_note_type_id` function as `get_id`.
5. The `#[note]` and `#[partial_note]` macros now generate both the `NoteType` and `NoteHash` traits.
6. `#[custom_note_interface]` macro has been renamed to `#[custom_note]` and it now implements the `NoteInterface` trait.

This led us to do the following changes to the interfaces:

```
-pub trait NoteInterface<let N: u32> {  
+pub trait NoteType {  
    fn get_id() -> Field;  
-    fn pack_content(self) -> [Field; N];  
-    fn unpack_content(fields: [Field; N]) -> Self;  
-    fn get_header(self) -> NoteHeader;  
-    fn set_header(&mut self, header: NoteHeader) -> ();  
-    fn compute_note_hash(self) -> Field;  
}  
  
pub trait NoteHash {  
+    fn compute_note_hash(self, storage_slot: Field) -> Field;  
  
    fn compute_nullifier(self, context: &mut PrivateContext, note_hash_for_nullify: Field) -> Field;  
  
-    unconstrained fn compute_nullifier_without_context(self) -> Field;  
+    unconstrained fn fn compute_nullifier_without_context(self, storage_slot: Field, contract_address: AztecAddress, note_nonce: Field) -> Field;  
}
```

If you are using `#[note]` or `#[partial_note(...)]` macros you will need to delete the implementations of the `NullifiableNote` (now `NoteHash`) trait as it now gets auto-generated.
Your note will also need to have an `owner` (a note struct field called owner) as its used in the auto-generated nullifier functions.

If you need a custom implementation of the `NoteHash` interface use the `#[custom_note]` macro.

If you used `#[note_custom_interface]` macro before you will need to update your notes by using the `#[custom_note]` macro and implementing the `compute_note_hash` function.
If you have no need for a custom implementation of the `compute_note_hash` function copy the default one:

```
fn compute_note_hash(self, storage_slot: Field) -> Field {  
    let inputs = aztec::protocol_types::utils::arrays::array_concat(self.pack(), [storage_slot]);  
    aztec::protocol_types::hash::poseidon2_hash_with_separator(inputs, aztec::protocol_types::constants::GENERATOR_INDEX__NOTE_HASH)  
}
```

If you need to keep the custom implementation of the packing functionality, manually implement the `Packable` trait:

```
+ use dep::aztec::protocol_types::traits::Packable;  
  
+impl Packable<N> for YourNote {  
+    fn pack(self) -> [Field; N] {  
+        ...  
+    }  
+  
+    fn unpack(fields: [Field; N]) -> Self {  
+        ...  
+    }  
+}
```

If you don't provide a custom implementation of the `Packable` trait, a default one will be generated.

## [Aztec.nr] Changes to state variablesSince we've removed `NoteHeader` from notes we no longer need to modify the header in the notes when working with state variables.
This means that we no longer need to be passing a mutable note reference which led to the following changes in the API.

## PrivateImmutableFor `PrivateImmutable` the changes are fairly straightforward.
Instead of passing in a mutable reference `&mut note` just pass in `note`.

```
impl<Note> PrivateImmutable<Note, &mut PrivateContext> {  
-    pub fn initialize<let N: u32>(self, note: &mut Note) -> NoteEmission<Note>  
+    pub fn initialize<let N: u32>(self, note: Note) -> NoteEmission<Note>  
    where  
        Note: NoteInterface<N> + NullifiableNote,  
    {  
        ...  
    }  
}
```

## PrivateSetFor `PrivateSet` the changes are a bit more involved than the changes in `PrivateImmutable`.
Instead of passing in a mutable reference `&mut note` to the `insert` function just pass in `note`.
The `remove` function now takes in a `RetrievedNote<Note>` instead of a `Note` and the `get_notes` function
now returns a vector `RetrievedNote`s instead of a vector `Note`s.
Note getters now generally return `RetrievedNote`s so getting a hold of the `RetrievedNote` for removal should be straightforward.

```
impl<Note, let N: u32> PrivateSet<Note, &mut PrivateContext>  
where  
    Note: NoteInterface<N> + NullifiableNote + Eq,  
{  
-    pub fn insert(self, note: &mut Note) -> NoteEmission<Note> {  
+    pub fn insert(self, note: Note) -> NoteEmission<Note> {  
        ...  
    }  
  
-    pub fn remove(self, note: Note) {  
+    pub fn remove(self, retrieved_note: RetrievedNote<Note>) {  
        ...  
    }  
  
    pub fn get_notes<PREPROCESSOR_ARGS, FILTER_ARGS>(  
        self,  
        options: NoteGetterOptions<Note, N, PREPROCESSOR_ARGS, FILTER_ARGS>,  
-    ) -> BoundedVec<Note, MAX_NOTE_HASH_READ_REQUESTS_PER_CALL> {  
+    ) -> BoundedVec<RetrievedNote<Note>, MAX_NOTE_HASH_READ_REQUESTS_PER_CALL> {  
        ...  
    }  
}  
  
- impl<Note, let N: u32> PrivateSet<Note, &mut PublicContext>  
- where  
-    Note: NoteInterface<N> + NullifiableNote,  
- {  
-    pub fn insert_from_public(self, note: &mut Note) {  
-        create_note_hash_from_public(self.context, self.storage_slot, note);  
-    }  
- }
```

## PrivateMutableFor `PrivateMutable` the changes are similar to the changes in `PrivateImmutable`.

```
impl<Note, let N: u32> PrivateMutable<Note, &mut PrivateContext>  
where  
    Note: NoteInterface<N> + NullifiableNote,  
{  
-    pub fn initialize(self, note: &mut Note) -> NoteEmission<Note> {  
+    pub fn initialize(self, note: Note) -> NoteEmission<Note> {  
        ...  
    }  
  
-    pub fn replace(self, new_note: &mut Note) -> NoteEmission<Note> {  
+    pub fn replace(self, new_note: Note) -> NoteEmission<Note> {  
        ...  
    }  
  
-    pub fn initialize_or_replace(self, note: &mut Note) -> NoteEmission<Note> {  
+    pub fn initialize_or_replace(self, note: Note) -> NoteEmission<Note> {  
        ...  
    }  
}
```

## 0.75.0## Changes to `TokenBridge` interface`get_token` and `get_portal_address` functions got merged into a single `get_config` function that returns a struct containing both the token and portal addresses.

## [Aztec.nr] `SharedMutable` can store size of packed length larger than 1`SharedMutable` has been modified such that now it can store type `T` which packs to a length larger than 1.
This is a breaking change because now `SharedMutable` requires `T` to implement `Packable` trait instead of `ToField` and `FromField` traits.

To implement the `Packable` trait for your type you can use the derive macro:

```
+ use std::meta::derive;  
  
+ #[derive(Packable)]  
pub struct YourType {  
    ...  
}
```

## [Aztec.nr] Introduction of `WithHash<T>``WithHash<T>` is a struct that allows for efficient reading of value `T` from public storage in private.
This is achieved by storing the value with its hash, then obtaining the values via an oracle and verifying them against the hash.
This results in in a fewer tree inclusion proofs for values `T` that are packed into more than a single field.

`WithHash<T>` is leveraged by state variables like `PublicImmutable`.
This is a breaking change because now we require values stored in `PublicImmutable` and `SharedMutable` to implement the `Eq` trait.

To implement the `Eq` trait you can use the `#[derive(Eq)]` macro:

```
+ use std::meta::derive;  
  
+ #[derive(Eq)]  
pub struct YourType {  
    ...  
}
```

## 0.73.0## [Token, FPC] Moving fee-related complexity from the Token to the FPCThere was a complexity leak of fee-related functionality in the token contract.
We've came up with a way how to achieve the same objective with the general functionality of the Token contract.
This lead to the removal of `setup_refund` and `complete_refund` functions from the Token contract and addition of `complete_refund` function to the FPC.

## [Aztec.nr] Improved storage slot allocationState variables are no longer assumed to be generic over a type that implements the `Serialize` trait: instead, they must implement the `Storage` trait with an `N` value equal to the number of slots they need to reserve.

For the vast majority of state variables, this simply means binding the serialization length to this trait:

```
+ impl<T, let N: u32> Storage<N> for MyStateVar<T> where T: Serialize<N> { };
```

## [Aztec.nr] Introduction of `Packable` traitWe have introduced a `Packable` trait that allows types to be serialized and deserialized with a focus on minimizing the size of the resulting Field array.
This is in contrast to the `Serialize` and `Deserialize` traits, which follows Noir's intrinsic serialization format.
This is a breaking change because we now require `Packable` trait implementation for any type that is to be stored in contract storage.

Example implementation of Packable trait for `U128` type from `noir::std`:

```
use crate::traits::{Packable, ToField};  
  
let U128_PACKED_LEN: u32 = 1;  
  
impl Packable<U128_PACKED_LEN> for U128 {  
    fn pack(self) -> [Field; U128_PACKED_LEN] {  
        [self.to_field()]  
    }  
  
    fn unpack(fields: [Field; U128_PACKED_LEN]) -> Self {  
        U128::from_integer(fields[0])  
    }  
}
```

## Logs for notes, partial notes, and events have been refactored.We're preparing to make log assembly more customisable. These paths have changed.

```
- use dep::aztec::encrypted_logs::encrypted_note_emission::encode_and_encrypt_note,  
+ use dep::aztec::messages::logs::note::encode_and_encrypt_note,
```

And similar paths for `encode_and_encrypt_note_unconstrained`, and for events and partial notes.

The way in which logs are assembled in this "default\_aes128" strategy is has also changed. I repeat: **Encrypted log layouts have changed**. The corresponding typescript for note discovery has also been changed, but if you've rolled your own functions for parsing and decrypting logs, those will be broken by this change.

## `NoteInferface` and `EventInterface` no-longer have a `to_be_bytes` method.You can remove this method from any custom notes or events that you've implemented.

## [Aztec.nr] Packing notes resulting in changes in `NoteInterface`Note interface implementation generated by our macros now packs note content instead of serializing it
With this change notes are being less costly DA-wise to emit when some of the note struct members implements the `Packable` trait (this is typically the `UintNote` which represents `value` as `U128` that gets serialized as 2 fields but packed as 1).
This results in the following changes in the `NoteInterface`:

```
pub trait NoteInterface<let N: u32> {  
-    fn serialize_content(self) -> [Field; N];  
+    fn pack_content(self) -> [Field; N];  
  
-    fn deserialize_content(fields: [Field; N]) -> Self;  
+    fn unpack_content(fields: [Field; N]) -> Self;  
  
    fn get_header(self) -> NoteHeader;  
    fn set_header(&mut self, header: NoteHeader) -> ();  
    fn get_note_type_id() -> Field;  
    fn compute_note_hash(self) -> Field;  
}
```

## [PXE] Cleanup of Contract and ContractClass information getters```
- pxe.isContractInitialized  
- pxe.getContractInstance  
- pxe.isContractPubliclyDeployed  
+ pxe.getContractMetadata
```

have been merged into getContractMetadata

```
- pxe.getContractClass  
- pxe.isContractClassPubliclyRegistered  
- pxe.getContractArtifact  
+ pxe.getContractClassMetadata
```

These functions have been merged into `pxe.getContractMetadata` and `pxe.getContractClassMetadata`.

## 0.72.0## Some functions in `aztec.js` and `@aztec/accounts` are now asyncIn our efforts to make libraries more browser-friendly and providing with more bundling options for `bb.js` (like a non top-level-await version), some functions are being made async, in particular those that access our cryptographic functions.

```
- AztecAddress.random();  
+ await AztecAddress.random();  
  
- getSchnorrAccount();  
+ await getSchnorrAccount();
```

## Public logs replace unencrypted logsAny log emitted from public is now known as a public log, rather than an unencrypted log. This means methods relating to these logs have been renamed e.g. in the pxe, archiver, txe:

```
- getUnencryptedLogs(filter: LogFilter): Promise<GetUnencryptedLogsResponse>  
- getUnencryptedEvents<T>(eventMetadata: EventMetadataDefinition, from: number, limit: number): Promise<T[]>  
+ getPublicLogs(filter: LogFilter): Promise<GetPublicLogsResponse>  
+ getPublicEvents<T>(eventMetadata: EventMetadataDefinition, from: number, limit: number): Promise<T[]>
```

The context method in aztec.nr is now:

```
- context.emit_unencrypted_log(log)  
+ context.emit_public_log(log)
```

These logs were treated as bytes in the node and as hashes in the protocol circuits. Now, public logs are treated as fields everywhere:

```
- unencryptedLogs: UnencryptedTxL2Logs  
- unencrypted_logs_hashes: [ScopedLogHash; MAX_UNENCRYPTED_LOGS_PER_TX]  
+ publicLogs: PublicLog[]  
+ public_logs: [PublicLog; MAX_PUBLIC_LOGS_PER_TX]
```

A `PublicLog` contains the log (as an array of fields) and the app address.

This PR also renamed encrypted events to private events:

```
- getEncryptedEvents<T>(eventMetadata: EventMetadataDefinition, from: number, limit: number, vpks: Point[]): Promise<T[]>  
+ getPrivateEvents<T>(eventMetadata: EventMetadataDefinition, from: number, limit: number, vpks: Point[]): Promise<T[]>
```

## 0.70.0## [Aztec.nr] Removal of `getSiblingPath` oracleUse `getMembershipWitness` oracle instead that returns both the sibling path and index.

## 0.68.0## [archiver, node, pxe] Remove contract artifacts in node and archiver and store function names insteadContract artifacts were only in the archiver for debugging purposes. Instead function names are now (optionally) emitted
when registering contract classes

Function changes in the Node interface and Contract Data source interface:

```
- addContractArtifact(address: AztecAddress, artifact: ContractArtifact): Promise<void>;  
+ registerContractFunctionNames(address: AztecAddress, names: Record<string, string>): Promise<void>;
```

So now the PXE registers this when calling `registerContract()`

```
await this.node.registerContractFunctionNames(instance.address, functionNames);
```

Function changes in the Archiver

```
- addContractArtifact(address: AztecAddress, artifact: ContractArtifact)  
-  getContractArtifact(address: AztecAddress)  
+  registerContractFunctionNames(address: AztecAddress, names: Record<string, string>): Promise<void>
```

## [fees, fpc] Changes in setting up FPC as fee payer on AztecJS and method names in FPCOn AztecJS, setting up `PrivateFeePaymentMethod` and `PublicFeePaymentMethod` are now the same. The don't need to specify a sequencer address or which coin to pay in. The coins are set up in the FPC contract!

```
- paymentMethod: new PrivateFeePaymentMethod(bananaCoin.address,bananaFPC.address,aliceWallet,sequencerAddress),  
+ paymentMethod: new PrivateFeePaymentMethod(bananaFPC.address, aliceWallet),  
  
- paymentMethod: new PublicFeePaymentMethod(bananaCoin.address, bananaFPC.address, aliceWallet),  
+ paymentMethod: new PublicFeePaymentMethod(bananaFPC.address, aliceWallet),
```

Changes in `FeePaymentMethod` class in AztecJS

```
- getAsset(): AztecAddress;  
+ getAsset(): Promise<AztecAddress>;
```

Changes in the token contract:
FPC specific methods, `setup_refund()` and `complete_refund()` have minor args rename.

Changes in FPC contract:
Rename of args in all of FPC functions as FPC now stores the accepted token address and admin and making it clearer the amounts are corresponding to the accepted token and not fee juice.
Also created a public function `pull_funds()` for admin to clawback any money in the FPC

Expect more changes in FPC in the coming releases!

## Name change from `contact` to `sender` in PXE API`contact` has been deemed confusing because the name is too similar to `contract`.
For this reason we've decided to rename it:

```
- await pxe.registerContact(address);  
+ await pxe.registerSender(address);  
- await pxe.getContacts();  
+ await pxe.getSenders();  
- await pxe.removeContact(address);  
+ await pxe.removeSender(address);
```

## 0.67.1## Noir contracts package no longer exposes artifacts as default exportTo reduce loading times, the package `@aztec/noir-contracts.js` no longer exposes all artifacts as its default export. Instead, it exposes a `ContractNames` variable with the list of all contract names available. To import a given artifact, use the corresponding export, such as `@aztec/noir-contracts.js/FPC`.

## BlobsWe now publish the majority of DA in L1 blobs rather than calldata, with only contract class logs remaining as calldata. This replaces all code that touched the `txsEffectsHash`.
In the rollup circuits, instead of hashing each child circuit's `txsEffectsHash` to form a tree, we track tx effects by absorbing them into a sponge for blob data (hence the name: `spongeBlob`). This sponge is treated like the state trees in that we check each rollup circuit 'follows' the next:

```
- let txs_effects_hash = sha256_to_field(left.txs_effects_hash, right.txs_effects_hash);  
+ assert(left.end_sponge_blob.eq(right.start_sponge_blob));  
+ let start_sponge_blob = left.start_sponge_blob;  
+ let end_sponge_blob = right.end_sponge_blob;
```

This sponge is used in the block root circuit to confirm that an injected array of all `txEffects` does match those rolled up so far in the `spongeBlob`. Then, the `txEffects` array is used to construct and prove opening of the polynomial representing the blob commitment on L1 (this is done efficiently thanks to the Barycentric formula).
On L1, we publish the array as a blob and verify the above proof of opening. This confirms that the tx effects in the rollup circuit match the data in the blob:

```
- bytes32 txsEffectsHash = TxsDecoder.decode(_body);  
+ bytes32 blobHash = _validateBlob(blobInput);
```

Where `blobInput` contains the proof of opening and evaluation calculated in the block root rollup circuit. It is then stored and used as a public input to verifying the epoch proof.

## 0.67.0## L2 Gas limit of 6M enforced for public portion of TXA 12M limit was previously enforced per-enqueued-public-call. The protocol now enforces a stricter limit that the entire public portion of a transaction consumes at most 6,000,000 L2 gas.

## [aztec.nr] Renamed `Header` and associated helpersThe `Header` struct has been renamed to `BlockHeader`, and the `get_header()` family of functions have been similarly renamed to `get_block_header()`.

```
- let header = context.get_header_at(block_number);  
+ let header = context.get_block_header_at(block_number);
```

## Outgoing Events removedPreviously, every event which was emitted included:

* Incoming Header (to convey the app contract address to the recipient)
* Incoming Ciphertext (to convey the note contents to the recipient)
* Outgoing Header (served as a backup, to convey the app contract address to the "outgoing viewer" - most likely the sender)
* Outgoing Ciphertext (served as a backup, encrypting the symmetric key of the incoming ciphertext to the "outgoing viewer" - most likely the sender)

The latter two have been removed from the `.emit()` functions, so now only an Incoming Header and Incoming Ciphertext will be emitted.

The interface for emitting a note has therefore changed, slightly. No more ovpk's need to be derived and passed into `.emit()` functions.

```
- nfts.at(to).insert(&mut new_note).emit(encode_and_encrypt_note(&mut context, from_ovpk_m, to, from));  
+ nfts.at(to).insert(&mut new_note).emit(encode_and_encrypt_note(&mut context, to, from));
```

The `getOutgoingNotes` function is removed from the PXE interface.

Some aztec.nr library methods' arguments are simplified to remove an `outgoing_viewer` parameter. E.g. `ValueNote::increment`, `ValueNote::decrement`, `ValueNote::decrement_by_at_most`, `EasyPrivateUint::add`, `EasyPrivateUint::sub`.

Further changes are planned, so that:

* Outgoing ciphertexts (or any kind of abstract ciphertext) can be emitted by a contract, and on the other side discovered and then processed by the contract.
* Headers will be removed, due to the new tagging scheme.

## 0.66## DEBUG env var is removedThe `DEBUG` variable is no longer used. Use `LOG_LEVEL` with one of `silent`, `fatal`, `error`, `warn`, `info`, `verbose`, `debug`, or `trace`. To tweak log levels per module, add a list of module prefixes with their overridden level. For example, LOG\_LEVEL="info; verbose: aztec:sequencer, aztec:archiver; debug: aztec:kv-store" sets `info` as the default log level, `verbose` for the sequencer and archiver, and `debug` for the kv-store. Module name match is done by prefix.

## `tty` resolve fallback required for browser bundlingWhen bundling `aztec.js` for web, the `tty` package now needs to be specified as an empty fallback:

```
resolve: {  
  plugins: [new ResolveTypeScriptPlugin()],  
  alias: { './node/index.js': false },  
  fallback: {  
    crypto: false,  
    os: false,  
    fs: false,  
    path: false,  
    url: false,  
+   tty: false,  
    worker_threads: false,  
    buffer: require.resolve('buffer/'),  
    util: require.resolve('util/'),  
    stream: require.resolve('stream-browserify'),  
  },  
},
```

## 0.65## [aztec.nr] Removed SharedImmutableThe `SharedImmutable` state variable has been removed, since it was essentially the exact same as `PublicImmutable`, which now contains functions for reading from private:

```
-   foo: SharedImmutable<T, Context>.  
+   foo: PublicImmutable<T, Context>.
```

## [aztec.nr] SharedImmutable renamings`SharedImmutable::read_private` and `SharedImmutable::read_public` were renamed to simply `read`, since only one of these versions is ever available depending on the current context.

```
// In private  
- let value = storage.my_var.read_private();  
+ let value = storage.my_var.read();  
  
// In public  
- let value = storage.my_var.read_public();  
+ let value = storage.my_var.read();
```

## [aztec.nr] SharedMutable renamings`SharedMutable` getters (`get_current_value_in_public`, etc.) were renamed by dropping the `_in<public|private|unconstrained>` suffix, since only one of these versions is ever available depending on the current context.

```
// In private  
- let value = storage.my_var.get_current_value_in_private();  
+ let value = storage.my_var.get_current_value();  
  
// In public  
- let value = storage.my_var.get_current_value_in_public();  
+ let value = storage.my_var.get_current_value();
```

## [aztec.js] Random addresses are now validThe `AztecAddress.random()` function now returns valid addresses, i.e. addresses that can receive encrypted messages and therefore have notes be sent to them. `AztecAddress.isValid()` was also added to check for validity of an address.

## 0.63.0## [PXE] Note tagging and discoveryPXE's trial decryption of notes has been replaced in favor of a tagging and discovery approach. It is much more efficient and should scale a lot better as the network size increases, since
notes can now be discovered on-demand. For the time being, this means that accounts residing *on different PXE instances* should add senders to their contact list, so notes can be discovered
(accounts created on the same PXE instance will be added as senders for each other by default)

```
+pxe.registerContact(senderAddress)
```

The note discovery process is triggered automatically whenever a contract invokes the `get_notes` oracle, meaning no contract changes are expected. Just in case, every contract has now a utility method
`sync_notes` that can trigger the process manually if necessary. This can be useful since now the `DebugInfo` object that can be obtained when sending a tx with the `debug` flag set to true
no longer contains the notes that were generated in the transaction:

```
const receipt = await inclusionsProofsContract.methods.create_note(owner, 5n).send().wait({ debug: true });  
-const { visibleIncomingNotes } = receipt.debugInfo!;  
-expect(visibleIncomingNotes.length).toEqual(1);  
+await inclusionsProofsContract.methods.sync_notes().simulate();  
+const incomingNotes = await wallet.getIncomingNotes({ txHash: receipt.txHash });  
+expect(incomingNotes.length).toEqual(1);
```

## [Token contract] Partial notes related refactorWe've decided to replace the old "shield" flow with one leveraging partial notes.
This led to a removal of `shield` and `redeem_shield` functions and an introduction of `transfer_to_private`.
An advantage of the new approach is that only 1 tx is required and the API of partial notes is generally nicer.
For more information on partial notes refer to docs.

## [Token contract] Function naming changesThere have been a few naming changes done for improved consistency.
These are the renamings:
`transfer_public` --> `transfer_in_public`
`transfer_from` --> `transfer_in_private`
`mint_public` --> `mint_to_public`
`burn` --> `burn_private`

## 0.62.0## [TXE] Single execution environmentThanks to recent advancements in Brillig TXE performs every single call as if it was a nested call, spawning a new ACVM or AVM simulator without performance loss.
This ensures every single test runs in a consistent environment and allows for clearer test syntax:

```
-let my_call_interface = MyContract::at(address).my_function(args);  
-env.call_private(my_contract_interface)  
+MyContract::at(address).my_function(args).call(&mut env.private());
```

This implies every contract has to be deployed before it can be tested (via `env.deploy` or `env.deploy_self`) and of course it has to be recompiled if its code was changed before TXE can use the modified bytecode.

## Uniqueness of L1 to L2 messagesL1 to L2 messages have been updated to guarantee their uniqueness. This means that the hash of an L1 to L2 message cannot be precomputed, and must be obtained from the `MessageSent` event emitted by the `Inbox` contract, found in the L1 transaction receipt that inserted the message:

```
event MessageSent(uint256 indexed l2BlockNumber, uint256 index, bytes32 indexed hash);
```

This event now also includes an `index`. This index was previously required to consume an L1 to L2 message in a public function, and now it is also required for doing so in a private function, since it is part of the message hash preimage. The `PrivateContext` in aztec-nr has been updated to reflect this:

```
pub fn consume_l1_to_l2_message(  
    &mut self,  
    content: Field,  
    secret: Field,  
    sender: EthAddress,  
+   leaf_index: Field,  
) {
```

This change has also modified the internal structure of the archiver database, making it incompatible with previous ones. Last, the API for obtaining an L1 to L2 message membership witness has been simplified to leverage message uniqueness:

```
getL1ToL2MessageMembershipWitness(  
  blockNumber: L2BlockNumber,  
  l1ToL2Message: Fr,  
- startIndex: bigint,  
): Promise<[bigint, SiblingPath<typeof L1_TO_L2_MSG_TREE_HEIGHT>] | undefined>;
```

## Address is now a pointThe address now serves as someone's public key to encrypt incoming notes. An address point has a corresponding address secret, which is used to decrypt the notes encrypted with the address point.

## Notes no longer store a hash of the nullifier public keys, and now store addressesBecause of removing key rotation, we can now store addresses as the owner of a note. Because of this and the above change, we can and have removed the process of registering a recipient, because now we do not need any keys of the recipient.

example\_note.nr

```
-npk_m_hash: Field  
+owner: AztecAddress
```

PXE Interface

```
-registerRecipient(completeAddress: CompleteAddress)
```

## 0.58.0## [l1-contracts] Inbox's MessageSent event emits global tree indexEarlier `MessageSent` event in Inbox emitted a subtree index (index of the message in the subtree of the l2Block). But the nodes and Aztec.nr expects the index in the global L1\_TO\_L2\_MESSAGES\_TREE. So to make it easier to parse this, Inbox now emits this global index.

## 0.57.0## Changes to PXE API and `ContractFunctionInteraction``PXE APIs have been refactored to better reflect the lifecycle of a Tx (`execute private -> simulate kernels -> simulate public (estimate gas) -> prove -> send`)

* `.simulateTx`: Now returns a `TxSimulationResult`, containing the output of private execution, kernel simulation and public simulation (optional).
* `.proveTx`: Now accepts the result of executing the private part of a transaction, so simulation doesn't have to happen again.

Thanks to this refactor, `ContractFunctionInteraction` has been updated to remove its internal cache and avoid bugs due to its mutable nature. As a result our type-safe interfaces now have to be used as follows:

```
-const action = MyContract.at(address).method(args);  
-await action.prove();  
-await action.send().wait();  
+const action = MyContract.at(address).method(args);  
+const provenTx = await action.prove();  
+await provenTx.send().wait();
```

It's still possible to use `.send()` as before, which will perform proving under the hood.

More changes are coming to these APIs to better support gas estimation mechanisms and advanced features.

## Changes to public calling conventionContracts that include public functions (that is, marked with `#[public]`), are required to have a function `public_dispatch(selector: Field)` which acts as an entry point. This will be soon the only public function registered/deployed in contracts. The calling convention is updated so that external calls are made to this function.

If you are writing your contracts using Aztec-nr, there is nothing you need to change. The `public_dispatch` function is automatically generated by the `#[aztec]` macro.

## [Aztec.nr] Renamed `unsafe_rand` to `random`Since this is an `unconstrained` function, callers are already supposed to include an `unsafe` block, so this function has been renamed for reduced verbosity.

```
-use aztec::oracle::unsafe_rand::unsafe_rand;  
+use aztec::oracle::random::random;  
  
-let random_value = unsafe { unsafe_rand() };  
+let random_value = unsafe { random() };
```

## [Aztec.js] Removed `L2Block.fromFields``L2Block.fromFields` was a syntactic sugar which is causing [issues](https://github.com/AztecProtocol/aztec-packages/issues/8340) so we've removed it.

```
-const l2Block = L2Block.fromFields({ header, archive, body });  
+const l2Block = new L2Block(archive, header, body);
```

## [Aztec.nr] Removed `SharedMutablePrivateGetter`This state variable was deleted due to it being difficult to use safely.

## [Aztec.nr] Changes to `NullifiableNote`The `compute_nullifier_without_context` function is now `unconstrained`. It had always been meant to be called in unconstrained contexts (which is why it did not receive the `context` object), but now that Noir supports trait functions being `unconstrained` this can be implemented properly. Users must add the `unconstrained` keyword to their implementations of the trait:

```
impl NullifiableNote for MyCustomNote {  
-    fn compute_nullifier_without_context(self) -> Field {  
+    unconstrained fn compute_nullifier_without_context(self) -> Field {
```

## [Aztec.nr] Make `TestEnvironment` unconstrainedAll of `TestEnvironment`'s functions are now `unconstrained`, preventing accidentally calling them in a constrained circuit, among other kinds of user error. Becuase they work with mutable references, and these are not allowed to cross the constrained/unconstrained barrier, tests that use `TestEnvironment` must also become `unconstrained`. The recommended practice is to make *all* Noir tests and test helper functions be `unconstrained:

```
#[test]  
-fn test_my_function() {  
+unconstrained fn test_my_function() {  
    let env = TestEnvironment::new();
```

## [Aztec.nr] removed `encode_and_encrypt_note` and renamed `encode_and_encrypt_note_with_keys` to `encode_and_encrypt_note````
contract XYZ {  
-   use dep::aztec::encrypted_logs::encrypted_note_emission::encode_and_encrypt_note_with_keys;  
+   use dep::aztec::encrypted_logs::encrypted_note_emission::encode_and_encrypt_note;  
...  
  
-    numbers.at(owner).initialize(&mut new_number).emit(encode_and_encrypt_note_with_keys(&mut context, owner_ovpk_m, owner_ivpk_m, owner));  
+    numbers.at(owner).initialize(&mut new_number).emit(encode_and_encrypt_note(&mut context, owner_ovpk_m, owner_ivpk_m, owner));  
}
```

## 0.56.0## [Aztec.nr] Changes to contract definitionWe've migrated the Aztec macros to use the newly introduce meta programming Noir feature. Due to being Noir-based, the new macros are less obscure and can be more easily modified.

As part of this transition, some changes need to be applied to Aztec contracts:

* The top level `contract` block needs to have the `#[aztec]` macro applied to it.
* All `#[aztec(name)]` macros are renamed to `#[name]`.
* The storage struct (the one that gets the `#[storage]` macro applied) but be generic over a `Context` type, and all state variables receive this type as their last generic type parameter.

```
+ use dep::aztec::macros::aztec;  
  
#[aztec]  
contract Token {  
+    use dep::aztec::macros::{storage::storage, events::event, functions::{initializer, private, view, public}};  
  
-    #[aztec(storage)]  
-    struct Storage {  
+    #[storage]  
+    struct Storage<Context> {  
-        admin: PublicMutable<AztecAddress>,  
+        admin: PublicMutable<AztecAddress, Context>,  
-        minters: Map<AztecAddress, PublicMutable<bool>>,  
+        minters: Map<AztecAddress, PublicMutable<bool, Context>, Context>,  
    }  
  
-    #[aztec(public)]  
-    #[aztec(initializer)]  
+    #[public]  
+    #[initializer]  
    fn constructor(admin: AztecAddress, name: str<31>, symbol: str<31>, decimals: u8) {  
        ...  
    }  
  
-    #[aztec(public)]  
-    #[aztec(view)]  
-    fn public_get_name() -> FieldCompressedString {  
+    #[public]  
+    #[view]  
    fn public_get_name() -> FieldCompressedString {  
        ...  
    }
```

## [Aztec.nr] Changes to `NoteInterface`The new macro model prevents partial trait auto-implementation: they either implement the entire trait or none of it. This means users can no longer implement part of `NoteInterface` and have the rest be auto-implemented.

For this reason we've separated the methods which are auto-implemented and those which needs to be implemented manually into two separate traits: the auto-implemented ones stay in the `NoteInterface` trace and the manually implemented ones were moved to `NullifiableNote` (name likely to change):

```
-#[aztec(note)]  
+#[note]  
struct AddressNote {  
    ...  
}  
  
-impl NoteInterface<ADDRESS_NOTE_LEN, ADDRESS_NOTE_BYTES_LEN> for AddressNote {  
+impl NullifiableNote for AddressNote {  
    fn compute_nullifier(self, context: &mut PrivateContext, note_hash_for_nullify: Field) -> Field {  
        ...  
    }  
  
    fn compute_nullifier_without_context(self) -> Field {  
        ...  
    }  
}
```

## [Aztec.nr] Changes to contract interfaceThe `Contract::storage()` static method has been renamed to `Contract::storage_layout()`.

```
-    let fee_payer_balances_slot = derive_storage_slot_in_map(Token::storage().balances.slot, fee_payer);  
-    let user_balances_slot = derive_storage_slot_in_map(Token::storage().balances.slot, user);  
+    let fee_payer_balances_slot = derive_storage_slot_in_map(Token::storage_layout().balances.slot, fee_payer);  
+    let user_balances_slot = derive_storage_slot_in_map(Token::storage_layout().balances.slot, user);
```

## Key rotation removedThe ability to rotate incoming, outgoing, nullifying and tagging keys has been removed - this feature was easy to misuse and not worth the complexity and gate count cost. As part of this, the Key Registry contract has also been deleted. The API for fetching public keys has been adjusted accordingly:

```
- let keys = get_current_public_keys(&mut context, account);  
+ let keys = get_public_keys(account);
```

## [Aztec.nr] Rework `NoteGetterOptions::select`The `select` function in both `NoteGetterOptions` and `NoteViewerOptions` no longer takes an `Option` of a comparator, but instead requires an explicit comparator to be passed. Additionally, the order of the parameters has been changed so that they are `(lhs, operator, rhs)`. These two changes should make invocations of the function easier to read:

```
- options.select(ValueNote::properties().value, amount, Option::none())  
+ options.select(ValueNote::properties().value, Comparator.EQ, amount)
```

## 0.53.0## [Aztec.nr] Remove `OwnedNote` and create `UintNote``OwnedNote` allowed having a U128 `value` in the custom note while `ValueNote` restricted to just a Field.

We have removed `OwnedNote` but are introducing a more genric `UintNote` within aztec.nr

```
#[aztec(note)]  
struct UintNote {  
    // The integer stored by the note  
    value: U128,  
    // The nullifying public key hash is used with the nsk_app to ensure that the note can be privately spent.  
    npk_m_hash: Field,  
    // Randomness of the note to hide its contents  
    randomness: Field,  
}
```

## [TXE] loggingYou can now use `debug_log()` within your contract to print logs when using the TXE

Remember to set the following environment variables to activate debug logging:

```
export DEBUG="aztec:*"  
export LOG_LEVEL="debug"
```

## [Account] no assert in is\_valid\_impl`is_valid_impl` method in account contract asserted if signature was true. Instead now we will return the verification to give flexibility to developers to handle it as they please.

```
- let verification = std::ecdsa_secp256k1::verify_signature(public_key.x, public_key.y, signature, hashed_message);  
- assert(verification == true);  
- true  
+ std::ecdsa_secp256k1::verify_signature(public_key.x, public_key.y, signature, hashed_message)
```

## 0.49.0## Key Rotation API overhaulPublic keys (ivpk, ovpk, npk, tpk) should no longer be fetched using the old `get_[x]pk_m` methods on the `Header` struct, but rather by calling `get_current_public_keys`, which returns a `PublicKeys` struct with all four keys at once:

```
+use dep::aztec::keys::getters::get_current_public_keys;  
  
-let header = context.header();  
-let owner_ivpk_m = header.get_ivpk_m(&mut context, owner);  
-let owner_ovpk_m = header.get_ovpk_m(&mut context, owner);  
+let owner_keys = get_current_public_keys(&mut context, owner);  
+let owner_ivpk_m = owner_keys.ivpk_m;  
+let owner_ovpk_m = owner_keys.ovpk_m;
```

If using more than one key per account, this will result in very large circuit gate count reductions.

Additionally, `get_historical_public_keys` was added to support reading historical keys using a historical header:

```
+use dep::aztec::keys::getters::get_historical_public_keys;  
  
let historical_header = context.header_at(some_block_number);  
-let owner_ivpk_m = header.get_ivpk_m(&mut context, owner);  
-let owner_ovpk_m = header.get_ovpk_m(&mut context, owner);  
+let owner_keys = get_historical_public_keys(historical_header, owner);  
+let owner_ivpk_m = owner_keys.ivpk_m;  
+let owner_ovpk_m = owner_keys.ovpk_m;
```

## 0.48.0## NoteInterface changes`compute_note_hash_and_nullifier*` functions were renamed as `compute_nullifier*` and the `compute_nullifier` function now takes `note_hash_for_nullify` as an argument (this allowed us to reduce gate counts and the hash was typically computed before). Also `compute_note_hash_for_consumption` function was renamed as `compute_note_hash_for_nullification`.

```
impl NoteInterface<VALUE_NOTE_LEN, VALUE_NOTE_BYTES_LEN> for ValueNote {  
-    fn compute_note_hash_and_nullifier(self, context: &mut PrivateContext) -> (Field, Field) {  
-        let note_hash_for_nullify = compute_note_hash_for_consumption(self);  
-        let secret = context.request_nsk_app(self.npk_m_hash);  
-        let nullifier = poseidon2_hash_with_separator([  
-            note_hash_for_nullify,  
-            secret,  
-        ],  
-            GENERATOR_INDEX__NOTE_NULLIFIER as Field,  
-        );  
-        (note_hash_for_nullify, nullifier)  
-    }  
-    fn compute_note_hash_and_nullifier_without_context(self) -> (Field, Field) {  
-        let note_hash_for_nullify = compute_note_hash_for_consumption(self);  
-        let secret = get_nsk_app(self.npk_m_hash);  
-        let nullifier = poseidon2_hash_with_separator([  
-            note_hash_for_nullify,  
-            secret,  
-        ],  
-            GENERATOR_INDEX__NOTE_NULLIFIER as Field,  
-        );  
-        (note_hash_for_nullify, nullifier)  
-    }  
  
+    fn compute_nullifier(self, context: &mut PrivateContext, note_hash_for_nullify: Field) -> Field {  
+        let secret = context.request_nsk_app(self.npk_m_hash);  
+        poseidon2_hash_with_separator([  
+            note_hash_for_nullify,  
+            secret  
+        ],  
+            GENERATOR_INDEX__NOTE_NULLIFIER as Field,  
+        )  
+    }  
+    fn compute_nullifier_without_context(self) -> Field {  
+        let note_hash_for_nullify = compute_note_hash_for_nullification(self);  
+        let secret = get_nsk_app(self.npk_m_hash);  
+        poseidon2_hash_with_separator([  
+            note_hash_for_nullify,  
+            secret,  
+        ],  
+            GENERATOR_INDEX__NOTE_NULLIFIER as Field,  
+        )  
+    }  
}
```

## Fee Juice renameThe name of the canonical Gas contract has changed to Fee Juice. Update noir code:

```
-GasToken::at(contract_address)  
+FeeJuice::at(contract_address)
```

Additionally, `NativePaymentMethod` and `NativePaymentMethodWithClaim` have been renamed to `FeeJuicePaymentMethod` and `FeeJuicePaymentMethodWithClaim`.

## PrivateSet::pop\_notes(...)")

The most common flow when working with notes is obtaining them from a `PrivateSet` via `get_notes(...)` and then removing them via `PrivateSet::remove(...)`.
This is cumbersome and it results in unnecessary constraints due to a redundant note read request checks in the remove function.

For this reason we've implemented `pop_notes(...)` which gets the notes, removes them from the set and returns them.
This tight coupling of getting notes and removing them allowed us to safely remove the redundant read request check.

Token contract diff:

```
-let options = NoteGetterOptions::with_filter(filter_notes_min_sum, target_amount).set_limit(max_notes);  
-let notes = self.map.at(owner).get_notes(options);  
-let mut subtracted = U128::from_integer(0);  
-for i in 0..options.limit {  
-    if i < notes.len() {  
-        let note = notes.get_unchecked(i);  
-        self.map.at(owner).remove(note);  
-        subtracted = subtracted + note.get_amount();  
-    }  
-}  
-assert(minuend >= subtrahend, "Balance too low");  
+let options = NoteGetterOptions::with_filter(filter_notes_min_sum, target_amount).set_limit(max_notes);  
+let notes = self.map.at(owner).pop_notes(options);  
+let mut subtracted = U128::from_integer(0);  
+for i in 0..options.limit {  
+    if i < notes.len() {  
+        let note = notes.get_unchecked(i);  
+        subtracted = subtracted + note.get_amount();  
+    }  
+}  
+assert(minuend >= subtrahend, "Balance too low");
```

Note that `pop_notes` may not have obtained and removed any notes! The caller must place checks on the returned notes, e.g. in the example above by checking a sum of balances, or by checking the number of returned notes (`assert_eq(notes.len(), expected_num_notes)`).

## 0.47.0# [Aztec sandbox] TXE deployment changes

The way simulated deployments are done in TXE tests has changed to avoid relying on TS interfaces. It is now possible to do it by directly pointing to a Noir standalone contract or workspace:

```
-let deployer = env.deploy("path_to_contract_ts_interface");  
+let deployer = env.deploy("path_to_contract_root_folder_where_nargo_toml_is", "ContractName");
```

Extended syntax for more use cases:

```
// The contract we're testing  
env.deploy_self("ContractName"); // We have to provide ContractName since nargo it's ready to support multi-contract files  
  
// A contract in a workspace  
env.deploy("../path/to/workspace@package_name", "ContractName"); // This format allows locating the artifact in the root workspace target folder, regardless of internal code organization
```

The deploy function returns a `Deployer`, which requires performing a subsequent call to `without_initializer()`, `with_private_initializer()` or `with_public_initializer()` just like before in order to **actually** deploy the contract.

## [CLI] Command refactor and unification + `aztec test`Sandbox commands have been cleaned up and simplified. Doing `aztec-up` now gets you the following top-level commands:

`aztec`: All the previous commands + all the CLI ones without having to prefix them with cli. Run `aztec` for help!
`aztec-nargo`: No changes

**REMOVED/RENAMED**:

* `aztec-sandbox` and `aztec sandbox`: now `aztec start --sandbox`
* `aztec-builder`: now `aztec codegen` and `aztec update`

**ADDED**:

* `aztec test [options]`: runs `aztec start --txe && aztec-nargo test --oracle-resolver http://aztec:8081 --silence-warnings [options]` via docker-compose allowing users to easily run contract tests using TXE

## 0.45.0## [Aztec.nr] Remove unencrypted logs from privateThey leak privacy so is a footgun!

## 0.44.0## [Aztec.nr] Autogenerate Serialize methods for events```
#[aztec(event)]  
struct WithdrawalProcessed {  
    who: Field,  
    amount: Field,  
}  
  
-impl Serialize<2> for WithdrawalProcessed {  
-    fn serialize(self: Self) -> [Field; 2] {  
-        [self.who.to_field(), self.amount as Field]  
-    }  
}
```

## [Aztec.nr] rename `encode_and_encrypt_with_keys` to `encode_and_encrypt_note_with_keys````
contract XYZ {  
-   use dep::aztec::encrypted_logs::encrypted_note_emission::encode_and_encrypt_with_keys;  
+   use dep::aztec::encrypted_logs::encrypted_note_emission::encode_and_encrypt_note_with_keys;  
....  
  
-    numbers.at(owner).initialize(&mut new_number).emit(encode_and_encrypt_with_keys(&mut context, owner_ovpk_m, owner_ivpk_m));  
+    numbers.at(owner).initialize(&mut new_number).emit(encode_and_encrypt_note_with_keys(&mut context, owner_ovpk_m, owner_ivpk_m));  
}
```

## [Aztec.nr] changes to `NoteInterface``compute_nullifier` function was renamed to `compute_note_hash_and_nullifier` and now the function has to return not only the nullifier but also the note hash used to compute the nullifier.
The same change was done to `compute_nullifier_without_context` function.
These changes were done because having the note hash exposed allowed us to not having to re-compute it again in `destroy_note` function of Aztec.nr which led to significant decrease in gate counts (see the [optimization PR](https://github.com/AztecProtocol/aztec-packages/pull/7103) for more details).

```
- impl NoteInterface<VALUE_NOTE_LEN, VALUE_NOTE_BYTES_LEN> for ValueNote {  
-    fn compute_nullifier(self, context: &mut PrivateContext) -> Field {  
-        let note_hash_for_nullify = compute_note_hash_for_consumption(self);  
-        let secret = context.request_nsk_app(self.npk_m_hash);  
-        poseidon2_hash([  
-            note_hash_for_nullify,  
-            secret,  
-            GENERATOR_INDEX__NOTE_NULLIFIER as Field,  
-        ])  
-    }  
-  
-    fn compute_nullifier_without_context(self) -> Field {  
-        let note_hash_for_nullify = compute_note_hash_for_consumption(self);  
-        let secret = get_nsk_app(self.npk_m_hash);  
-        poseidon2_hash([  
-            note_hash_for_nullify,  
-            secret,  
-            GENERATOR_INDEX__NOTE_NULLIFIER as Field,  
-        ])  
-    }  
- }  
+ impl NoteInterface<VALUE_NOTE_LEN, VALUE_NOTE_BYTES_LEN> for ValueNote {  
+    fn compute_note_hash_and_nullifier(self, context: &mut PrivateContext) -> (Field, Field) {  
+        let note_hash_for_nullify = compute_note_hash_for_consumption(self);  
+        let secret = context.request_nsk_app(self.npk_m_hash);  
+        let nullifier = poseidon2_hash([  
+            note_hash_for_nullify,  
+            secret,  
+            GENERATOR_INDEX__NOTE_NULLIFIER as Field,  
+        ]);  
+        (note_hash_for_nullify, nullifier)  
+    }  
+  
+    fn compute_note_hash_and_nullifier_without_context(self) -> (Field, Field) {  
+        let note_hash_for_nullify = compute_note_hash_for_consumption(self);  
+        let secret = get_nsk_app(self.npk_m_hash);  
+        let nullifier = poseidon2_hash([  
+            note_hash_for_nullify,  
+            secret,  
+            GENERATOR_INDEX__NOTE_NULLIFIER as Field,  
+        ]);  
+        (note_hash_for_nullify, nullifier)  
+    }  
+ }
```

## [Aztec.nr] `note_getter` returns `BoundedVec`The `get_notes` and `view_notes` function no longer return an array of options (i.e. `[Option<Note>, N_NOTES]`) but instead a `BoundedVec<Note, N_NOTES>`. This better conveys the useful property the old array had of having all notes collapsed at the beginning of the array, which allows for powerful optimizations and gate count reduction when setting the `options.limit` value.

A `BoundedVec` has a `max_len()`, which equals the number of elements it can hold, and a `len()`, which equals the number of elements it currently holds. Since `len()` is typically not knwon at compile time, iterating over a `BoundedVec` looks slightly different than iterating over an array of options:

```
- let option_notes = get_notes(options);  
- for i in 0..option_notes.len() {  
-     if option_notes[i].is_some() {  
-         let note = option_notes[i].unwrap_unchecked();  
-     }  
- }  
+ let notes = get_notes(options);  
+ for i in 0..notes.max_len() {  
+     if i < notes.len() {  
+         let note = notes.get_unchecked(i);  
+     }  
+ }
```

To further reduce gate count, you can iterate over `options.limit` instead of `max_len()`, since `options.limit` is guaranteed to be larger or equal to `len()`, and smaller or equal to `max_len()`:

```
- for i in 0..notes.max_len() {  
+ for i in 0..options.limit {
```

## [Aztec.nr] static private authwitThe private authwit validation is now making a static call to the account contract instead of passing over control flow. This is to ensure that it cannot be used for re-entry.

To make this change however, we cannot allow emitting a nullifier from the account contract, since that would break the static call. Instead, we will be changing the `spend_private_authwit` to a `verify_private_authwit` and in the `auth` library emit the nullifier. This means that the "calling" contract will now be emitting the nullifier, and not the account. For example, for a token contract, the nullifier is now emitted by the token contract. However, as this is done inside the `auth` library, the token contract doesn't need to change much.

The biggest difference is related to "cancelling" an authwit. Since it is no longer in the account contract, you cannot just emit a nullifier from it anymore. Instead it must rely on the token contract providing functionality for cancelling.

There are also a few general changes to how authwits are generated, namely to more easily support the data required for a validity lookup now. Previously we could lookup the `message_hash` directly at the account contract, now we instead need to use the `inner_hash` and the contract of the consumer to figure out if it have already been emitted.

A minor extension have been made to the authwit creations to make it easier to sign a specific a hash with a specific caller, e.g., the `inner_hash` can be provided as `{consumer, inner_hash}` to the `createAuthWit` where it previously needed to do a couple of manual steps to compute the outer hash. The `computeOuterAuthWitHash` have been made internal and the `computeAuthWitMessageHash` can instead be used to compute the values similarly to other authwit computations.

```
const innerHash = computeInnerAuthWitHash([Fr.ZERO, functionSelector.toField(), entrypointPackedArgs.hash]);  
-const outerHash = computeOuterAuthWitHash(  
-    this.dappEntrypointAddress,  
-    new Fr(this.chainId),  
-    new Fr(this.version),  
-    innerHash,  
-);  
+const messageHash = computeAuthWitMessageHash(  
+    { consumer: this.dappEntrypointAddress, innerHash },  
+    { chainId: new Fr(this.chainId), version: new Fr(this.version) },  
+);
```

If the wallet is used to compute the authwit, it will populate the chain id and version instead of requiring it to be provided by tha actor.

```
const innerHash = computeInnerAuthWitHash([Fr.fromString('0xdead')]);  
-const outerHash = computeOuterAuthWitHash(wallets[1].getAddress(), chainId, version, innerHash);  
-const witness = await wallets[0].createAuthWit(outerHash);  
+ const witness = await wallets[0].createAuthWit({ comsumer: accounts[1].address, inner_hash });
```

## 0.43.0## [Aztec.nr] break `token.transfer()` into `transfer` and `transferFrom`Earlier we had just one function - `transfer()` which used authwits to handle the case where a contract/user wants to transfer funds on behalf of another user.
To reduce circuit sizes and proof times, we are breaking up `transfer` and introducing a dedicated `transferFrom()` function like in the ERC20 standard.

## [Aztec.nr] `options.limit` has to be constantThe `limit` parameter in `NoteGetterOptions` and `NoteViewerOptions` is now required to be a compile-time constant. This allows performing loops over this value, which leads to reduced circuit gate counts when setting a `limit` value.

## [Aztec.nr] canonical public authwit registryThe public authwits are moved into a shared registry (auth registry) to make it easier for sequencers to approve for their non-revertible (setup phase) whitelist. Previously, it was possible to DOS a sequencer by having a very expensive authwit validation that fails at the end, now the whitelist simply need the registry.

Notable, this means that consuming a public authwit will no longer emit a nullifier in the account contract but instead update STORAGE in the public domain. This means that there is a larger difference between private and public again. However, it also means that if contracts need to approve, and use the approval in the same tx, it is transient and don't need to go to DA (saving 96 bytes).

For the typescript wallets this is handled so the APIs don't change, but account contracts should get rid of their current setup with `approved_actions`.

```
- let actions = AccountActions::init(&mut context, ACCOUNT_ACTIONS_STORAGE_SLOT, is_valid_impl);  
+ let actions = AccountActions::init(&mut context, is_valid_impl);
```

For contracts we have added a `set_authorized` function in the auth library that can be used to set values in the registry.

```
- storage.approved_action.at(message_hash).write(true);  
+ set_authorized(&mut context, message_hash, true);
```

## [Aztec.nr] emit encrypted logsEmitting or broadcasting encrypted notes are no longer done as part of the note creation, but must explicitly be either emitted or discarded instead.

```
+ use dep::aztec::encrypted_logs::encrypted_note_emission::{encode_and_encrypt, encode_and_encrypt_with_keys};  
  
- storage.balances.sub(from, amount);  
+ storage.balances.sub(from, amount).emit(encode_and_encrypt_with_keys(&mut context, from, from));  
+ storage.balances.sub(from, amount).emit(encode_and_encrypt_with_keys(&mut context, from_ovpk, from_ivpk));  
+ storage.balances.sub(from, amount).discard();
```

## 0.42.0## [Aztec.nr] Unconstrained ContextTop-level unconstrained execution is now marked by the new `UnconstrainedContext`, which provides access to the block number and contract address being used in the simulation. Any custom state variables that provided unconstrained functions should update their specialization parameter:

```
+ use dep::aztec::context::UnconstrainedContext;  
  
- impl MyStateVariable<()> {  
+ impl MyStateVariable<UnconstrainedContext> {
```

## [Aztec.nr] Filtering is now constrainedThe `filter` argument of `NoteGetterOptions` (typically passed via the `with_filter()` function) is now applied in a constraining environment, meaning any assertions made during the filtering are guaranteed to hold. This mirrors the behavior of the `select()` function.

## [Aztec.nr] Emitting encrypted notes and logsThe `emit_encrypted_log` context function is now `encrypt_and_emit_log` or `encrypt_and_emit_note`.

```
- context.emit_encrypted_log(log1);  
+ context.encrypt_and_emit_log(log1);  
+ context.encrypt_and_emit_note(note1);
```

Broadcasting a note will call `encrypt_and_emit_note` in the background. To broadcast a generic event, use `encrypt_and_emit_log`
with the same encryption parameters as notes require. Currently, only fields and arrays of fields are supported as events.

By default, logs emitted via `encrypt_and_emit_log` will be siloed with a *masked* contract address. To force the contract address to be revealed, so everyone can check it rather than just the log recipient, provide `randomness = 0`.

## Public execution migrated to the Aztec Virtual Machine**What does this mean for me?**

It should be mostly transparent, with a few caveats:

* Not all Noir blackbox functions are supported by the AVM. Only `Sha256`, `PedersenHash`, `Poseidon2Permutation`, `Keccak256`, and `ToRadix` are supported.
* For public functions, `context.nullifier_exists(...)` will now also consider pending nullifiers.
* The following methods of `PublicContext` are not supported anymore: `fee_recipient`, `fee_per_da_gas`, `fee_per_l2_gas`, `call_public_function_no_args`, `static_call_public_function_no_args`, `delegate_call_public_function_no_args`, `call_public_function_with_packed_args`, `set_return_hash`, `finish`. However, in terms of functionality, the new context's interface should be equivalent (unless otherwise specified in this list).
* Delegate calls are not yet supported in the AVM.
* If you have types with custom serialization that you use across external contracts calls, you might need to modify its serialization to match how Noir would serialize it. This is a known problem unrelated to the AVM, but triggered more often when using it.
* A few error messages might change format, so you might need to change your test assertions.

**Internal details**

Before this change, public bytecode was executed using the same simulator as in private: the ACIR simulator (and internally, the Brillig VM). On the Aztec.nr side, public functions accessed the context through `PublicContext`.

After this change, public bytecode will be run using the AVM simulator (the simulator for our upcoming zkVM). This bytecode is generated from Noir contracts in two steps: First, `nargo compile` produces an artifact which has Brillig bytecode for public functions, just as it did before. Second: the `avm-transpiler` takes that artifact, and it transpiles Brillig bytecode to AVM bytecode. This final artifact can now be deployed and used with the new public runtime.

On the Aztec.nr side, public functions keep accessing the context using `PublicContext` but the underlying implementation is switch with what formerly was the `AvmContext`.

## 0.41.0## [Aztec.nr] State variable reworkAztec.nr state variables have been reworked so that calling private functions in public and vice versa is detected as an error during compilation instead of at runtime. This affects users in a number of ways:

## New compile time errorsIt used to be that calling a state variable method only available in public from a private function resulted in obscure runtime errors in the form of a failed `_is_some` assertion.

Incorrect usage of the state variable methods now results in compile time errors. For example, given the following function:

```
#[aztec(public)]  
fn get_decimals() -> pub u8 {  
    storage.decimals.read_private()  
}
```

The compiler will now error out with

```
Expected type SharedImmutable<_, &mut PrivateContext>, found type SharedImmutable<u8, &mut PublicContext>
```

The key component is the second generic parameter: the compiler expects a `PrivateContext` (becuse `read_private` is only available during private execution), but a `PublicContext` is being used instead (because of the `#[aztec(public)]` attribute).

## Generic parameters in `Storage`The `Storage` struct (the one marked with `#[aztec(storage)]`) should now be generic over a `Context` type, which matches the new generic parameter of all Aztec.nr libraries. This parameter is always the last generic parameter.

This means that, without any additional features, we'd end up with some extra boilerplate when declaring this struct:

```
#[aztec(storage)]  
- struct Storage {  
+ struct Storage<Context> {  
-   nonce_for_burn_approval: PublicMutable<Field>,  
+   nonce_for_burn_approval: PublicMutable<Field, Context>,  
-   portal_address: SharedImmutable<EthAddress>,  
+   portal_address: SharedImmutable<EthAddress, Context>,  
-   approved_action: Map<Field, PublicMutable<bool>>,  
+   approved_action: Map<Field, PublicMutable<bool, Context>, Context>,  
}
```

Because of this, the `#[aztec(storage)]` macro has been updated to **automatically inject** this `Context` generic parameter. The storage declaration does not require any changes.

## Removal of `Context`The `Context` type no longer exists. End users typically didn't use it, but if imported it needs to be deleted.

It is now possible to explicitly state a function doesn't perform any state alterations (including storage, logs, nullifiers and/or messages from L2 to L1) with the `#[aztec(view)]` attribute, similarly to solidity's `view` function modifier.

```
    #[aztec(public)]  
+   #[aztec(view)]  
    fn get_price(asset_id: Field) -> Asset {  
        storage.assets.at(asset_id).read()  
    }
```

View functions only generate a `StaticCallInterface` that doesn't include `.call` or `.enqueue` methods. Also, the denomination `static` has been completely removed from the interfaces, in favor of the more familiar `view`

```
- let price = PriceFeed::at(asset.oracle).get_price(0).static_call(&mut context).price;  
+ let price = PriceFeed::at(asset.oracle).get_price(0).view(&mut context).price;
```

```
#[aztec(private)]  
fn enqueue_public_get_value_from_child(target_contract: AztecAddress, value: Field) {  
-   StaticChild::at(target_contract).pub_get_value(value).static_enqueue(&mut context);  
+   StaticChild::at(target_contract).pub_get_value(value).enqueue_view(&mut context);  
}
```

Additionally, the Noir LSP will now honor "go to definitions" requests for contract interfaces (Ctrl+click), taking the user to the original function implementation.

## [Aztec.js] Simulate changes* `.simulate()` now tracks closer the process performed by `.send().wait()`, specifically going through the account contract entrypoint instead of directly calling the intended function.
* `wallet.viewTx(...)` has been renamed to `wallet.simulateUnconstrained(...)` to better clarify what it does.

## [Aztec.nr] Keys: Token note now stores an owner master nullifying public key hash instead of an owner addressi.e.

```
struct TokenNote {  
    amount: U128,  
-   owner: AztecAddress,  
+   npk_m_hash: Field,  
    randomness: Field,  
}
```

Creating a token note and adding it to storage now looks like this:

```
- let mut note = ValueNote::new(new_value, owner);  
- storage.a_private_value.insert(&mut note, true);  
+ let owner_npk_m_hash = get_npk_m_hash(&mut context, owner);  
+ let owner_ivpk_m = get_ivpk_m(&mut context, owner);  
+ let mut note = ValueNote::new(new_value, owner_npk_m_hash);  
+ storage.a_private_value.insert(&mut note, true, owner_ivpk_m);
```

Computing the nullifier similarly changes to use this master nullifying public key hash.

## 0.40.0## [Aztec.nr] Debug loggingThe function `debug_log_array_with_prefix` has been removed. Use `debug_log_format` with `{}` instead. The special sequence `{}` will be replaced with the whole array. You can also use `{0}`, `{1}`, ... as usual with `debug_log_format`.

```
- debug_log_array_with_prefix("Prefix", my_array);  
+ debug_log_format("Prefix {}", my_array);
```

## 0.39.0## [Aztec.nr] Mutable delays in `SharedMutable`The type signature for `SharedMutable` changed from `SharedMutable<T, DELAY>` to `SharedMutable<T, INITIAL_DELAY>`. The behavior is the same as before, except the delay can now be changed after deployment by calling `schedule_delay_change`.

## [Aztec.nr] get\_public\_key oracle replaced with get\_ivpk\_mWhen implementing changes according to a new key scheme we had to change oracles.
What used to be called encryption public key is now master incoming viewing public key.

```
- use dep::aztec::oracles::get_public_key::get_public_key;  
+ use dep::aztec::keys::getters::get_ivpk_m;  
  
- let encryption_pub_key = get_public_key(self.owner);  
+ let ivpk_m = get_ivpk_m(context, self.owner);
```

## 0.38.0## [Aztec.nr] Emitting encrypted logsThe `emit_encrypted_log` function is now a context method.

```
- use dep::aztec::log::emit_encrypted_log;  
- use dep::aztec::logs::emit_encrypted_log;  
  
- emit_encrypted_log(context, log1);  
+ context.emit_encrypted_log(log1);
```

## 0.36.0## `FieldNote` removed`FieldNote` only existed for testing purposes, and was not a note type that should be used in any real application. Its name unfortunately led users to think that it was a note type suitable to store a `Field` value, which it wasn't.

If using `FieldNote`, you most likely want to use `ValueNote` instead, which has both randomness for privacy and an owner for proper nullification.

## `SlowUpdatesTree` replaced for `SharedMutable`The old `SlowUpdatesTree` contract and libraries have been removed from the codebase, use the new `SharedMutable` library instead. This will require that you add a global variable specifying a delay in blocks for updates, and replace the slow updates tree state variable with `SharedMutable` variables.

```
+ global CHANGE_ROLES_DELAY_BLOCKS = 5;  
  
struct Storage {  
-  slow_update: SharedImmutable<AztecAddress>,  
+  roles: Map<AztecAddress, SharedMutable<UserFlags, CHANGE_ROLES_DELAY_BLOCKS>>,  
}
```

Reading from `SharedMutable` is much simpler, all that's required is to call `get_current_value_in_public` or `get_current_value_in_private`, depending on the domain.

```
- let caller_roles = UserFlags::new(U128::from_integer(slow.read_at_pub(context.msg_sender().to_field()).call(&mut context)));  
+ let caller_roles = storage.roles.at(context.msg_sender()).get_current_value_in_public();
```

Finally, you can remove all capsule usage on the client code or tests, since those are no longer required when working with `SharedMutable`.

## [Aztec.nr & js] Portal addressesDeployments have been modified. No longer are portal addresses treated as a special class, being immutably set on creation of a contract. They are no longer passed in differently compared to the other variables and instead should be implemented using usual storage by those who require it. One should use the storage that matches the usecase - likely shared storage to support private and public.

This means that you will likely add the portal as a constructor argument

```
- fn constructor(token: AztecAddress) {  
-    storage.token.write(token);  
- }  
+ struct Storage {  
    ...  
+   portal_address: SharedImmutable<AztecAddress>,  
+ }  
+ fn constructor(token: AztecAddress, portal_address: EthAddress) {  
+    storage.token.write(token);  
+    storage.portal_address.initialize(portal_address);  
+ }
```

And read it from storage whenever needed instead of from the context.

```
- context.this_portal_address(),  
+ storage.portal_address.read_public(),
```

## [Aztec.nr] OraclesOracle `get_nullifier_secret_key` was renamed to `get_app_nullifier_secret_key` and `request_nullifier_secret_key` function on PrivateContext was renamed as `request_app_nullifier_secret_key`.

```
- let secret = get_nullifier_secret_key(self.owner);  
+ let secret = get_app_nullifier_secret_key(self.owner);
```

```
- let secret = context.request_nullifier_secret_key(self.owner);  
+ let secret = context.request_app_nullifier_secret_key(self.owner);
```

## [Aztec.nr] Contract interfacesIt is now possible to import contracts on another contracts and use their automatic interfaces to perform calls. The interfaces have the same name as the contract, and are automatically exported. Parameters are automatically serialized (using the `Serialize<N>` trait) and return values are automatically deserialized (using the `Deserialize<N>` trait). Serialize and Deserialize methods have to conform to the standard ACVM serialization schema for the interface to work!

1. Only fixed length types are supported
2. All numeric types become Fields
3. Strings become arrays of Fields, one per char
4. Arrays become arrays of Fields following rules 2 and 3
5. Structs become arrays of Fields, with every item defined in the same order as they are in Noir code, following rules 2, 3, 4 and 5 (recursive)

```
- context.call_public_function(  
-   storage.gas_token_address.read_private(),  
-   FunctionSelector::from_signature("pay_fee(Field)"),  
-   [42]  
- );  
-  
- context.call_public_function(  
-   storage.gas_token_address.read_private(),  
-   FunctionSelector::from_signature("pay_fee(Field)"),  
-   [42]  
- );  
-  
- let _ = context.call_private_function(  
-           storage.subscription_token_address.read_private(),  
-           FunctionSelector::from_signature("transfer((Field),(Field),Field,Field)"),  
-           [  
-            context.msg_sender().to_field(),  
-            storage.subscription_recipient_address.read_private().to_field(),  
-            storage.subscription_price.read_private(),  
-            nonce  
-           ]  
-  );  
+ use dep::gas_token::GasToken;  
+ use dep::token::Token;  
+  
+ ...  
+ // Public call from public land  
+ GasToken::at(storage.gas_token_address.read_private()).pay_fee(42).call(&mut context);  
+ // Public call from private land  
+ GasToken::at(storage.gas_token_address.read_private()).pay_fee(42).enqueue(&mut context);  
+ // Private call from private land  
+ Token::at(asset).transfer(context.msg_sender(), storage.subscription_recipient_address.read_private(), amount, nonce).call(&mut context);
```

It is also possible to use these automatic interfaces from the local contract, and thus enqueue public calls from private without having to rely on low level `context` calls.

## [Aztec.nr] Rename max block number setterThe `request_max_block_number` function has been renamed to `set_tx_max_block_number` to better reflect that it is not a getter, and that the setting is transaction-wide.

```
- context.request_max_block_number(value);  
+ context.set_tx_max_block_number(value);
```

## [Aztec.nr] Get portal addressThe `get_portal_address` oracle was removed. If you need to get the portal address of SomeContract, add the following methods to it

```
#[aztec(private)]  
fn get_portal_address() -> EthAddress {  
    context.this_portal_address()  
}  
  
#[aztec(public)]  
fn get_portal_address_public() -> EthAddress {  
    context.this_portal_address()  
}
```

and change the call to `get_portal_address`

```
- let portal_address = get_portal_address(contract_address);  
+ let portal_address = SomeContract::at(contract_address).get_portal_address().call(&mut context);
```

## [Aztec.nr] Required gas limits for public-to-public callsWhen calling a public function from another public function using the `call_public_function` method, you must now specify how much gas you're allocating to the nested call. This will later allow you to limit the amount of gas consumed by the nested call, and handle any out of gas errors.

Note that gas limits are not yet enforced. For now, it is suggested you use `dep::aztec::context::gas::GasOpts::default()` which will forward all available gas.

```
+ use dep::aztec::context::gas::GasOpts;  
  
- context.call_public_function(target_contract, target_selector, args);  
+ context.call_public_function(target_contract, target_selector, args, GasOpts::default());
```

Note that this is not required when enqueuing a public function from a private one, since top-level enqueued public functions will always consume all gas available for the transaction, as it is not possible to handle any out-of-gas errors.

## [Aztec.nr] Emitting unencrypted logsThe `emit_unencrypted_logs` function is now a context method.

```
- use dep::aztec::log::emit_unencrypted_log;  
- use dep::aztec::log::emit_unencrypted_log_from_private;  
  
- emit_unencrypted_log(context, log1);  
- emit_unencrypted_log_from_private(context, log2);  
+ context.emit_unencrypted_log(log1);  
+ context.emit_unencrypted_log(log2);
```

## 0.33## [Aztec.nr] Storage struct annotationThe storage struct now identified by the annotation `#[aztec(storage)]`, instead of having to rely on it being called `Storage`.

```
- struct Storage {  
-    ...  
- }  
+ #[aztec(storage)]  
+ struct MyStorageStruct {  
+    ...  
+ }
```

## [Aztec.js] Storage layout and note infoStorage layout and note information are now exposed in the TS contract artifact

```
- const note = new Note([new Fr(mintAmount), secretHash]);  
- const pendingShieldStorageSlot = new Fr(5n); // storage slot for pending_shields  
- const noteTypeId = new Fr(84114971101151129711410111011678111116101n); // note type id for TransparentNote  
- const extendedNote = new ExtendedNote(  
-   note,  
-   admin.address,  
-   token.address,  
-   pendingShieldStorageSlot,  
-   noteTypeId,  
-   receipt.txHash,  
- );  
- await pxe.addNote(extendedNote);  
+ const note = new Note([new Fr(mintAmount), secretHash]);  
+ const extendedNote = new ExtendedNote(  
+   note,  
+   admin.address,  
+   token.address,  
+   TokenContract.storage.pending_shields.slot,  
+   TokenContract.notes.TransparentNote.id,  
+   receipt.txHash,  
+ );  
+ await pxe.addNote(extendedNote);
```

## [Aztec.nr] rand oracle is now called unsafe\_rand`oracle::rand::rand` has been renamed to `oracle::unsafe_rand::unsafe_rand`.
This change was made to communicate that we do not constrain the value in circuit and instead we just trust our PXE.

```
- let random_value = rand();  
+ let random_value = unsafe_rand();
```

## [AztecJS] Simulate and get return values for ANY call and introducing `prove()`Historically it have been possible to "view" `unconstrained` functions to simulate them and get the return values, but not for `public` nor `private` functions.
This has lead to a lot of bad code where we have the same function implemented thrice, once in `private`, once in `public` and once in `unconstrained`.
It is not possible to call `simulate` on any call to get the return values!
However, beware that it currently always returns a Field array of size 4 for private and public.
This will change to become similar to the return values of the `unconstrained` functions with proper return types.

```
-    #[aztec(private)]  
-    fn get_shared_immutable_constrained_private() -> pub Leader {  
-        storage.shared_immutable.read_private()  
-    }  
-  
-    unconstrained fn get_shared_immutable() -> pub Leader {  
-        storage.shared_immutable.read_public()  
-    }  
  
+    #[aztec(private)]  
+    fn get_shared_immutable_private() -> pub Leader {  
+        storage.shared_immutable.read_private()  
+    }  
  
- const returnValues = await contract.methods.get_shared_immutable().view();  
+ const returnValues = await contract.methods.get_shared_immutable_private().simulate();
```

```
await expect(  
-   asset.withWallet(wallets[1]).methods.update_admin(newAdminAddress).simulate()).rejects.toThrow(  
+   asset.withWallet(wallets[1]).methods.update_admin(newAdminAddress).prove()).rejects.toThrow(  
        "Assertion failed: caller is not admin 'caller_roles.is_admin'",  
);
```

## 0.31.0## [Aztec.nr] Public storage historical read API improvement`history::public_value_inclusion::prove_public_value_inclusion` has been renamed to `history::public_storage::public_storage_historical_read`, and its API changed slightly. Instead of receiving a `value` parameter it now returns the historical value stored at that slot.

If you were using an oracle to get the value to pass to `prove_public_value_inclusion`, drop the oracle and use the return value from `public_storage_historical_read` instead:

```
- let value = read_storage();  
- prove_public_value_inclusion(value, storage_slot, contract_address, context);  
+ let value = public_storage_historical_read(storage_slot, contract_address, context);
```

If you were proving historical existence of a value you got via some other constrained means, perform an assertion against the return value of `public_storage_historical_read` instead:

```
- prove_public_value_inclusion(value, storage_slot, contract_address, context);  
+ assert(public_storage_historical_read(storage_slot, contract_address, context) == value);
```

## 0.30.0## [AztecJS] Simplify authwit syntax```
- const messageHash = computeAuthWitMessageHash(accounts[1].address, action.request());  
- await wallets[0].setPublicAuth(messageHash, true).send().wait();  
+ await wallets[0].setPublicAuthWit({ caller: accounts[1].address, action }, true).send().wait();
```

```
const action = asset  
    .withWallet(wallets[1])  
    .methods.unshield(accounts[0].address, accounts[1].address, amount, nonce);  
-const messageHash = computeAuthWitMessageHash(accounts[1].address, action.request());  
-const witness = await wallets[0].createAuthWitness(messageHash);  
+const witness = await wallets[0].createAuthWit({ caller: accounts[1].address, action });  
await wallets[1].addAuthWitness(witness);
```

Also note some of the naming changes:
`setPublicAuth` -> `setPublicAuthWit`
`createAuthWitness` -> `createAuthWit`

## [Aztec.nr] Automatic NoteInterface implementation and selector changesImplementing a note required a fair amount of boilerplate code, which has been substituted by the `#[aztec(note)]` attribute.

```
+ #[aztec(note)]  
struct AddressNote {  
    address: AztecAddress,  
    owner: AztecAddress,  
    randomness: Field,  
    header: NoteHeader  
}  
  
impl NoteInterface<ADDRESS_NOTE_LEN>  for AddressNote {  
-    fn serialize_content(self) -> [Field; ADDRESS_NOTE_LEN]{  
-        [self.address.to_field(), self.owner.to_field(), self.randomness]  
-    }  
-  
-    fn deserialize_content(serialized_note: [Field; ADDRESS_NOTE_LEN]) -> Self {  
-        AddressNote {  
-            address: AztecAddress::from_field(serialized_note[0]),  
-            owner: AztecAddress::from_field(serialized_note[1]),  
-            randomness: serialized_note[2],  
-            header: NoteHeader::empty(),  
-        }  
-    }  
-  
-    fn compute_note_content_hash(self) -> Field {  
-        pedersen_hash(self.serialize_content(), 0)  
-    }  
-  
    fn compute_nullifier(self, context: &mut PrivateContext) -> Field {  
        let note_hash_for_nullify = compute_note_hash_for_consumption(self);  
        let secret = context.request_nullifier_secret_key(self.owner);  
        pedersen_hash([  
            note_hash_for_nullify,  
            secret.low,  
            secret.high,  
        ],0)  
    }  
  
    fn compute_nullifier_without_context(self) -> Field {  
        let note_hash_for_nullify = compute_note_hash_for_consumption(self);  
        let secret = get_nullifier_secret_key(self.owner);  
        pedersen_hash([  
            note_hash_for_nullify,  
            secret.low,  
            secret.high,  
        ],0)  
    }  
  
-    fn set_header(&mut self, header: NoteHeader) {  
-        self.header = header;  
-    }  
-  
-    fn get_header(note: Self) -> NoteHeader {  
-        note.header  
-    }  
  
    fn broadcast(self, context: &mut PrivateContext, slot: Field) {  
        let encryption_pub_key = get_public_key(self.owner);  
        emit_encrypted_log(  
            context,  
            (*context).this_address(),  
            slot,  
            Self::get_note_type_id(),  
            encryption_pub_key,  
            self.serialize_content(),  
        );  
    }  
  
-    fn get_note_type_id() -> Field {  
-        6510010011410111511578111116101  
-    }  
}
```

Automatic note (de)serialization implementation also means it is now easier to filter notes using `NoteGetterOptions.select` via the `::properties()` helper:

Before:

```
let options = NoteGetterOptions::new().select(0, amount, Option::none()).select(1, owner.to_field(), Option::none()).set_limit(1);
```

After:

```
let options = NoteGetterOptions::new().select(ValueNote::properties().value, amount, Option::none()).select(ValueNote::properties().owner, owner.to_field(), Option::none()).set_limit(1);
```

The helper returns a metadata struct that looks like this (if autogenerated)

```
ValueNoteProperties {  
    value: PropertySelector { index: 0, offset: 0, length: 32 },  
    owner: PropertySelector { index: 1, offset: 0, length: 32 },  
    randomness: PropertySelector { index: 2, offset: 0, length: 32 },  
}
```

It can also be used for the `.sort` method.

## 0.27.0## `initializer` macro replaces `constructor`Before this version, every contract was required to have exactly one `constructor` private function, that was used for deployment. We have now removed this requirement, and made `constructor` a function like any other.

To signal that a function can be used to **initialize** a contract, you must now decorate it with the `#[aztec(initializer)]` attribute. Initializers are regular functions that set an "initialized" flag (a nullifier) for the contract. A contract can only be initialized once, and contract functions can only be called after the contract has been initialized, much like a constructor. However, if a contract defines no initializers, it can be called at any time. Additionally, you can define as many initializer functions in a contract as you want, both private and public.

To migrate from current code, simply add an initializer attribute to your constructor functions.

```
+ #[aztec(initializer)]  
#[aztec(private)]  
fn constructor() { ... }
```

If your private constructor was used to just call a public internal initializer, then remove the private constructor and flag the public function as initializer. And if your private constructor was an empty one, just remove it.

## 0.25.0## [Aztec.nr] Static callsIt is now possible to perform static calls from both public and private functions. Static calls forbid any modification to the state, including L2->L1 messages or log generation. Once a static context is set through a static all, every subsequent call will also be treated as static via context propagation.

```
context.static_call_private_function(targetContractAddress, targetSelector, args);  
  
context.static_call_public_function(targetContractAddress, targetSelector, args);
```

## [Aztec.nr] Introduction to `prelude`A new `prelude` module to include common Aztec modules and types.
This simplifies dependency syntax. For example:

```
use dep::aztec::protocol_types::address::AztecAddress;  
use dep::aztec::{  
    context::{PrivateContext, Context}, note::{note_header::NoteHeader, utils as note_utils},  
    state_vars::Map  
};
```

Becomes:

```
use dep::aztec::prelude::{AztecAddress, NoteHeader, PrivateContext, Map};  
use dep::aztec::context::Context;  
use dep::aztec::notes::utils as note_utils;
```

This will be further simplified in future versions (See [4496](https://github.com/AztecProtocol/aztec-packages/pull/4496) for further details).

The prelude consists of

[Edit: removed because the prelude no-longer exists]

## `internal` is now a macroThe `internal` keyword is now removed from Noir, and is replaced by an `aztec(internal)` attribute in the function. The resulting behavior is exactly the same: these functions will only be callable from within the same contract.

Before:

```
#[aztec(private)]  
internal fn double(input: Field) -> Field {  
    input * 2  
}
```

After:

```
#[aztec(private)]  
#[aztec(internal)]  
fn double(input: Field) -> Field {  
    input * 2  
}
```

## [Aztec.nr] No SafeU120 anymore!Noir now have overflow checks by default. So we don't need SafeU120 like libraries anymore.

You can replace it with `U128` instead

Before:

```
SafeU120::new(0)
```

Now:

```
U128::from_integer(0)
```

## [Aztec.nr] `compute_note_hash_and_nullifier` is now autogeneratedHistorically developers have been required to include a `compute_note_hash_and_nullifier` function in each of their contracts. This function is now automatically generated, and all instances of it in contract code can be safely removed.

It is possible to provide a user-defined implementation, in which case auto-generation will be skipped (though there are no known use cases for this).

## [Aztec.nr] Updated naming of state variable wrappersWe have decided to change the naming of our state variable wrappers because the naming was not clear.
The changes are as follows:

1. `Singleton` -> `PrivateMutable`
2. `ImmutableSingleton` -> `PrivateImmutable`
3. `StablePublicState` -> `SharedImmutable`
4. `PublicState` -> `PublicMutable`

This is the meaning of "private", "public" and "shared":
Private: read (R) and write (W) from private, not accessible from public
Public: not accessible from private, R/W from public
Shared: R from private, R/W from public

Note: `SlowUpdates` will be renamed to `SharedMutable` once the implementation is ready.

## [Aztec.nr] Authwit updatesAuthentication Witnesses have been updates such that they are now cancellable and scoped to a specific consumer.
This means that the `authwit` nullifier must be emitted from the account contract, which require changes to the interface.
Namely, the `assert_current_call_valid_authwit_public` and `assert_current_call_valid_authwit` in `auth.nr` will **NO LONGER** emit a nullifier.
Instead it will call a `spend_*_authwit` function in the account contract - which will emit the nullifier and perform a few checks.
This means that the `is_valid` functions have been removed to not confuse it for a non-mutating function (static).
Furthermore, the `caller` parameter of the "authwits" have been moved "further out" such that the account contract can use it in validation, allowing scoped approvals from the account POV.
For most contracts, this won't be changing much, but for the account contract, it will require a few changes.

Before:

```
#[aztec(public)]  
fn is_valid_public(message_hash: Field) -> Field {  
    let actions = AccountActions::public(&mut context, ACCOUNT_ACTIONS_STORAGE_SLOT, is_valid_impl);  
    actions.is_valid_public(message_hash)  
}  
  
#[aztec(private)]  
fn is_valid(message_hash: Field) -> Field {  
    let actions = AccountActions::private(&mut context, ACCOUNT_ACTIONS_STORAGE_SLOT, is_valid_impl);  
    actions.is_valid(message_hash)  
}
```

After:

```
#[aztec(private)]  
fn verify_private_authwit(inner_hash: Field) -> Field {  
    let actions = AccountActions::private(&mut context, ACCOUNT_ACTIONS_STORAGE_SLOT, is_valid_impl);  
    actions.verify_private_authwit(inner_hash)  
}  
  
#[aztec(public)]  
fn spend_public_authwit(inner_hash: Field) -> Field {  
    let actions = AccountActions::public(&mut context, ACCOUNT_ACTIONS_STORAGE_SLOT, is_valid_impl);  
    actions.spend_public_authwit(inner_hash)  
}
```

## 0.24.0## Introduce Note Type IDsNote Type IDs are a new feature which enable contracts to have multiple `Map`s with different underlying note types, something that was not possible before. This is done almost without any user intervention, though some minor changes are required.

The mandatory `compute_note_hash_and_nullifier` now has a fifth parameter `note_type_id`. Use this instead of `storage_slot` to determine which deserialization function to use.

Before:

```
unconstrained fn compute_note_hash_and_nullifier(  
    contract_address: AztecAddress,  
    nonce: Field,  
    storage_slot: Field,  
    preimage: [Field; TOKEN_NOTE_LEN]  
) -> pub [Field; 4] {  
    let note_header = NoteHeader::new(contract_address, nonce, storage_slot);  
  
    if (storage_slot == storage.pending_shields.get_storage_slot()) {  
        note_utils::compute_note_hash_and_nullifier(TransparentNote::deserialize_content, note_header, preimage)  
    } else if (note_type_id == storage.slow_update.get_storage_slot()) {  
        note_utils::compute_note_hash_and_nullifier(FieldNote::deserialize_content, note_header, preimage)  
    } else {  
        note_utils::compute_note_hash_and_nullifier(TokenNote::deserialize_content, note_header, preimage)  
    }
```

Now:

```
unconstrained fn compute_note_hash_and_nullifier(  
    contract_address: AztecAddress,  
    nonce: Field,  
    storage_slot: Field,  
    note_type_id: Field,  
    preimage: [Field; TOKEN_NOTE_LEN]  
) -> pub [Field; 4] {  
    let note_header = NoteHeader::new(contract_address, nonce, storage_slot);  
  
    if (note_type_id == TransparentNote::get_note_type_id()) {  
        note_utils::compute_note_hash_and_nullifier(TransparentNote::deserialize_content, note_header, preimage)  
    } else if (note_type_id == FieldNote::get_note_type_id()) {  
        note_utils::compute_note_hash_and_nullifier(FieldNote::deserialize_content, note_header, preimage)  
    } else {  
        note_utils::compute_note_hash_and_nullifier(TokenNote::deserialize_content, note_header, preimage)  
    }
```

The `NoteInterface` trait now has an additional `get_note_type_id()` function. This implementation will be autogenerated in the future, but for now providing any unique ID will suffice. The suggested way to do it is by running the Python command shown in the comment below:

```
impl NoteInterface<N> for MyCustomNote {  
    fn get_note_type_id() -> Field {  
        // python -c "print(int(''.join(str(ord(c)) for c in 'MyCustomNote')))"  
       771216711711511611110978111116101  
    }  
}
```

## [js] Importing contracts in JS`@aztec/noir-contracts` is now `@aztec/noir-contracts.js`. You'll need to update your package.json & imports.

Before:

```
import { TokenContract } from "@aztec/noir-contracts/Token";
```

Now:

```
import { TokenContract } from "@aztec/noir-contracts.js/Token";
```

## [Aztec.nr] Aztec.nr contracts location change in Nargo.tomlAztec contracts are now moved outside of the `yarn-project` folder and into `noir-projects`, so you need to update your imports.

Before:

```
easy_private_token_contract = {git = "https://github.com/AztecProtocol/aztec-packages/", tag ="v0.23.0", directory = "yarn-project/noir-contracts/contracts/easy_private_token_contract"}
```

Now, update the `yarn-project` folder for `noir-projects`:

```
easy_private_token_contract = {git = "https://github.com/AztecProtocol/aztec-packages/", tag ="v0.24.0", directory = "noir-projects/noir-contracts/contracts/easy_private_token_contract"}
```

## 0.22.0## `Note::compute_note_hash` renamed to `Note::compute_note_content_hash`The `compute_note_hash` function in of the `Note` trait has been renamed to `compute_note_content_hash` to avoid being confused with the actual note hash.

Before:

```
impl NoteInterface for CardNote {  
    fn compute_note_hash(self) -> Field {  
        pedersen_hash([  
            self.owner.to_field(),  
        ], 0)  
    }
```

Now:

```
impl NoteInterface for CardNote {  
    fn compute_note_content_hash(self) -> Field {  
        pedersen_hash([  
            self.owner.to_field(),  
        ], 0)  
    }
```

## Introduce `compute_note_hash_for_consumption` and `compute_note_hash_for_insertion`Makes a split in logic for note hash computation for consumption and insertion. This is to avoid confusion between the two, and to make it clear that the note hash for consumption is different from the note hash for insertion (sometimes).

`compute_note_hash_for_consumption` replaces `compute_note_hash_for_read_or_nullify`.
`compute_note_hash_for_insertion` is new, and mainly used in `lifecycle.nr``

## `Note::serialize_content` and `Note::deserialize_content` added to `NoteInterfaceThe `NoteInterface` have been extended to include `serialize_content` and `deserialize_content` functions. This is to convey the difference between serializing the full note, and just the content. This change allows you to also add a `serialize` function to support passing in a complete note to a function.

Before:

```
impl Serialize<ADDRESS_NOTE_LEN> for AddressNote {  
    fn serialize(self) -> [Field; ADDRESS_NOTE_LEN]{  
        [self.address.to_field(), self.owner.to_field(), self.randomness]  
    }  
}  
impl Deserialize<ADDRESS_NOTE_LEN> for AddressNote {  
    fn deserialize(serialized_note: [Field; ADDRESS_NOTE_LEN]) -> Self {  
        AddressNote {  
            address: AztecAddress::from_field(serialized_note[0]),  
            owner: AztecAddress::from_field(serialized_note[1]),  
            randomness: serialized_note[2],  
            header: NoteHeader::empty(),  
        }  
    }
```

Now

```
impl NoteInterface<ADDRESS_NOTE_LEN>  for AddressNote {  
    fn serialize_content(self) -> [Field; ADDRESS_NOTE_LEN]{  
        [self.address.to_field(), self.owner.to_field(), self.randomness]  
    }  
  
    fn deserialize_content(serialized_note: [Field; ADDRESS_NOTE_LEN]) -> Self {  
        AddressNote {  
            address: AztecAddress::from_field(serialized_note[0]),  
            owner: AztecAddress::from_field(serialized_note[1]),  
            randomness: serialized_note[2],  
            header: NoteHeader::empty(),  
        }  
    }  
    ...  
}
```

## [Aztec.nr] No storage.init() and `Serialize`, `Deserialize`, `NoteInterface` as Traits, removal of SerializationMethods and SERIALIZED\_LENStorage definition and initialization has been simplified. Previously:

```
struct Storage {  
    leader: PublicState<Leader, LEADER_SERIALIZED_LEN>,  
    legendary_card: Singleton<CardNote, CARD_NOTE_LEN>,  
    profiles: Map<AztecAddress, Singleton<CardNote, CARD_NOTE_LEN>>,  
    test: Set<CardNote, CARD_NOTE_LEN>,  
    imm_singleton: PrivateImmutable<CardNote, CARD_NOTE_LEN>,  
}  
  
impl Storage {  
        fn init(context: Context) -> Self {  
            Storage {  
                leader: PublicMutable::new(  
                    context,  
                    1,  
                    LeaderSerializationMethods,  
                ),  
                legendary_card: PrivateMutable::new(context, 2, CardNoteMethods),  
                profiles: Map::new(  
                    context,  
                    3,  
                    |context, slot| {  
                        PrivateMutable::new(context, slot, CardNoteMethods)  
                    },  
                ),  
                test: Set::new(context, 4, CardNoteMethods),  
                imm_singleton: PrivateImmutable::new(context, 4, CardNoteMethods),  
            }  
        }  
    }
```

Now:

```
struct Storage {  
    leader: PublicMutable<Leader>,  
    legendary_card: Singleton<CardNote>,  
    profiles: Map<AztecAddress, Singleton<CardNote>>,  
    test: Set<CardNote>,  
    imm_singleton: PrivateImmutable<CardNote>,  
}
```

For this to work, Notes must implement Serialize, Deserialize and NoteInterface Traits. Previously:

```
use dep::aztec::protocol_types::address::AztecAddress;  
use dep::aztec::{  
    note::{  
        note_header::NoteHeader,  
        note_interface::NoteInterface,  
        utils::compute_note_hash_for_read_or_nullify,  
    },  
    oracle::{  
        nullifier_key::get_nullifier_secret_key,  
        get_public_key::get_public_key,  
    },  
    log::emit_encrypted_log,  
    hash::pedersen_hash,  
    context::PrivateContext,  
};  
  
// Shows how to create a custom note  
  
global CARD_NOTE_LEN: Field = 1;  
  
impl CardNote {  
    pub fn new(owner: AztecAddress) -> Self {  
        CardNote {  
            owner,  
        }  
    }  
  
    pub fn serialize(self) -> [Field; CARD_NOTE_LEN] {  
        [self.owner.to_field()]  
    }  
  
    pub fn deserialize(serialized_note: [Field; CARD_NOTE_LEN]) -> Self {  
        CardNote {  
            owner: AztecAddress::from_field(serialized_note[1]),  
        }  
    }  
  
    pub fn compute_note_hash(self) -> Field {  
        pedersen_hash([  
            self.owner.to_field(),  
        ],0)  
    }  
  
    pub fn compute_nullifier(self, context: &mut PrivateContext) -> Field {  
        let note_hash_for_nullify = compute_note_hash_for_read_or_nullify(CardNoteMethods, self);  
        let secret = context.request_nullifier_secret_key(self.owner);  
        pedersen_hash([  
            note_hash_for_nullify,  
            secret.high,  
            secret.low,  
        ],0)  
    }  
  
    pub fn compute_nullifier_without_context(self) -> Field {  
        let note_hash_for_nullify = compute_note_hash_for_read_or_nullify(CardNoteMethods, self);  
        let secret = get_nullifier_secret_key(self.owner);  
        pedersen_hash([  
            note_hash_for_nullify,  
            secret.high,  
            secret.low,  
        ],0)  
    }  
  
    pub fn set_header(&mut self, header: NoteHeader) {  
        self.header = header;  
    }  
  
    // Broadcasts the note as an encrypted log on L1.  
    pub fn broadcast(self, context: &mut PrivateContext, slot: Field) {  
        let encryption_pub_key = get_public_key(self.owner);  
        emit_encrypted_log(  
            context,  
            (*context).this_address(),  
            slot,  
            encryption_pub_key,  
            self.serialize(),  
        );  
    }  
}  
  
fn deserialize(serialized_note: [Field; CARD_NOTE_LEN]) -> CardNote {  
    CardNote::deserialize(serialized_note)  
}  
  
fn serialize(note: CardNote) -> [Field; CARD_NOTE_LEN] {  
    note.serialize()  
}  
  
fn compute_note_hash(note: CardNote) -> Field {  
    note.compute_note_hash()  
}  
  
fn compute_nullifier(note: CardNote, context: &mut PrivateContext) -> Field {  
    note.compute_nullifier(context)  
}  
  
fn compute_nullifier_without_context(note: CardNote) -> Field {  
    note.compute_nullifier_without_context()  
}  
  
fn get_header(note: CardNote) -> NoteHeader {  
    note.header  
}  
  
fn set_header(note: &mut CardNote, header: NoteHeader) {  
    note.set_header(header)  
}  
  
// Broadcasts the note as an encrypted log on L1.  
fn broadcast(context: &mut PrivateContext, slot: Field, note: CardNote) {  
    note.broadcast(context, slot);  
}  
  
global CardNoteMethods = NoteInterface {  
    deserialize,  
    serialize,  
    compute_note_hash,  
    compute_nullifier,  
    compute_nullifier_without_context,  
    get_header,  
    set_header,  
    broadcast,  
};
```

Now:

```
use dep::aztec::{  
    note::{  
        note_header::NoteHeader,  
        note_interface::NoteInterface,  
        utils::compute_note_hash_for_read_or_nullify,  
    },  
    oracle::{  
        nullifier_key::get_nullifier_secret_key,  
        get_public_key::get_public_key,  
    },  
    log::emit_encrypted_log,  
    hash::pedersen_hash,  
    context::PrivateContext,  
    protocol_types::{  
        address::AztecAddress,  
        traits::{Serialize, Deserialize, Empty}  
    }  
};  
  
// Shows how to create a custom note  
  
global CARD_NOTE_LEN: Field = 1;  
  
impl CardNote {  
    pub fn new(owner: AztecAddress) -> Self {  
        CardNote {  
            owner,  
        }  
    }  
}  
  
impl NoteInterface for CardNote {  
    fn compute_note_content_hash(self) -> Field {  
        pedersen_hash([  
            self.owner.to_field(),  
        ],0)  
    }  
  
    fn compute_nullifier(self, context: &mut PrivateContext) -> Field {  
        let note_hash_for_nullify = compute_note_hash_for_read_or_nullify(self);  
        let secret = context.request_nullifier_secret_key(self.owner);  
        pedersen_hash([  
            note_hash_for_nullify,  
            secret.high,  
            secret.low,  
        ],0)  
    }  
  
    fn compute_nullifier_without_context(self) -> Field {  
        let note_hash_for_nullify = compute_note_hash_for_read_or_nullify(self);  
        let secret = get_nullifier_secret_key(self.owner);  
        pedersen_hash([  
            note_hash_for_nullify,  
            secret.high,  
            secret.low,  
        ],0)  
    }  
  
    fn set_header(&mut self, header: NoteHeader) {  
        self.header = header;  
    }  
  
    fn get_header(note: CardNote) -> NoteHeader {  
        note.header  
    }  
  
    fn serialize_content(self) -> [Field; CARD_NOTE_LEN]{  
        [self.owner.to_field()]  
    }  
  
    fn deserialize_content(serialized_note: [Field; CARD_NOTE_LEN]) -> Self {  
        AddressNote {  
            owner: AztecAddress::from_field(serialized_note[0]),  
            header: NoteHeader::empty(),  
        }  
    }  
  
    // Broadcasts the note as an encrypted log on L1.  
    fn broadcast(self, context: &mut PrivateContext, slot: Field) {  
        let encryption_pub_key = get_public_key(self.owner);  
        emit_encrypted_log(  
            context,  
            (*context).this_address(),  
            slot,  
            encryption_pub_key,  
            self.serialize(),  
        );  
    }  
}
```

Public state must implement Serialize and Deserialize traits.

It is still possible to manually implement the storage initialization (for custom storage wrappers or internal types that don't implement the required traits). For the above example, the `impl Storage` section would look like this:

```
impl Storage {  
    fn init(context: Context) -> Self {  
        Storage {  
            leader: PublicMutable::new(  
                context,  
                1  
            ),  
            legendary_card: PrivateMutable::new(context, 2),  
            profiles: Map::new(  
                context,  
                3,  
                |context, slot| {  
                    PrivateMutable::new(context, slot)  
                },  
            ),  
            test: Set::new(context, 4),  
            imm_singleton: PrivateImmutable::new(context, 4),  
        }  
    }  
}
```

## 0.20.0## [Aztec.nr] Changes to `NoteInterface`1. Changing `compute_nullifier()` to `compute_nullifier(private_context: PrivateContext)`

   This API is invoked for nullifier generation within private functions. When using a secret key for nullifier creation, retrieve it through:

   `private_context.request_nullifier_secret_key(account_address)`

   The private context will generate a request for the kernel circuit to validate that the secret key does belong to the account.

   Before:

   ```
    pub fn compute_nullifier(self) -> Field {  
        let secret = oracle.get_secret_key(self.owner);  
        pedersen_hash([  
            self.value,  
            secret.low,  
            secret.high,  
        ])  
    }
   ```

   Now:

   ```
    pub fn compute_nullifier(self, context: &mut PrivateContext) -> Field {  
        let secret = context.request_nullifier_secret_key(self.owner);  
        pedersen_hash([  
            self.value,  
            secret.low,  
            secret.high,  
        ])  
    }
   ```
2. New API `compute_nullifier_without_context()`.

   This API is used within unconstrained functions where the private context is not available, and using an unverified nullifier key won't affect the network or other users. For example, it's used in `compute_note_hash_and_nullifier()` to compute values for the user's own notes.

   ```
   pub fn compute_nullifier_without_context(self) -> Field {  
        let secret = oracle.get_nullifier_secret_key(self.owner);  
        pedersen_hash([  
            self.value,  
            secret.low,  
            secret.high,  
        ])  
    }
   ```

   > Note that the `get_secret_key` oracle API has been renamed to `get_nullifier_secret_key`.

## 0.18.0## [Aztec.nr] Remove `protocol_types` from Nargo.tomlThe `protocol_types` package is now being reexported from `aztec`. It can be accessed through `dep::aztec::protocol_types`.

```
aztec = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="yarn-project/aztec-nr/aztec" }
```

## [Aztec.nr] key type definition in MapThe `Map` class now requires defining the key type in its declaration which *must* implement the `ToField` trait.

Before:

```
struct Storage {  
    balances: Map<PublicMutable<Field, FIELD_SERIALIZED_LEN>>  
}  
  
let user_balance = balances.at(owner.to_field())
```

Now:

```
struct Storage {  
    balances: Map<AztecAddress, PublicState<Field, FIELD_SERIALIZED_LEN>>  
}  
  
let user_balance = balances.at(owner)
```

## [js] Updated function names* `waitForSandbox` renamed to `waitForPXE` in `@aztec/aztec.js`
* `getSandboxAccountsWallets` renamed to `getInitialTestAccountsWallets` in `@aztec/accounts/testing`

## 0.17.0## [js] New `@aztec/accounts` packageBefore:

```
import { getSchnorrAccount } from "@aztec/aztec.js"; // previously you would get the default accounts from the `aztec.js` package:
```

Now, import them from the new package `@aztec/accounts`

```
import { getSchnorrAccount } from "@aztec/accounts";
```

## Typed AddressesAddress fields in Aztec.nr now is of type `AztecAddress` as opposed to `Field`

Before:

```
unconstrained fn compute_note_hash_and_nullifier(contract_address: Field, nonce: Field, storage_slot: Field, serialized_note: [Field; VALUE_NOTE_LEN]) -> [Field; 4] {  
        let note_header = NoteHeader::new(_address, nonce, storage_slot);  
        ...
```

Now:

```
unconstrained fn compute_note_hash_and_nullifier(  
        contract_address: AztecAddress,  
        nonce: Field,  
        storage_slot: Field,  
        serialized_note: [Field; VALUE_NOTE_LEN]  
    ) -> pub [Field; 4] {  
        let note_header = NoteHeader::new(contract_address, nonce, storage_slot);
```

Similarly, there are changes when using aztec.js to call functions.

To parse a `AztecAddress` to BigInt, use `.inner`
Before:

```
const tokenBigInt = await bridge.methods.token().simulate();
```

Now:

```
const tokenBigInt = (await bridge.methods.token().simulate()).inner;
```

## [Aztec.nr] Add `protocol_types` to Nargo.toml```
aztec = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="yarn-project/aztec-nr/aztec" }  
protocol_types = { git="https://github.com/AztecProtocol/aztec-packages/", tag="v3.0.0-devnet.20251212", directory="yarn-project/noir-protocol-circuits/crates/types"}
```

## [Aztec.nr] moving compute\_address func to AztecAddressBefore:

```
let calculated_address = compute_address(pub_key_x, pub_key_y, partial_address);
```

Now:

```
let calculated_address = AztecAddress::compute(pub_key_x, pub_key_y, partial_address);
```

## [Aztec.nr] moving `compute_selector` to FunctionSelectorBefore:

```
let selector = compute_selector("_initialize((Field))");
```

Now:

```
let selector = FunctionSelector::from_signature("_initialize((Field))");
```

## [js] Importing contracts in JSContracts are now imported from a file with the type's name.

Before:

```
import { TokenContract } from "@aztec/noir-contracts/types";
```

Now:

```
import { TokenContract } from "@aztec/noir-contracts/Token";
```

## [Aztec.nr] Aztec example contracts location change in Nargo.tomlAztec contracts are now moved outside of the `src` folder, so you need to update your imports.

Before:

```
easy_private_token_contract = {git = "https://github.com/AztecProtocol/aztec-packages/", tag ="v0.16.9", directory = "noir-projects/noir-contracts/contracts/easy_private_token_contract"}
```

Now, just remove the `src` folder,:

```
easy_private_token_contract = {git = "https://github.com/AztecProtocol/aztec-packages/", tag ="v0.17.0", directory = "noir-projects/noir-contracts/contracts/easy_private_token_contract"}
```

---


# Aztec Connect Sunset

Source: https://docs.aztec.network/aztec_connect_sunset

On this page

Deprecated

Aztec Connect is no longer being actively developed.

The rollup instance operated by Aztec stopped accepting deposits on March 21st, 2023. Read the full announcement [here](https://medium.com/aztec-protocol/sunsetting-aztec-connect-a786edce5cae).

We will continue to process transactions and withdrawals for funds that are already in the rollup until March 31st, 2024, at which point we will stop running the sequencer. Users should withdraw funds immediately. See the [zk.money](#zkmoney) section below for details on how to withdraw funds.

## Run your own ACAll of the infrastructure and associated code required to run and interact with the Aztec Connect rollup is open source, so anyone can publish blocks after we stop, or run their own instance of the rollup software.

You can find the old documentation site that includes all of the pertinent information on the [`aztec-connect` branch](https://github.com/AztecProtocol/docs/tree/aztec-connect) of the docs repository.

The code has been open sourced and you can find the relevant repositories linked below.

## Source CodeFollow the links for more information about each package.

* [Running the rollup service](https://github.com/AztecProtocol/aztec-connect/blob/master/yarn-project/README.md)
* [Sequencer](https://github.com/AztecProtocol/aztec-connect/tree/master/yarn-project/falafel)
* [Contracts](https://github.com/AztecProtocol/aztec-connect/tree/master/contracts)
* [SDK](https://github.com/AztecProtocol/aztec-connect/tree/master/yarn-project/sdk)
* [Block Explorer](https://github.com/AztecProtocol/aztec-connect-explorer)
* [Alpha SDK](https://github.com/AztecProtocol/aztec-connect/tree/master/yarn-project/alpha-sdk)
* [Wallet UI](https://github.com/AztecProtocol/wallet-ui)

## Zk.money## Exiting Defi Positions1. Navigate to your zk.money homepage and click “Wallet”.
2. Scroll down to “Tokens” and “Earn Positions”.
3. Click “Earn Positions”.
4. Click “Claim & Exit” on the position you wish to exit.

   ![](/assets/ideal-img/defiexit1.4a5220c.640.png)
5. All exit transactions are free in “Batched Mode” proceed to step 6 to get a free transaction.
6. Click “Max” to exit the full amount, and then select a speed for your transaction.

   ![](/assets/ideal-img/defiexit2.a772f8c.640.png)
7. Once you have done so, click “Next”.
8. Review the amount you will receive is correct, tick the disclaimer, and click “Confirm Transaction”.

   ![](/assets/ideal-img/defiexit3.e83484b.640.png)
9. After clicking confirm transaction, sign the signature request using your connected wallet (e.g. Metamask in this example).

   ![](/assets/ideal-img/defiexit4.c2308cb.640.png)
10. Wait until your transaction is confirmed.

    ![](/assets/ideal-img/defiexit5.e8cdc83.640.png)
11. Navigate back to your wallet homepage and click “Earn Positions”.
12. The status of your exit will be displayed here, as shown by “Exiting” (1 tick).

    ![](/assets/ideal-img/defiexit6.7f17c55.640.png)
13. To the left, click the transaction hash icon to be taken to the block explorer page to see the transaction status.

    ![](/assets/ideal-img/defiexit7.84f2a41.640.png)
14. Your funds will appear in your dashboard once the transaction has settled.

## Exiting LUSD BorrowingYour LUSD debt is repaid using a flash loan. Part of your ETH collateral then repays the flash loan, and the remaining ETH is returned to your account. Your total TB-275 tokens represents the entirety of your share of the collateral. Spending all your TB-275 will release your entire share of the collateral (minus the market value of the debt to be repaid).

Liquity: <https://docs.liquity.org/>

1. Navigate to your zk.money homepage and click “Wallet”.
2. Scroll down to “Tokens” and “Earn Positions”.
3. Click “Earn Positions”.
4. On your Liquity Trove position, click “Repay & Exit”.

   ![](/assets/ideal-img/lusdexit1.8799f9b.640.png)
5. Click “Max” to exit the full amount, then select a speed for your transaction.

   ![](/assets/ideal-img/lusdexit2.6aa73a9.640.png)
6. Once you have done so, click “Next”.
7. Review the amount you will receive is correct, tick the disclaimer, and click “Confirm Transaction”.

   ![](/assets/ideal-img/lusdexit3.70935ae.640.png)
8. After clicking confirm transaction, sign the signature request using your connected wallet (e.g. Metamask).
9. Wait until your transaction is confirmed.
10. Navigate to your zk.money wallet homepage and click “Earn Positions”.
11. The status of your exit will be displayed here, as shown by “Exiting” (1 tick).

    ![](/assets/ideal-img/lusdexit4.4bd5b39.640.png)
12. Click the transaction hash icon to be taken to the block explorer page to see the transaction status.

    ![](/assets/ideal-img/lusdexit5.4de033e.640.png)
13. Your funds will appear in your dashboard once the transaction has settled.

## Withdrawing AssetsHow to withdraw ETH, DAI and LUSD.

1. Navigate to your zk.money homepage and click “Wallet”.
2. Scroll down to “Tokens” and “Earn Positions”.
3. Click “Tokens”.
4. Click “Exit” on the desired token you would like to withdraw.

   ![](/assets/ideal-img/withdraw1.1202e0c.640.png)
5. Click “Withdraw to L1”.

   ![](/assets/ideal-img/withdraw2.480879c.640.png)
6. Enter your recipient address.
7. Click “Max” to withdraw the full amount.
8. Select a speed for your transaction (transactions are free in “Batched Mode”).
9. Click “Next”.
10. Review the amount you are withdrawing is correct, tick the disclaimer, and click “Confirm Transaction”.

    ![](/assets/ideal-img/withdraw3.516b724.640.png)
11. Sign the signature request using your connected wallet (e.g. Metamask).
12. Wait until your transaction is confirmed.

    ![](/assets/ideal-img/withdraw4.58b53e5.640.png)
13. Navigate back to your wallet homepage, under Transaction History. Click the transaction hash to check the status of your transaction on the block explorer.

    ![](/assets/ideal-img/withdraw5.a694eba.640.png)
14. Your funds will appear in your recipient wallet once the transaction has settled.

---


# Testnet (v2.1.4)

Source: https://docs.aztec.network/network/testnet

Version: Testnet (v2.1.4)

On this page

## OverviewThe Aztec network is a decentralized privacy-focused rollup on Ethereum. Network nodes work together to process transactions, maintain state, and generate proofs that ensure rollup integrity. This guide provides an overview of node types, their roles, best practices, and how to get started.

## Actors and RolesThe Aztec network consists of several types of actors, each serving a specific purpose:

## Full NodesFull nodes provide users with the ability to connect and interact with the network. They maintain a complete copy of the blockchain state and allow users to send and receive transactions without relying on third parties.

**Key responsibilities:**

* Maintain synchronized copy of the blockchain state
* Provide RPC interface for transaction submission
* Validate and relay transactions
* Offer privacy-preserving interaction with the network

[Learn more about running a full node →](/network/testnet/setup/running_a_node)

## Sequencer NodesSequencer nodes order transactions and produce blocks. Selected via a proof-of-stake mechanism, they play a critical role in the consensus process.

**Key responsibilities:**

* Assemble unprocessed transactions and propose new blocks
* Execute public functions in transactions
* Attest to correct execution when part of the sequencer committee
* Submit successfully attested blocks to L1

Before publication, blocks must be validated by a committee of sequencer nodes who re-execute public transactions and verify private function proofs. Committee members attest to validity by signing the block header. Once sufficient attestations are collected (two-thirds of the committee plus one), the block can be submitted to L1.

[Learn more about running a sequencer →](/network/testnet/setup/sequencer_management)

## ProversProvers generate cryptographic proofs that attest to transaction correctness. They produce the final rollup proof submitted to Ethereum, ensuring rollup integrity.

**Key components and responsibilities:**

* **Prover node**: Polls L1 for unproven epochs, creates prover jobs, and submits final proofs
* **Prover broker**: Manages job queues and distributes work to agents
* **Prover agents**: Execute proof generation jobs in a stateless manner

Note that running provers require:

* High-performance hardware (typically data center-grade)
* Significant computational resources for proof generation
* Technical expertise in operating distributed systems

[Learn more about running a prover →](/network/testnet/setup/running_a_prover)

## How Nodes Work TogetherThe Aztec network operates through the coordinated interaction of these different node types:

1. **Transaction Flow**: Users submit transactions to full nodes, which validate and propagate them through the P2P network
2. **Block Production**: Sequencer nodes collect transactions from the mempool, order them, and propose new blocks
3. **Consensus**: The sequencer committee validates proposed blocks and provides attestations
4. **Proof Generation**: Prover nodes generate cryptographic proofs for epochs of blocks
5. **L1 Submission**: Sequencers submit attested blocks and provers submit epoch proofs to Ethereum

## Using Your Own L1 NodeFor optimal performance and reliability, it's highly recommended to run your own Ethereum L1 node rather than relying on third-party RPC providers.

**Benefits:**

* Better performance and lower latency
* No rate limiting or request throttling
* Greater reliability and uptime control
* Enhanced privacy for your node operations

**Requirements:**

* Access to both execution and consensus client endpoints
* Endpoints must support high throughput
* Must be connected to Sepolia testnet for Aztec testnet

See [Eth Docker's guide](https://ethdocker.com/Usage/QuickStart) for setting up your own L1 node.

## Next Steps* **Check Prerequisites**: Review the [prerequisites guide](/network/testnet/prerequisites) to ensure you have everything needed
* **Run a Full Node**: Follow the [complete full node guide](/network/testnet/setup/running_a_node) for detailed setup instructions
* **Operate a Sequencer**: Learn how to [run a sequencer node](/network/testnet/setup/sequencer_management) and join the validator set
* **Operate a Prover**: Set up [prover infrastructure](/network/testnet/setup/running_a_prover) to generate rollup proofs
* **Join the Community**: Connect with other operators on [Discord](https://discord.gg/aztec)

---


# Ignition (v2.1.9)

Source: https://docs.aztec.network/network

Version: Ignition (v2.1.9)

On this page

## OverviewThe Aztec network is a decentralized privacy-focused rollup on Ethereum. Network nodes work together to process transactions, maintain state, and generate proofs that ensure rollup integrity. This guide provides an overview of node types, their roles, best practices, and how to get started.

## Actors and RolesThe Aztec network consists of several types of actors, each serving a specific purpose:

## Full NodesFull nodes provide users with the ability to connect and interact with the network. They maintain a complete copy of the blockchain state and allow users to send and receive transactions without relying on third parties.

**Key responsibilities:**

* Maintain synchronized copy of the blockchain state
* Provide RPC interface for transaction submission
* Validate and relay transactions
* Offer privacy-preserving interaction with the network

[Learn more about running a full node →](/network/setup/running_a_node)

## Sequencer NodesSequencer nodes order transactions and produce blocks. Selected via a proof-of-stake mechanism, they play a critical role in the consensus process.

**Key responsibilities:**

* Assemble unprocessed transactions and propose new blocks
* Execute public functions in transactions
* Attest to correct execution when part of the sequencer committee
* Submit successfully attested blocks to L1

Before publication, blocks must be validated by a committee of sequencer nodes who re-execute public transactions and verify private function proofs. Committee members attest to validity by signing the block header. Once sufficient attestations are collected (two-thirds of the committee plus one), the block can be submitted to L1.

[Learn more about running a sequencer →](/network/setup/sequencer_management)

## ProversProvers generate cryptographic proofs that attest to transaction correctness. They produce the final rollup proof submitted to Ethereum, ensuring rollup integrity.

**Key components and responsibilities:**

* **Prover node**: Polls L1 for unproven epochs, creates prover jobs, and submits final proofs
* **Prover broker**: Manages job queues and distributes work to agents
* **Prover agents**: Execute proof generation jobs in a stateless manner

Note that running provers require:

* High-performance hardware (typically data center-grade)
* Significant computational resources for proof generation
* Technical expertise in operating distributed systems

[Learn more about running a prover →](/network/setup/running_a_prover)

## How Nodes Work TogetherThe Aztec network operates through the coordinated interaction of these different node types:

1. **Transaction Flow**: Users submit transactions to full nodes, which validate and propagate them through the P2P network
2. **Block Production**: Sequencer nodes collect transactions from the mempool, order them, and propose new blocks
3. **Consensus**: The sequencer committee validates proposed blocks and provides attestations
4. **Proof Generation**: Prover nodes generate cryptographic proofs for epochs of blocks
5. **L1 Submission**: Sequencers submit attested blocks and provers submit epoch proofs to Ethereum

## Using Your Own L1 NodeFor optimal performance and reliability, it's highly recommended to run your own Ethereum L1 node rather than relying on third-party RPC providers.

**Benefits:**

* Better performance and lower latency
* No rate limiting or request throttling
* Greater reliability and uptime control
* Enhanced privacy for your node operations

**Requirements:**

* Access to both execution and consensus client endpoints
* Endpoints must support high throughput
* Must be connected to Ethereum Mainnet

See [Eth Docker's guide](https://ethdocker.com/Usage/QuickStart) for setting up your own L1 node.

## Next Steps* **Check Prerequisites**: Review the [prerequisites guide](/network/prerequisites) to ensure you have everything needed
* **Run a Full Node**: Follow the [complete full node guide](/network/setup/running_a_node) for detailed setup instructions
* **Operate a Sequencer**: Learn how to [run a sequencer node](/network/setup/sequencer_management) and join the validator set
* **Operate a Prover**: Set up [prover infrastructure](/network/setup/running_a_prover) to generate rollup proofs
* **Join the Community**: Connect with other operators on [Discord](https://discord.gg/aztec)

---


# Prerequisites

Source: https://docs.aztec.network/network/prerequisites

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide covers the prerequisites and setup requirements for running nodes on the Aztec network.

## Common PrerequisitesThe following prerequisites apply to all node types.

## Operating SystemThe node software can be run on any Unix system released after 2020.

* Linux (common flavors)
* MacOS (ARM and intel)

## Docker and Docker ComposeDocker and Docker Compose are required for all node types. All Aztec nodes run in Docker containers managed by Docker Compose.

**On Linux:** Install Docker Engine and Docker Compose separately:

1. Install Docker:

```
curl -fsSL https://get.docker.com -o get-docker.sh  
sudo sh get-docker.sh
```

2. Add your user to the docker group so `sudo` is not needed:

```
sudo groupadd docker  
sudo usermod -aG docker $USER  
newgrp docker  
# Test without sudo  
docker run hello-world
```

3. Install Docker Compose by following the [Docker Compose installation guide](https://docs.docker.com/compose/install/).

**On macOS:** Install [Docker Desktop](https://docs.docker.com/desktop/install/mac-install/), which includes both Docker and Docker Compose.

## Aztec ToolchainThe Aztec toolchain provides CLI utilities for key generation, validator registration, and other operational tasks. While not required for running nodes (which use Docker Compose), it is needed for:

* Generating validator keystores and creating staking registration data (`aztec validator-keys`)
* Registering sequencers on L1 (`aztec add-l1-validator`)

Install the Aztec toolchain using the official installer:

```
bash -i <(curl -s https://install.aztec.network)
```

Install the correct version for the current network:

```
aztec-up 2.1.9
```

## L1 Ethereum Node AccessAll Aztec nodes require access to Ethereum L1 node endpoints:

* **Execution client endpoint** (e.g., Geth, Nethermind, Besu, Erigon)
* **Consensus client endpoint** (e.g., Prysm, Lighthouse, Teku, Nimbus)

**Options:**

1. **Run your own L1 node** (recommended for best performance):

   * Better performance and lower latency
   * No rate limiting or request throttling
   * Greater reliability and uptime control
   * Enhanced privacy for your node operations
   * See [Eth Docker's guide](https://ethdocker.com/Usage/QuickStart) for setup instructions
2. **Use a third-party RPC provider**:

   * Easier to set up initially
   * May have rate limits and throttling
   * Ensure the provider supports beacon apis

High Throughput Required

Your L1 endpoints must support high throughput to avoid degraded node performance.

## Port Forwarding and ConnectivityFor nodes participating in the P2P network (full nodes, sequencers, provers), proper port configuration is essential:

**Required steps:**

1. Configure your router to forward both UDP and TCP traffic on your P2P port (default: 40400) to your node's local IP address
2. Ensure your firewall allows traffic on the required ports:
   * P2P port: 40400 (default, both TCP and UDP)
   * HTTP API port: 8080 (default)
3. Set the `P2P_IP` environment variable to your external IP address
4. Verify the P2P port is accessible from the internet

**Find your public IP address:**

```
curl ipv4.icanhazip.com
```

**Verify port connectivity:**

```
# For TCP traffic on port 40400  
nc -zv [YOUR_EXTERNAL_IP] 40400  
  
# For UDP traffic on port 40400  
nc -zuv [YOUR_EXTERNAL_IP] 40400
```

Port Forwarding Required

If port forwarding isn't properly configured, your node may not be able to participate in P2P duties.

## Next StepsOnce you have met the prerequisites, proceed to set up your desired node type:

* [Run a Full Node →](/network/setup/running_a_node)
* [Run a Sequencer Node →](/network/setup/sequencer_management)
* [Run a Prover Node →](/network/setup/running_a_prover)

---


# Running a Full Node

Source: https://docs.aztec.network/network/setup/running_a_node

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide covers the steps required to run a full node on Aztec using Docker Compose.

A full node allows you to connect and interact with the network, providing an interface to send and receive transactions and state updates without relying on third parties.

You should run your own full node if you want to interact with the network in the most privacy-preserving way. It's also a great way to support the Aztec network and get involved with the community.

## Minimum Hardware Requirements* 8 core / 16 vCPU (released in 2015 or later)
* 16 GB RAM
* 1 TB NVMe SSD
* 25 Mbps network connection

These requirements are subject to change as the network throughput increases.

**Before proceeding:** Ensure you've reviewed and completed the [prerequisites](/network/prerequisites).

This setup includes only essential settings. The `--network mainnet` flag applies network-specific defaults—see the [CLI reference](/network/reference/cli_reference) for all available configuration options.

## Setup## Step 1: Set Up Directory StructureCreate the directory structure for node data:

```
mkdir -p aztec-node/data  
cd aztec-node  
touch .env
```

## Step 2: Configure Environment VariablesAdd the following to your `.env` file:

```
DATA_DIRECTORY=./data  
LOG_LEVEL=info  
ETHEREUM_HOSTS=[your L1 execution endpoint]  
L1_CONSENSUS_HOST_URLS=[your L1 consensus endpoint]  
P2P_IP=[your external IP address]  
P2P_PORT=40400  
AZTEC_PORT=8080  
AZTEC_ADMIN_PORT=8880
```

tip

Find your public IP address with: `curl ipv4.icanhazip.com`

## Step 3: Create Docker Compose FileCreate a `docker-compose.yml` file in your `aztec-node` directory:

```
services:  
  aztec-node:  
    image: "aztecprotocol/aztec:2.1.9"  
    container_name: "aztec-node"  
    ports:  
      - ${AZTEC_PORT}:${AZTEC_PORT}  
      - ${P2P_PORT}:${P2P_PORT}  
      - ${P2P_PORT}:${P2P_PORT}/udp  
    volumes:  
      - ${DATA_DIRECTORY}:/var/lib/data  
    environment:  
      DATA_DIRECTORY: /var/lib/data  
      LOG_LEVEL: ${LOG_LEVEL}  
      ETHEREUM_HOSTS: ${ETHEREUM_HOSTS}  
      L1_CONSENSUS_HOST_URLS: ${L1_CONSENSUS_HOST_URLS}  
      P2P_IP: ${P2P_IP}  
      P2P_PORT: ${P2P_PORT}  
      AZTEC_PORT: ${AZTEC_PORT}  
      AZTEC_ADMIN_PORT: ${AZTEC_ADMIN_PORT}  
    entrypoint: >-  
      node  
      --no-warnings  
      /usr/src/yarn-project/aztec/dest/bin/index.js  
      start  
      --node  
      --archiver  
      --network mainnet  
    networks:  
      - aztec  
    restart: always  
  
networks:  
  aztec:  
    name: aztec
```

Security: Admin Port Not Exposed

The admin port (8880) is intentionally **not exposed** to the host machine for security reasons. The admin API provides sensitive operations like configuration changes and database rollbacks that should never be accessible from outside the container.

If you need to access admin endpoints, use `docker exec`:

```
docker exec -it aztec-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_getConfig","params":[],"id":1}'
```

## Step 4: Start the NodeStart the node:

```
docker compose up -d
```

## VerificationOnce your node is running, verify it's working correctly:

## Check Node Sync StatusCheck the current sync status:

```
curl -s -X POST -H 'Content-Type: application/json' \  
-d '{"jsonrpc":"2.0","method":"node_getL2Tips","params":[],"id":67}' \  
http://localhost:8080 | jq -r ".result.proven.number"
```

Compare the output with block explorers like [Aztec Scan](https://aztecscan.xyz/) or [Aztec Explorer](https://aztecexplorer.xyz/).

## Check Node Status```
curl http://localhost:8080/status
```

## Verify Port Connectivity```
# Check TCP connectivity on port 40400  
nc -vz [YOUR_EXTERNAL_IP] 40400  
# Should return: "Connection to [YOUR_EXTERNAL_IP] 40400 port [tcp/*] succeeded!"  
  
# Check UDP connectivity on port 40400  
nc -vu [YOUR_EXTERNAL_IP] 40400  
# Should return: "Connection to [YOUR_EXTERNAL_IP] 40400 port [udp/*] succeeded!"
```

## View Logs```
docker compose logs -f aztec-node
```

If all checks pass, your node should be up, running, and connected to the network.

## Troubleshooting## Port forwarding not working**Issue**: Your node cannot connect to peers.

**Solutions**:

* Verify your external IP address matches the `P2P_IP` setting
* Check firewall rules on your router and local machine
* Test connectivity using: `nc -zv [your-ip] 40400`

## Node not syncing**Issue**: Your node is not synchronizing with the network.

**Solutions**:

* Check L1 endpoint connectivity
* Verify both execution and consensus clients are fully synced
* Review logs for specific error messages
* Ensure L1 endpoints support high throughput

## Docker issues**Issue**: Container won't start or crashes.

**Solutions**:

* Ensure Docker and Docker Compose are up to date
* Check disk space availability
* Verify the `.env` file is properly formatted
* Review container logs: `docker compose logs aztec-node`

## Next Steps* Review [syncing best practices](/network/setup/syncing_best_practices) for faster synchronization
* Learn about [bootnode operation](/network/setup/bootnode_operation) for peer discovery
* Check the [CLI reference](/network/reference/cli_reference) for advanced configuration options
* Join the [Aztec Discord](https://discord.gg/aztec) for support and community discussions

---


# Running a Sequencer

Source: https://docs.aztec.network/network/setup/sequencer_management

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide covers sequencer lifecycle management on the Aztec network: keystore configuration, node setup, registration, ongoing operations, and eventual exit.

Minimum Stake Requirement

To participate as a sequencer on the Aztec network, you must stake a minimum of **200,000 AZTEC tokens**. Ensure you have sufficient tokens before proceeding with sequencer setup and registration.

Sequencer nodes are critical infrastructure responsible for ordering transactions and producing blocks. They perform three key actions:

1. Assemble unprocessed transactions and propose the next block
2. Attest to correct execution of transactions in proposed blocks (when part of the sequencer committee)
3. Submit successfully attested blocks to L1

Before publication, blocks must be validated by a committee of sequencer nodes who re-execute public transactions and verify private function proofs. Committee members attest to validity by signing the block header.

Once sufficient attestations are collected (two-thirds of the committee plus one), the block can be submitted to L1.

## Minimum Hardware Requirements* 8 core / 16 vCPU (released in 2015 or later)
* 16 GB RAM
* 1 TB NVMe SSD
* 25 Mbps network connection

These requirements are subject to change as the network throughput increases.

**Before proceeding:** Ensure you've reviewed and completed the [prerequisites](/network/prerequisites).

## Keystore ExplanationSequencers require private keys to identify themselves as valid proposers and attesters. These keys are configured through a private keystore file.

## Private Keystore StructureThe private keystore file (`keystore.json`) uses the following structure:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "ETH_PRIVATE_KEY",  
        "bls": "BLS_PRIVATE_KEY"  
      },  
      "publisher": ["PUBLISHER_PRIVATE_KEY"],  // Optional: defaults to attester key  
      "feeRecipient": "0x0000000000000000000000000000000000000000000000000000000000000000",  // Not currently used, set to all zeros  
      "coinbase": "ETH_ADDRESS"  
    }  
  ]  
}
```

info

The attester field contains both Ethereum and BLS keys:

* **ETH key**: Derives the address that serves as your sequencer's unique identifier in the protocol
* **BLS key**: Used to sign proposals and attestations, as well as for staking operations

## Field Descriptions## attester (required)")

**Your sequencer's identity.** Contains both Ethereum and BLS keys:

* **Format**: Object with `eth` and `bls` fields
* **eth**: Ethereum private key - the derived address serves as your sequencer's unique identifier in the protocol
* **bls**: BLS private key - actually signs proposals and attestations, and is used for staking operations (validator registration and proof of possession)
* **Purpose**: The ETH address identifies your sequencer, while the BLS key performs the cryptographic signing of consensus messages

## publisher (optional)")

Separate private key(s) for submitting BLS-signed messages to L1. The publisher just pays gas to post already-signed proposals and attestations.

* **Format**: Array of Ethereum private keys
* **Default**: Uses attester key if not specified
* **Purpose**: Posts signed messages to L1 and pays for gas (doesn't participate in signing)
* **Rule of thumb**: Ensure every publisher account maintains at least 0.1 ETH per attester account it serves. This balance allows the selected publisher to successfully post transactions when chosen.

tip

If you're using the attester ETH key for publishing (no separate publisher keys), you can omit the `publisher` field entirely from your keystore, but you will still need to fund the attester account according to the rule of thumb above.

## feeRecipientAztec address that would receive L2 transaction fees.

* **Format**: 32-byte Aztec address (64 hex characters)
* **Current status**: Not currently used by the protocol - set to `0x0000000000000000000000000000000000000000000000000000000000000000`
* **Purpose**: Reserved for future fee distribution mechanisms

## coinbase (optional)")

Ethereum address that receives all L1 block rewards and tx fees.

* **Format**: Ethereum address
* **Default**: Uses attester address if not specified

## Generating KeysUse the Aztec CLI's keystore utility to generate both your private and public keystores:

```
aztec validator-keys new \  
  --fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000 \  
  --staker-output \  
  --gse-address 0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f \  
  --l1-rpc-urls $ETH_RPC
```

**Relevant parameters:**

* `--fee-recipient`: Set to all zeros (not currently used by the protocol)
* `--staker-output`: Generate the public keystore for the staking dashboard
* `--gse-address`: The GSE (Governance Staking Escrow) contract address (`0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f` for mainnet)
* `--l1-rpc-urls`: Your Ethereum mainnet RPC endpoint
  + Set `ETH_RPC` environment variable, or replace `$ETH_RPC` with your Ethereum mainnet RPC URL (e.g., `https://mainnet.infura.io/v3/YOUR_API_KEY`)
* `--count`: Number of validator identities to generate (default: 1)
  + Use this to generate multiple attester identities in a single keystore
  + Example: `--count 5` generates 5 validator identities with sequential addresses
  + All identities are derived from the same mnemonic using different derivation paths
  + Useful for operators running multiple sequencer identities or delegated staking providers
* `--publisher-count` Number of publisher accounts per validator (default 0)

**This command creates two JSON files:**

1. **Private keystore** (`~/.aztec/keystore/keyN.json`) - Contains your ETH and BLS private keys for running the node
2. **Public keystore** (`~/.aztec/keystore/keyN_staker_output.json`) - Contains only public information (public keys and proof of possession) for the staking dashboard

Where `N` is an auto-incrementing number (e.g., `key1.json`, `key2.json`, etc.)

**What gets generated:**

* Automatically generates a mnemonic for key derivation (or provide your own with `--mnemonic`)
* Creates an ETH key (for your sequencer identifier) and BLS key (for signing)
* Computes BLS public keys (G1 and G2) and proof of possession
* Outputs your attester address, publisher address and BLS public keys to the console

**Example output (single validator):**

```
No mnemonic provided, generating new one...  
Using new mnemonic:  
  
word1 word2 word3 word4 word5 word6 word7 word8 word9 word10 word11 word12  
  
Wrote validator keystore to /Users/aztec/.aztec/keystore/key1.json  
Wrote staker output for 1 validator(s) to /Users/aztec/.aztec/keystore/key1_staker_output.json  
  
acc1:  
  attester:  
    eth: 0xA55aB561877E479361BA033c4ff7B516006CF547  
    bls: 0xa931139040533679ff3990bfc4f40b63f50807815d77346e3c02919d71891dc1
```

**Example output (multiple validators with `--count 3`):**

```
No mnemonic provided, generating new one...  
Using new mnemonic:  
  
word1 word2 word3 word4 word5 word6 word7 word8 word9 word10 word11 word12  
  
Wrote validator keystore to /Users/aztec/.aztec/keystore/key1.json  
Wrote staker output for 3 validator(s) to /Users/aztec/.aztec/keystore/key1_staker_output.json  
  
acc1:  
  attester:  
    eth: 0xA55aB561877E479361BA033c4ff7B516006CF547  
    bls: 0xa931139040533679ff3990bfc4f40b63f50807815d77346e3c02919d71891dc1  
acc2:  
  attester:  
    eth: 0xB66bC672988F590472CA144e5D8d9F82307DA658  
    bls: 0xb842240151644780ff4991cfd5f51c74f61918926e88457f4d13020e82902ed2  
acc3:  
  attester:  
    eth: 0xC77cD783999F601583DB255f6E9e0F93418EB769  
    bls: 0xc953351262755891ff5aa2dfe6f62d85f72a29a37f99568f5e24131f93a13fe3
```

**Critical: Save your mnemonic phrase!**

* The mnemonic is the **only thing you must save** - it can regenerate all your keys, addresses, and keystores
* Store it securely offline (not on the server running the node)

**For convenience, note:**

* **Attester address** (eth): Your sequencer's identifier (e.g., `0xA55aB...F547`) - useful for registration and monitoring
* **File paths**: Where the keystores were saved

All other information (BLS keys, public keys, addresses) can be re-derived from the mnemonic if needed.

Provide Your Own Mnemonic

For deterministic key generation or to recreate keys later, provide your own mnemonic:

```
aztec validator-keys new \  
  --fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000 \  
  --staker-output \  
  --gse-address 0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f \  
  --l1-rpc-urls $ETH_RPC \  
  --mnemonic "your twelve word mnemonic phrase here"
```

Generate Multiple Validator Identities

To generate multiple validator identities (useful for delegated staking providers or operators running multiple sequencers):

```
# Generate 5 validator identities from the same mnemonic  
aztec validator-keys new \  
  --fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000 \  
  --staker-output \  
  --gse-address 0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f \  
  --l1-rpc-urls $ETH_RPC \  
  --count 5
```

Each identity gets a unique attester address derived from sequential derivation paths. All identities are included in:

* The same private keystore file (`keyN.json`)
* The same public keystore file (`keyN_staker_output.json`)

For detailed instructions, advanced options, and complete examples, see the [Creating Sequencer Keystores guide](/network/operation/keystore/creating_keystores).

## Setup with Docker Compose## Step 1: Set Up Directory StructureCreate the directory structure for sequencer data storage:

```
mkdir -p aztec-sequencer/keys aztec-sequencer/data  
cd aztec-sequencer  
touch .env
```

## Step 2: Generate and Move Private Keystore to Docker DirectoryIf you haven't already generated your private and public keystores, do so now (see [Generating Keys](#generating-keys) above).

Move the private keystore (not the public keystore) into the Docker directory:

```
# Move the private keystore to Docker directory (replace N with your key number)  
cp ~/.aztec/keystore/keyN.json aztec-sequencer/keys/keystore.json  
  
# Keep the public keystore for later use with the staking dashboard  
# It will be at ~/.aztec/keystore/keyN_staker_output.json
```

## Step 3: Fund Your Publisher AccountYour sequencer needs ETH to pay for gas when submitting blocks to L1. Fund the account that will act as the publisher.

**Determine which address to fund:**

```
# Get your attester address (this will be your publisher if no separate publisher is configured)  
jq -r '.[0].attester' ~/.aztec/keystore/keyN_staker_output.json  
  
# If you have a separate publisher configured: (Note this returns the publisher private key)  
jq -r '.validators[0].publisher[0]' aztec-sequencer/keys/keystore.json
```

**Funding requirements:**

* **Rule of thumb**: Maintain at least **0.1 ETH per attester account** in each publisher account
* Publisher accounts submit blocks to L1 and pay for gas fees
* The system does not retry with another publisher if a transaction fails due to insufficient funds

**Examples:**

* 1 attester with 1 publisher (or using attester as publisher) → Maintain ≥ 0.1 ETH
* 3 attesters with 1 publisher → Maintain ≥ 0.3 ETH in that publisher account
* 3 attesters with 2 publishers → Maintain ≥ 0.15 ETH in each publisher account (0.3 ETH total)

tip

Set up monitoring or alerts to notify you when the publisher balance falls below the recommended threshold to prevent failed block publications.

## Step 4: Configure Environment VariablesAdd the following to your `.env` file:

```
DATA_DIRECTORY=./data  
KEY_STORE_DIRECTORY=./keys  
LOG_LEVEL=info  
ETHEREUM_HOSTS=[your Ethereum mainnet execution endpoint, or a comma separated list if you have multiple]  
L1_CONSENSUS_HOST_URLS=[your Ethereum mainnet consensus endpoint, or a comma separated list if you have multiple]  
P2P_IP=[your external IP address]  
P2P_PORT=40400  
AZTEC_PORT=8080  
AZTEC_ADMIN_PORT=8880
```

tip

Find your public IP address with: `curl ipv4.icanhazip.com`

Nethermind Users

If you are using Nethermind as your L1 execution client, you must add the following environment variable:

```
# Required for Nethermind  
L1_FIXED_PRIORITY_FEE_PER_GAS=1
```

## Step 5: Create Docker Compose FileCreate a `docker-compose.yml` file in your `aztec-sequencer` directory:

```
services:  
  aztec-sequencer:  
    image: "aztecprotocol/aztec:2.1.9"  
    container_name: "aztec-sequencer"  
    ports:  
      - ${AZTEC_PORT}:${AZTEC_PORT}  
      - ${P2P_PORT}:${P2P_PORT}  
      - ${P2P_PORT}:${P2P_PORT}/udp  
    volumes:  
      - ${DATA_DIRECTORY}:/var/lib/data  
      - ${KEY_STORE_DIRECTORY}:/var/lib/keystore  
    environment:  
      KEY_STORE_DIRECTORY: /var/lib/keystore  
      DATA_DIRECTORY: /var/lib/data  
      LOG_LEVEL: ${LOG_LEVEL}  
      ETHEREUM_HOSTS: ${ETHEREUM_HOSTS}  
      L1_CONSENSUS_HOST_URLS: ${L1_CONSENSUS_HOST_URLS}  
      P2P_IP: ${P2P_IP}  
      P2P_PORT: ${P2P_PORT}  
      AZTEC_PORT: ${AZTEC_PORT}  
      AZTEC_ADMIN_PORT: ${AZTEC_ADMIN_PORT}  
    entrypoint: >-  
      node  
      --no-warnings  
      /usr/src/yarn-project/aztec/dest/bin/index.js  
      start  
      --node  
      --archiver  
      --sequencer  
      --network mainnet  
    networks:  
      - aztec  
    restart: always  
  
networks:  
  aztec:  
    name: aztec
```

Security: Admin Port Not Exposed

The admin port (8880) is intentionally **not exposed** to the host machine for security reasons. The admin API provides sensitive operations like configuration changes and database rollbacks that should never be accessible from outside the container.

If you need to access admin endpoints, use `docker exec`:

```
docker exec -it aztec-sequencer curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_getConfig","params":[],"id":1}'
```

This configuration includes only essential settings. The `--network mainnet` flag applies network-specific defaults—see the [CLI reference](/network/reference/cli_reference) for all available configuration options.

## Step 6: Start the SequencerStart the sequencer:

```
docker compose up -d
```

## VerificationOnce your sequencer is running, verify it's working correctly:

## Check Sync StatusCheck the current sync status (this may take a few minutes):

```
curl -s -X POST -H 'Content-Type: application/json' \  
-d '{"jsonrpc":"2.0","method":"node_getL2Tips","params":[],"id":67}' \  
http://localhost:8080 | jq -r ".result.proven.number"
```

Compare the output with block explorers like [Aztec Scan](https://aztecscan.xyz/) or [Aztec Explorer](https://aztecexplorer.xyz/).

## Check Node Status```
curl http://localhost:8080/status
```

## View Logs```
docker compose logs -f --tail 100 aztec-sequencer
```

## Next Steps: Registering Your SequencerNow that your sequencer node is set up and running, you need to register it with the network. There are two ways to participate as a sequencer:

## Option 1: Self-Staking via Staking DashboardRegister your sequencer and provide your own stake through the staking dashboard. This is the most common approach for individual operators.

**→ [Register Your Sequencer (Self-Staking)](/network/operation/sequencer_management/registering_sequencer)**

You'll use the **public keystore** file (`keyN_staker_output.json`) that was generated when you created your keys.

## Option 2: Running with Delegated StakeOperate sequencers backed by tokens from delegators. This non-custodial system allows you to run sequencer infrastructure while delegators provide the economic backing.

**→ [Run with Delegated Stake](/network/operation/sequencer_management/become_a_staking_provider)**

As a provider, you'll register with the Staking Registry and manage a queue of sequencer identities that activate when delegators stake to you.

Which Option Should I Choose?

* **Self-staking**: You have tokens and want to run your own sequencer
* **Delegated staking**: You want to operate sequencer infrastructure and earn commission from delegators' stake

Both options use the same node setup from this guide.

## Monitoring Sequencer StatusYou can query the status of any sequencer (attester) using the Rollup and GSE (Governance Staking Escrow) contracts on L1.

## Prerequisites* Foundry installed (`cast` command)
* Ethereum RPC endpoint
* Registry contract address for your network

## Get Contract AddressesFirst, get the canonical Rollup contract address from the Registry:

```
# Get the canonical rollup address  
cast call [REGISTRY_CONTRACT_ADDRESS] "getCanonicalRollup()" --rpc-url [YOUR_RPC_URL]
```

Then get the GSE contract address from the Rollup:

```
# Get the GSE contract address  
cast call [ROLLUP_ADDRESS] "getGSE()" --rpc-url [YOUR_RPC_URL]
```

## Query Sequencer StatusCheck the complete status and information for a specific sequencer:

```
# Get full attester view (status, balance, exit info, config)  
cast call [ROLLUP_ADDRESS] "getAttesterView(address)" [ATTESTER_ADDRESS] --rpc-url [YOUR_RPC_URL]
```

This returns an `AttesterView` struct containing:

1. **status** - The sequencer's current status (see Status Codes below)
2. **effectiveBalance** - The sequencer's effective stake balance
3. **exit** - Exit information (if the sequencer is exiting)
4. **config** - Attester configuration (withdrawer address and public key)

## Status Codes| Status | Name | Meaning |
| --- | --- | --- |
| 0 | NONE | The sequencer does not exist in the sequencer set |
| 1 | VALIDATING | The sequencer is currently active and participating in consensus |
| 2 | ZOMBIE | The sequencer is not active (balance fell below ejection threshold, possibly due to slashing) but still has funds in the system |
| 3 | EXITING | The sequencer has initiated withdrawal and is in the exit delay period |

## Performance MonitoringTrack your sequencer's performance by monitoring:

* **Effective balance** - Should remain above the ejection threshold
* **Status** - Should be VALIDATING for active participation
* **Attestation rate** - How many attestations you've successfully submitted
* **Proposal success rate** - How many of your proposed blocks were accepted
* **Network participation metrics** - Overall participation in network consensus

## Exiting a Sequencerwarning

Information about the exit process will be added when the mechanism is finalized. Check the [Aztec Discord](https://discord.gg/aztec) for the latest information on exiting the sequencer set.

## Troubleshooting## Port forwarding not working**Issue**: Your node cannot connect to peers.

**Solutions**:

* Verify your external IP address matches the `P2P_IP` setting
* Check firewall rules on your router and local machine
* Test connectivity using: `nc -zv [your-ip] 40400`

## Sequencer not syncing**Issue**: Your node is not synchronizing with the network.

**Solutions**:

* Check L1 endpoint connectivity
* Verify both execution and consensus clients are fully synced
* Review logs for specific error messages
* Ensure L1 endpoints support high throughput

## Private keystore issues**Issue**: Private keystore not loading or errors about invalid keys.

**Solutions**:

* Ensure `keystore.json` is properly formatted
* Verify private keys are valid Ethereum private keys
* Check file permissions on the keys directory

## Docker issues**Issue**: Container won't start or crashes.

**Solutions**:

* Ensure Docker and Docker Compose are up to date
* Check disk space availability
* Verify the `.env` file is properly formatted
* Review container logs: `docker compose logs aztec-sequencer`

## Common IssuesSee the [Operator FAQ](/network/operation/operator_faq) for additional common issues and resolutions.

## Additional ResourcesAfter setting up and registering your sequencer:

* **[Register Your Sequencer](/network/operation/sequencer_management/registering_sequencer)** - Complete registration via staking dashboard
* **[Monitor Sequencer Status](#monitoring-sequencer-status)** - Track performance and attestation rate
* **[Operator FAQ](/network/operation/operator_faq)** - Common issues and resolutions
* **[Creating and Voting on Proposals](/network/operation/sequencer_management/creating_and_voting_on_proposals)** - Participate in governance
* **[High Availability Setup](/network/setup/high_availability_sequencers)** - Run your sequencer across multiple nodes for redundancy
* **[Advanced Keystore Patterns](/network/operation/keystore/advanced_patterns)** - Manage multiple sequencer identities

**Community support:**

* Join the [Aztec Discord](https://discord.gg/aztec) for operator support and network updates

---


# Registering a Sequencer

Source: https://docs.aztec.network/network/operation/sequencer_management/registering_sequencer

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide covers registering your sequencer on the Aztec network through the staking dashboard for **self-staking**. This is one of two ways to participate as a sequencer:

1. **Self-staking** (this guide): You provide your own stake via the staking dashboard
2. **Delegated staking**: You receive stake from delegators (see [Running with Delegated Stake](/network/operation/sequencer_management/become_a_staking_provider))

Before proceeding, ensure you have completed the [Sequencer Setup Guide](/network/setup/sequencer_management) and your node is running.

## Prerequisites* Completed sequencer node setup with keystore generated
* Access to your **public keystore** file (`keyN_staker_output.json`)
* Sufficient ATP/ATV tokens for staking
* Wallet with ETH for gas fees
* Web browser for accessing the staking dashboard

## Understanding Your KeystoreWhen you generated your sequencer keys, two files were automatically created:

1. **Private keystore** (`~/.aztec/keystore/keyN.json`) - Contains private keys, used by your sequencer node. Keep this secure and never share it.
2. **Public keystore** (`~/.aztec/keystore/keyN_staker_output.json`) - Contains only public information, used for registration via the staking dashboard.

## Public Keystore StructureThe public keystore contains the following information needed for registration:

```
[  
  {  
    "attester": "0xYOUR_ATTESTER_ADDRESS",  
    "publicKeyG1": {  
      "x": "FIELD_ELEMENT_AS_DECIMAL_STRING",  
      "y": "FIELD_ELEMENT_AS_DECIMAL_STRING"  
    },  
    "publicKeyG2": {  
      "x0": "FIELD_ELEMENT_AS_DECIMAL_STRING",  
      "x1": "FIELD_ELEMENT_AS_DECIMAL_STRING",  
      "y0": "FIELD_ELEMENT_AS_DECIMAL_STRING",  
      "y1": "FIELD_ELEMENT_AS_DECIMAL_STRING"  
    },  
    "proofOfPossession": {  
      "x": "FIELD_ELEMENT_AS_DECIMAL_STRING",  
      "y": "FIELD_ELEMENT_AS_DECIMAL_STRING"  
    }  
  }  
]
```

**Fields explained:**

* **`attester`**: Your Ethereum attester address (sequencer identifier)
* **`publicKeyG1`**: BLS public key on the G1 curve (x, y coordinates)
* **`publicKeyG2`**: BLS public key on the G2 curve (x0, x1, y0, y1 coordinates)
* **`proofOfPossession`**: Cryptographic proof to prevent rogue key attacks

tip

The public keystore contains no private keys and is safe to share with the staking dashboard or other parties.

## Preparing Your Keystore File## Single SequencerIf you're registering one sequencer, simply use the `keyN_staker_output.json` file that was generated when you created your keys.

## Multiple SequencersIf you're registering multiple sequencers in a single transaction, combine the individual keystore files into a single JSON array. Each object in the array represents one sequencer.

**Example for two sequencers:**

```
[  
  {  
    "attester": "0xATTESTER_ADDRESS_1",  
    "publicKeyG1": {  
      "x": "0x...",  
      "y": "0x..."  
    },  
    "publicKeyG2": {  
      "x0": "0x...",  
      "x1": "0x...",  
      "y0": "0x...",  
      "y1": "0x..."  
    },  
    "proofOfPossession": {  
      "x": "0x...",  
      "y": "0x..."  
    }  
  },  
  {  
    "attester": "0xATTESTER_ADDRESS_2",  
    "publicKeyG1": {  
      "x": "0x...",  
      "y": "0x..."  
    },  
    "publicKeyG2": {  
      "x0": "0x...",  
      "x1": "0x...",  
      "y0": "0x...",  
      "y1": "0x..."  
    },  
    "proofOfPossession": {  
      "x": "0x...",  
      "y": "0x..."  
    }  
  }  
]
```

Simply copy the contents of each `keyN_staker_output.json` file and combine them into a single array.

## Registration StepsFollow these steps to register your sequencer(s) through the staking dashboard:

1. **Navigate to the staking dashboard** at <https://stake.aztec.network>
2. **Connect your wallet** with the account that holds your ATP/ATV tokens
3. **Click "Stake"**

   ![Staking dashboard home](/assets/images/staking_dashboard_1-f9dd165c0b8b06914f0baa0befa2281a.png)
4. **Select "Run your own Sequencer"**

   ![Select sequencer option](/assets/images/staking_dashboard_2-ef4c50308b741b29cfec7f995b46baf1.png)
5. **Click through "Start Registration"** after reviewing the requirements
6. **Select the ATP/ATV tokens you want to stake**
7. **Upload your keystore JSON file** (either single or combined multi-sequencer file)

   ![Upload keystore file](/assets/images/staking_dashboard_3-8dcef3510e7073a4b559b11ddb78a254.png)
8. **Confirm your attester/sequencer addresses**

   ![Confirm addresses](/assets/images/staking_dashboard_4-f7c95ff94cdad0fa714e05a8765c49ac.png)
9. **Approve token spend** in your wallet

   ![Approve tokens](/assets/images/staking_dashboard_5-9a9d86173c781ba0d45525487dd4fbd5.png)
10. **Add staking for all sequencers to the queue**

    ![Add to queue](/assets/images/staking_dashboard_6-e791f311fccc92f91dd1901022d73f73.png)
11. **Execute transactions** in the dashboard

    ![Execute transactions](/assets/images/staking_dashboard_7-411d05fecee6a3f62ff3d580aa932410.png)
12. **Confirm each transaction** in your wallet
13. **Click "Complete"** when all transactions are confirmed
14. **Verification**: Your sequencers have entered the queue. You can verify this at <https://dashtec.xyz/queue>

## VerificationAfter registration, verify your sequencer is properly registered:

## Via Staking DashboardUse the staking dashboard to:

* View your sequencer's registration status
* Monitor your stake amount
* Track sequencer performance metrics

## Via Blockchain ExplorerYou can verify your sequencers are in the queue at <https://dashtec.xyz/queue>

## Via Smart ContractYou can also query the status directly using the Rollup contract. See [Monitoring Sequencer Status](/network/setup/sequencer_management#monitoring-sequencer-status) for detailed instructions.

## Next StepsAfter registering your sequencer:

1. **Monitor performance**: Track your sequencer's attestation rate and block proposals via the staking dashboard
2. **Maintain uptime**: Keep your sequencer node running with high availability
3. **Monitor your stake**: Ensure your stake remains above the ejection threshold
4. **Stay informed**: Join the [Aztec Discord](https://discord.gg/aztec) for operator support and network updates

## Alternative: Running with Delegated StakeIf you prefer to run a sequencer backed by delegated stake instead of self-staking, see the [Running with Delegated Stake](/network/operation/sequencer_management/become_a_staking_provider) guide.

---


# Become a Staking Provider

Source: https://docs.aztec.network/network/operation/sequencer_management/become_a_staking_provider

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide covers running a sequencer with delegated stake on the Aztec network. Unlike conventional setups where you must have your own stake, delegated stake lets you (the "provider") operate sequencers backed by tokens from delegators.

**This is a non-custodial system**: Delegators retain full control and ownership of their tokens at all times. You never take custody of the delegated tokens—they remain in the delegator's control while providing economic backing for your sequencer operations.

## PrerequisitesBefore proceeding, ensure you have:

* Knowledge of running a sequencer node (see [Sequencer Setup Guide](/network/setup/sequencer_management))
* An Ethereum wallet with sufficient ETH for gas fees
* Understanding of basic Aztec staking mechanics
* Foundry installed for `cast` commands
* Aztec CLI v2.1.9 or later installed:

```
bash -i <(curl -s https://install.aztec.network)  
aztec-up --version 2.1.9
```

## Contract Addresses (Sepolia)")

* Staking Registry: `0xc3860c45e5F0b1eF3000dbF93149756f16928ADB`
* GSE (Governance Staking Escrow): `0xfb243b9112bb65785a4a8edaf32529accf003614`

## How Delegated Stake WorksYou register with the StakingRegistry contract and add sequencer identities (keystores) to a queue. When delegators stake to your provider, the system:

1. Dequeues one keystore from your provider queue
2. Creates a [Split contract](https://docs.splits.org/core/split) for reward distribution
3. Registers the sequencer into the staking queue using the dequeued keystore

## Reward DistributionWhen a delegator stakes to your provider, a Split contract is automatically created to manage reward distribution. You configure your sequencer to use the Split contract address as the coinbase (see [After Delegation: Configure Sequencer Coinbase](#after-delegation-configure-sequencer-coinbase)).

Rewards are distributed according to your agreed commission rate:

* **Provider commission**: Your `providerRewardsRecipient` address receives your commission rate (e.g., 5% for 500 basis points)
* **Delegator rewards**: The delegator's Aztec Token Vault (ATV) receives the remaining percentage

**Rewards flow:**

1. Rewards accumulate in the rollup under the coinbase address (the Split contract)
2. After governance unlocks rewards, anyone can release them from the rollup to the `coinbase` address.
3. Anyone can then disperse the rewards from the Split contract to both the ATV and your `providerRewardsRecipient`

This design ensures delegators maintain control of their rewards while you earn commission for operating the sequencer infrastructure.

## Setup ProcessBefore starting these steps, ensure your sequencer node infrastructure is set up (see [Prerequisites](#prerequisites)).

Follow these steps to set up delegated stake:

1. Register your provider with the Staking Registry
2. Add sequencer identities to your provider queue
3. Set your metadata in the GitHub repo (or via email)

**After a delegator stakes:** Configure your sequencer's coinbase (see [After Delegation](#after-delegation-configure-sequencer-coinbase))

## Step 1: Register Your ProviderRegister with the `StakingRegistry` contract as a provider for delegated staking. Registration is permissionless and open to anyone.

**Function signature:**

```
function registerProvider(  
    address _providerAdmin,  
    uint16 _providerTakeRate,  
    address _providerRewardsRecipient  
) external returns (uint256);
```

**Parameters:**

* `_providerAdmin`: Address that can update provider configuration
* `_providerTakeRate`: Commission rate in basis points (500 = 5%)
* `_providerRewardsRecipient`: Address receiving commission payments

**Returns:** Your unique `providerIdentifier`. Save this—you'll need it for all provider operations.

**Example:**

```
# Register a provider with 5% commission rate  
cast send $STAKING_REGISTRY_ADDRESS \  
  "registerProvider(address,uint16,address)" \  
  $PROVIDER_ADMIN_ADDRESS \  
  500 \  
  $REWARDS_RECIPIENT_ADDRESS \  
  --rpc-url $RPC_URL \  
  --private-key $YOUR_PRIVATE_KEY
```

## Extracting Your Provider IDOnce the transaction is confirmed, you need to extract your `providerIdentifier` from the transaction logs. The provider ID is emitted as the second topic in the registration event log.

**Method 1: Using cast receipt**

```
cast receipt [TX_HASH] --rpc-url $RPC_URL | grep "return" | awk '{print $2}' | xargs cast to-dec
```

**Method 2: From transaction logs**

The transaction receipt will contain one log where the second topic is your `providerId` in hex format:

```
# Example log output  
logs [{"address":"0xc3860c45e5f0b1ef3000dbf93149756f16928adb",  
       "topics":["0x43fe1b4477c9a580955f586c904f4670929e184ef4bef4936221c52d0a79a75b",  
                 "0x0000000000000000000000000000000000000000000000000000000000000002",  # This is your providerId  
                 "0x000000000000000000000000efdb4c5f3a2f04e0cb393725bcae2dd675cc3718",  
                 "0x00000000000000000000000000000000000000000000000000000000000001f4"],  
       ...  
      }]
```

Convert the hex value to decimal:

```
cast to-dec 0x0000000000000000000000000000000000000000000000000000000000000002  
# Output: 2
```

**Save your `providerIdentifier`**—you'll need it for all subsequent provider operations.

## Step 2: Add Sequencer IdentitiesAdd sequencer identities (keystores) to your provider queue. Each keystore represents one sequencer that can be activated when a delegator stakes to you.

**Function signature:**

```
function addKeysToProvider(  
    uint256 _providerIdentifier,  
    KeyStore[] calldata _keyStores  
) external;
```

**Parameters:**

* `_providerIdentifier`: Your provider identifier from registration
* `_keyStores`: Array of keystore structures (max 100 per transaction)

**KeyStore structure:**

```
struct KeyStore {  
    address attester;              // Sequencer's address  
    BN254Lib.G1Point publicKeyG1; // BLS public key (G1)  
    BN254Lib.G2Point publicKeyG2; // BLS public key (G2)  
    BN254Lib.G1Point proofOfPossession;    // BLS signature (prevents rogue key attacks)  
}
```

Critical: Key Management for Delegated Staking

**⚠️ If you run out of keys, users cannot delegate tokens to you.**

The Staking Registry **DOES NOT** check for duplicate keys. Please take **EXTREME** care when registering keys:

* Duplicate keys will cause delegation failures when that duplicate is at the top of your queue
* The only way to fix this is by calling `dripProviderQueue(_providerIdentifier, _numberOfKeysToDrip)` to remove the duplicate
* Always verify keys before registration to avoid user experience issues

## Generating Keys for RegistrationUse the `aztec validator-keys` command with the `--staker-output` flag to automatically generate properly formatted registration data:

```
aztec validator-keys new \  
  --fee-recipient $AZTEC_ADDRESS \  
  --staker-output \  
  --gse-address 0xfb243b9112bb65785a4a8edaf32529accf003614 \  
  --l1-rpc-urls $RPC_URL
```

This command automatically:

1. Generates the keystore with ETH and BLS keys
2. Computes G1 and G2 public keys
3. Generates the proof of possession signature
4. Outputs the data in the correct format for the `addKeysToProvider` function

For more details on keystore creation, see the [Creating Sequencer Keystores](/network/operation/keystore/creating_keystores) guide.

## Building the Registration CommandYou have two options for constructing the `addKeysToProvider` command:

**Option 1: Use the helper script (Recommended)**

Use this helper script to automatically build the command from your `validator-keys` output:

<https://gist.github.com/koenmtb1/1b665d055fbc22581c288f90cdc60d88>

The script reads the JSON output from `validator-keys staker` and constructs the properly formatted `cast send` command.

**Option 2: Manual construction**

If you need to manually construct the command, the function signature is:

```
addKeysToProvider(uint256,(address,(uint256,uint256),(uint256,uint256,uint256,uint256),(uint256,uint256))[])
```

**Parameters:**

* First `uint256`: Your provider identifier (from registration in Step 1)
* Tuple array: `KeyStore[]` where each element contains:
  + `address`: Sequencer address
  + `(uint256,uint256)`: publicKeyG1 (x, y coordinates)
  + `(uint256,uint256,uint256,uint256)`: publicKeyG2 (x0, x1, y0, y1 coordinates)
  + `(uint256,uint256)`: proofOfPossession (x, y coordinates)

Example with placeholder values:

```
cast send $STAKING_REGISTRY_ADDRESS \  
  "addKeysToProvider(uint256,(address,(uint256,uint256),(uint256,uint256,uint256,uint256),(uint256,uint256))[])" \  
  $YOUR_PROVIDER_IDENTIFIER \  
  "[(0x742d35Cc6634C0532925a3b844Bc9e7595f0bEb,(12345,67890),(11111,22222,33333,44444),(98765,43210))]" \  
  --rpc-url $RPC_URL \  
  --private-key $ADMIN_PRIVATE_KEY
```

**Important:**

* Replace all values above with actual data from `aztec validator-keys new --staker-output`
* Add a maximum of 100 keystores per transaction to avoid gas limit issues
* Verify each keystore is unique before adding to prevent duplicate key issues

## Step 3: Set Your MetadataTo be featured on the staking dashboard, submit metadata about your provider.

**Required metadata:**

* Provider name and description
* Contact email
* Logo image (PNG or SVG, recommended size: 256x256px)
* Website URL
* Discord username
* Your `providerIdentifier`

**Submission process:**

Once made public, you'll create a pull request to the [staking-dashboard-external GitHub repository](https://github.com/AztecProtocol/staking-dashboard-external/tree/master/providers).

For now, email your provider metadata to [koen@aztec.foundation](mailto:koen@aztec.foundation) in the following JSON format. **Make sure to specify if it's for testnet or mainnet!**

```
{  
  "providerId": 1,  
  "providerName": "Example provider",  
  "providerDescription": "Brief description of the provider",  
  "providerEmail": "contact@provider.com",  
  "providerWebsite": "https://provider.com",  
  "providerLogoUrl": "https://provider.com/logo.png",  
  "discordUsername": "username"  
}
```

Good metadata helps delegators understand your offering and builds trust.

## After Delegation: Configure Sequencer CoinbaseOnce a delegator stakes to your provider, the system creates a Split contract for that delegation and activates the corresponding sequencer. **Configure the sequencer to use the Split contract address as the coinbase.**

## Why This MattersThe coinbase address determines where your sequencer's block rewards are sent. Setting it to the Split contract address ensures rewards are distributed according to your agreed commission rate, which is critical for maintaining trust with your delegators.

## How to Configure the CoinbaseUpdate the `coinbase` field in your sequencer node's keystore configuration to the Split contract address created for this delegation.

**Example keystore configuration:**

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0x...",  // Your Ethereum sequencer private key  
        "bls": "0x..."   // Your BLS sequencer private key  
      },  
      "publisher": ["0x..."],  // Address that submits blocks to L1  
      "coinbase": "0x[SPLIT_CONTRACT_ADDRESS]",  // Split contract for this delegation  
      "feeRecipient": "0x..."  // Your Aztec address for L2 fees  
    }  
  ]  
}
```

Replace `[SPLIT_CONTRACT_ADDRESS]` with the actual Split contract address created for this delegation. You can find this address in the staking dashboard (see "Finding Your Split Contract Address" below).

For detailed information about keystore configuration, including different storage methods and advanced patterns, see the [Advanced Keystore Guide](/network/operation/keystore).

## Finding Your Split Contract Address**You have to manually monitor the delegations you receive and update the `coinbase` address to the correct Split contract!**

You can retrieve the Split contract address for a specific delegation through the **Staking Dashboard**:

1. Navigate to your provider dashboard on the staking dashboard
2. Look for the dropdown called **"Sequencer Registered (x)"** where x is the number of registered sequencers
3. Click on the dropdown to expand it
4. This shows the Sequencer address → Split contract relation
5. Set the Split contract as the `coinbase` for the respective Sequencer address on your node

The dropdown will display a table showing which Split contract corresponds to each of your sequencer addresses, making it easy to configure the correct coinbase for each sequencer.

**Manual monitoring approach:**

Since coinbase configuration must be done manually, you should:

* Regularly check the staking dashboard for new delegations
* Set up alerts or scheduled checks (daily or more frequently during high activity)
* Update keystore configurations promptly when new delegations appear
* Maintain a record of which Split contracts map to which keystores

## Important Notes* **Monitor delegations actively**: The system does not automatically notify you of new delegations
* Configure the coinbase immediately after each delegation to ensure rewards flow correctly from the start
* Each delegation creates a unique Split contract—configure each sequencer with its specific Split contract address
* Restart your sequencer node after updating the keystore for changes to take effect
* Keep a mapping of sequencer addresses to Split contracts for operational tracking

## Monitoring Keystore AvailabilityAs a provider, you must maintain sufficient sequencer identities (keystores) in your queue to handle incoming delegations. When a delegator stakes to your provider and your queue is empty, they cannot activate a sequencer—this results in a poor delegator experience and lost opportunity.

## Why Monitoring MattersEach time a delegator stakes to your provider:

1. One keystore is dequeued from your provider queue
2. A sequencer is activated using that keystore
3. Your available keystore count decreases by one

If your queue runs empty, new delegations cannot activate sequencers until you add more keystores. This could cause delegators to choose other providers.

## Checking Available KeystoresCheck your current keystore queue with this call:

```
# Check provider queue length  
cast call [STAKING_REGISTRY_ADDRESS] \  
  "getProviderQueueLength(uint256) (uint256)" \  
  [YOUR_PROVIDER_IDENTIFIER] \  
  --rpc-url [RPC_URL]  
  
This returns your provider's queue length, which is the number of keystores currently available.  
  
## Setting Up Automated Monitoring  
  
Implement automated monitoring to alert you when your keystore queue runs low.  
  
## Cron Job Example  
  
The following script monitors your keystore queue and alerts when it drops below a threshold. Replace the placeholder values and uncomment your preferred alert method (webhook or email):  
  
```bash  
#!/bin/bash  
# check-keystores.sh  
  
THRESHOLD=5  # Alert when fewer than 5 keystores remain  
REGISTRY_ADDRESS="[STAKING_REGISTRY_ADDRESS]"  
PROVIDER_ID="[YOUR_PROVIDER_IDENTIFIER]"  
RPC_URL="[YOUR_RPC_URL]"  
WEBHOOK_URL="[YOUR_WEBHOOK_URL]"  # Optional: for Slack/Discord notifications  
  
# Gets current queue length  
QUEUE_LENGTH=$(cast call "$REGISTRY_ADDRESS" \  
  "getProviderQueueLength(uint256)" \  
  "$PROVIDER_ID" \  
  --rpc-url "$RPC_URL")  
  
echo "Queue length: $QUEUE_LENGTH"  
  
# Check if queue is running low  
if [ "$QUEUE_LENGTH" -lt "$THRESHOLD" ]; then  
  echo "WARNING: Keystore queue running low! Only $QUEUE_LENGTH keystores remaining."  
  
  # Send alert (uncomment and configure your preferred method)  
  # Slack/Discord webhook:  
  # curl -X POST "$WEBHOOK_URL" -H "Content-Type: application/json" \  
  #   -d "{\"text\":\"⚠️ Keystore queue low: $QUEUE_LENGTH remaining (threshold: $THRESHOLD)\"}"  
  
  # Email via mail command:  
  # echo "Keystore queue has $QUEUE_LENGTH keys remaining" | mail -s "Low Keystore Alert" your-email@example.com  
fi
```

Make the script executable and schedule it with cron:

```
# Make the script executable  
chmod +x /path/to/check-keystores.sh  
  
# Edit crontab  
crontab -e  
  
# Add this line to check every 4 hours  
0 */4 * * * /path/to/check-keystores.sh >> /var/log/keystore-monitor.log 2>&1
```

## When to Add More KeystoresAdd keystores proactively before running out:

* Monitor your delegation growth rate
* Add in batches (max 100 per transaction)
* Stay ahead of demand during high-activity periods

See [Step 2: Add Sequencer Identities](#step-2-add-sequencer-identities) for instructions.

## Managing Your ProviderUpdate your provider configuration using these functions. All must be called from your `providerAdmin` address.

## Update Admin AddressTransfer provider administration to a new address:

```
cast send [STAKING_REGISTRY_ADDRESS] \  
  "updateProviderAdmin(uint256,address)" \  
  [YOUR_PROVIDER_IDENTIFIER] \  
  [NEW_ADMIN_ADDRESS] \  
  --rpc-url [RPC_URL] \  
  --private-key [CURRENT_ADMIN_PRIVATE_KEY]
```

## Update Rewards RecipientChange the address receiving commission payments:

```
cast send [STAKING_REGISTRY_ADDRESS] \  
  "updateProviderRewardsRecipient(uint256,address)" \  
  [YOUR_PROVIDER_IDENTIFIER] \  
  [NEW_REWARDS_RECIPIENT_ADDRESS] \  
  --rpc-url [RPC_URL] \  
  --private-key [ADMIN_PRIVATE_KEY]
```

## Update Commission RateModify your commission rate (applies only to new delegations):

```
cast send [STAKING_REGISTRY_ADDRESS] \  
  "updateProviderTakeRate(uint256,uint16)" \  
  [YOUR_PROVIDER_IDENTIFIER] \  
  [NEW_RATE_BASIS_POINTS] \  
  --rpc-url [RPC_URL] \  
  --private-key [ADMIN_PRIVATE_KEY]
```

note

Rate changes only apply to new delegations. Existing delegations retain the original commission rate they agreed to.

## VerificationVerify your setup is working correctly.

## Check Provider RegistrationQuery the StakingRegistry to confirm your provider details:

```
cast call [STAKING_REGISTRY_ADDRESS] \  
  "providerConfigurations(uint256) (address,uint16,address)" \  
  [YOUR_PROVIDER_IDENTIFIER] \  
  --rpc-url [RPC_URL]
```

This returns:

1. The provider's admin address
2. The provider's commission rate in bps
3. The provider's rewards recipient

## Verify Queue LengthCheck your provider queue length:

```
cast call [STAKING_REGISTRY_ADDRESS] \  
  "getProviderQueueLength(uint256)" \  
  [YOUR_PROVIDER_IDENTIFIER] \  
  --rpc-url [RPC_URL]
```

## Monitor DelegationsView these metrics on the staking dashboard:

* Total stake delegated to your provider
* Number of active sequencers
* Commission earned
* Provider performance metrics

## Confirm Node OperationEnsure your sequencer nodes are running and synced. See [Useful Commands](/network/operation/sequencer_management/useful_commands) for commands to check sequencer status.

## Troubleshooting## Registration transaction fails**Issue**: The `registerProvider` transaction reverts or fails.

**Solutions**:

* Ensure your wallet has sufficient ETH for gas fees
* Verify the StakingRegistry contract address is correct
* Check that the commission rate is within acceptable bounds (typically 0-10000 basis points)
* Review transaction logs for specific error messages using a block explorer

## Cannot add sequencer identities**Issue**: The `addKeysToProvider` function fails.

**Solutions**:

* Confirm you're calling from the `providerAdmin` address
* Verify your `providerIdentifier` is correct
* Ensure BLS signatures in `KeyStore` are properly formatted (use the keystore creation utility)
* Check that the sequencer addresses aren't already registered elsewhere
* Reduce batch size if hitting gas limits (max 100 keystores per transaction)

## No delegators appearing**Issue**: No delegators are staking to your provider.

**Solutions**:

* Verify your provider is visible on the staking dashboard
* Complete all metadata fields to build trust
* Ensure your commission rate is competitive with other providers
* Confirm your sequencer nodes are operational and performing well
* Engage with the community on Discord to build your reputation

## Commission not being received**Issue**: Commission payments aren't arriving at the rewards recipient address.

**Solutions**:

* Verify the `providerRewardsRecipient` address is correct
* Check that delegations are active and generating fees
* Confirm your sequencers are producing blocks and earning fees
* Allow time for reward distribution (may not be immediate)
* Check the contract for pending distributions that need to be claimed

## Best Practices**Maintain Sufficient Keystores**: Set up automated monitoring to ensure your keystore queue never runs empty. See [Monitoring Keystore Availability](#monitoring-keystore-availability) for guidance on implementing alerts.

**Communicate Changes**: Inform delegators about commission rate changes, planned maintenance, or infrastructure updates. Good communication builds trust.

**Monitor Performance**: Track your sequencers' attestation rates, block proposals, and uptime. Poor performance may cause delegators to withdraw.

**Secure Your Keys**: The `providerAdmin` key controls your provider configuration. Store it securely and consider using a hardware wallet or multisig.

## Next StepsAfter completing this setup:

1. Monitor your provider's performance through the staking dashboard
2. Maintain high uptime for your sequencer nodes
3. Keep open communication with delegators
4. Regularly add new keystores to your provider queue (see [Monitoring Keystore Availability](#monitoring-keystore-availability))
5. Join the [Aztec Discord](https://discord.gg/aztec) for provider support and community discussions

---


# High Availability Sequencers

Source: https://docs.aztec.network/network/setup/high_availability_sequencers

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide shows you how to set up high availability (HA) for your sequencer by running the same sequencer identity across multiple physical nodes. This configuration provides redundancy and resilience, ensuring your sequencer continues operating even if individual nodes fail.

**What is High Availability for sequencers?**

High availability means running multiple sequencer nodes that share the same attester identity but use different publisher addresses. This allows your sequencer to:

* Continue attesting even if one node goes offline
* Maintain uptime during maintenance windows and upgrades
* Protect against infrastructure failures
* Ensure you don't miss attestation duties

## PrerequisitesBefore setting up HA sequencers, ensure you have:

* Experience running a single sequencer node (see the [Sequencer Management guide](/network/setup/sequencer_management))
* Understanding of basic keystore structure and configuration
* Access to multiple servers or VMs for running separate nodes
* Ability to securely distribute keys across infrastructure

## Why High Availability?## Benefits of HA Configuration**1. Redundancy and Fault Tolerance**

If one node crashes, experiences network issues, or needs maintenance, the other node continues operating. You won't miss attestations or proposals during:

* Hardware failures
* Network outages
* Planned maintenance
* Software upgrades
* Infrastructure provider issues

**2. Improved Uptime**

With properly configured HA, your sequencer can achieve near-perfect uptime. You can perform rolling upgrades, switching nodes in and out of service without missing duties.

## The Core ConceptIn an HA setup:

* **Attester identity is shared** across both nodes (same private key)
* **Publisher identity is unique** per node (different private keys)
* Both nodes run simultaneously and can attest independently
* Only one proposal is accepted per slot (enforced by L1)

## Setting Up High Availability Sequencers## Infrastructure Requirements**HA Setup (2 nodes):**

* 2 separate servers/VMs
* Each meeting the minimum sequencer requirements (see [Sequencer Management](/network/setup/sequencer_management))
* Different physical locations or availability zones (recommended)
* Reliable network connectivity for both nodes
* Access to the same L1 infrastructure (or separate L1 endpoints)
* Monitoring and alerting for both nodes

## Key ManagementYou'll need to generate:

1. **One shared attester key** - Your sequencer's identity (used by both nodes)
2. **One unique publisher key per node** - For submitting proposals
3. **Secure distribution method** - For safely deploying the shared attester key

Secure Key Distribution

The shared attester key must be distributed securely to both nodes. Consider using remote signers with:

* Encrypted secrets management (HashiCorp Vault, AWS Secrets Manager, etc.)
* Hardware security modules (HSMs) for production deployments

Never transmit private keys over unencrypted channels or store them in version control.

## Step 1: Generate KeysGenerate a base keystore with multiple publishers using the Aztec CLI. This will create one attester identity with multiple publisher keys that can be distributed across your nodes.

```
# Generate base keystore with one attester and 2 publishers  
aztec validator-keys new \  
  --fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000 \  
  --staker-output \  
  --gse-address 0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f \  
  --l1-rpc-urls $ETH_RPC \  
  --mnemonic "your shared mnemonic phrase for key derivation" \  
  --address-index 0 \  
  --publisher-count 2 \  
  --data-dir ~/ha-keys-temp
```

This command generates:

* **One attester** with both ETH and BLS keys (at derivation index 0)
* **Two publisher keys** (at derivation indices 1 and 2)
* All keys saved to `~/ha-keys-temp/key1.json`

The output will show the complete keystore JSON with all generated keys. **Save this output securely** as you'll need to extract keys from it for each node.

Managing Your Mnemonic

Store your mnemonic phrase securely in a password manager or hardware wallet. You'll need it to:

* Regenerate keys if lost
* Add more publishers later
* Recover your sequencer setup

Never commit mnemonics to version control or share them over insecure channels.

## Step 2: Fund Publisher AccountsEach publisher account needs ETH to pay for L1 gas when submitting proposals. You must maintain at least **0.1 ETH** in each publisher account.

**Check publisher balances:**

```
# Check balance for Publisher 1  
cast balance [PUBLISHER_1_ADDRESS] --rpc-url $ETH_RPC  
  
# Check balance for Publisher 2  
cast balance [PUBLISHER_2_ADDRESS] --rpc-url $ETH_RPC
```

**Example:**

```
cast balance 0x742d35Cc6634C0532925a3b844Bc9e7595f0bEb --rpc-url $ETH_RPC  
# Output: 100000000000000000 (0.1 ETH in wei)
```

Balance Monitoring

Monitor these balances regularly to ensure they don't drop below 0.1 ETH. Falling below this threshold risks slashing. Consider setting up automated alerts when balances drop below 0.15 ETH.

## Step 3: Extract Keys from Generated KeystoreOpen the generated keystore file (`~/ha-keys-temp/key1.json`) and extract the keys. The file will look something like this:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xABC...123",  // Shared attester ETH key  
        "bls": "0xDEF...456"   // Shared attester BLS key  
      },  
      "publisher": [  
        "0x111...AAA",  // Publisher 1 (for Node 1)  
        "0x222...BBB"   // Publisher 2 (for Node 2)  
      ],  
      "feeRecipient": "0x0000000000000000000000000000000000000000000000000000000000000000"  
    }  
  ]  
}
```

You'll use:

* The **same attester keys** (both ETH and BLS) on both nodes
* A **different publisher key** for each node

## Step 4: Create Node-Specific KeystoresCreate a separate keystore file for each node, using the same attester but different publishers:

**Node 1 Keystore** (`~/node1/keys/keystore.json`):

Use the same attester ETH and BLS keys, but only Publisher 1:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xABC...123",  
        "bls": "0xDEF...456"  
      },  
      "publisher": ["0x111...AAA"],  
      "feeRecipient": "0x0000000000000000000000000000000000000000000000000000000000000000"  
    }  
  ]  
}
```

**Node 2 Keystore** (`~/node2/keys/keystore.json`):

Use the same attester ETH and BLS keys, but only Publisher 2:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xABC...123",  
        "bls": "0xDEF...456"  
      },  
      "publisher": ["0x222...BBB"],  
      "feeRecipient": "0x0000000000000000000000000000000000000000000000000000000000000000"  
    }  
  ]  
}
```

Security Best Practice

After creating node-specific keystores, **securely delete** the base keystore file (`~/ha-keys-temp/key1.json`) that contains all publishers together. Each node should only have access to its own publisher key.

## Step 5: Deploy Keystores to NodesSecurely transfer each keystore to its respective node:

```
# Example: Copy keystores to remote nodes via SCP  
scp ~/node1/keys/keystore.json user@node1-server:~/aztec/keys/  
scp ~/node2/keys/keystore.json user@node2-server:~/aztec/keys/
```

Ensure proper file permissions on each node:

```
chmod 600 ~/aztec/keys/keystore.json
```

## Step 6: Start All NodesStart each node (assuming you are using Docker Compose):

```
# On each server  
docker compose up -d
```

Ensure both nodes are configured with:

* The same network (`--network mainnet`)
* Proper L1 endpoints
* Correct P2P configuration
* Adequate resources

## Verification and Monitoring## Verify Your HA Setup**1. Check that both nodes are running:**

```
# On each server  
curl http://localhost:8080/status  
  
# Or for Docker  
docker compose logs -f aztec-sequencer
```

**2. Confirm nodes recognize the shared attester:**

Check logs for messages indicating the attester address is loaded correctly. Both nodes should show the same attester address.

**3. Verify different publishers:**

Each node's logs should show a different publisher address being used for submitting transactions.

**4. Monitor attestations:**

Watch L1 for attestations from your sequencer's attester address. You should see attestations being submitted even if one node goes offline.

## Testing FailoverTo verify HA is working correctly:

1. **Monitor baseline**: Note the attestation rate with both nodes running
2. **Stop one node**: `docker compose down` on one server
3. **Verify continuity**: Check that attestations continue from the remaining node
4. **Check logs**: The remaining node should show normal operation
5. **Restart the stopped node**: Verify it rejoins seamlessly

If attestations stop when you stop one node, your HA configuration is not working correctly.

## Operational Best Practices## Load Balancing L1 AccessIf possible, configure each node with its own L1 infrastructure:

* **Node 1**: L1 endpoints in Region A
* **Node 2**: L1 endpoints in Region B

This protects against L1 provider outages affecting both nodes simultaneously.

## Geographic DistributionFor maximum resilience, distribute nodes across:

* Multiple data centers
* Different cloud providers
* Different geographic regions
* Different network availability zones

This protects against regional failures, provider outages, and network issues.

## Regular TestingPeriodically test your HA setup:

* Simulate node failures (stop nodes intentionally)
* Test network partitions (firewall rules)
* Verify monitoring and alerting
* Practice recovery procedures
* Test rolling upgrades

## Troubleshooting## Both Nodes Stopped Attesting**Issue**: No attestations from either node.

**Solutions**:

* Verify both nodes aren't simultaneously offline
* Check L1 connectivity from each node
* Verify the shared attester key is correct in both keystores
* Check that the sequencer is still registered and active on L1
* Review logs for errors on both nodes

## Duplicate Proposals Appearing**Issue**: Seeing multiple proposals for the same slot from your sequencer.

**Solutions**:

* Verify each node has a unique publisher key
* Check that publisher keys aren't duplicated across keystores
* Ensure nodes aren't sharing the same keystore file
* Review keystore configuration on each node

## One Node Not Contributing**Issue**: One node running but not attesting/proposing.

**Solutions**:

* Check that node's sync status
* Verify keystore is loaded correctly
* Check network connectivity to L1
* Review logs for specific errors
* Confirm publisher account has sufficient ETH

## Keystore Loading Failures**Issue**: Node fails to load the keystore.

**Solutions**:

* Verify keystore.json syntax is valid
* Check file permissions (readable by the node process)
* Ensure the keystore path is correct
* Validate all private keys are properly formatted
* Review the [Keystore Troubleshooting guide](/network/operation/keystore/troubleshooting)

## Related GuidesRunning Multiple Sequencers Per Node

Want to run multiple sequencer identities on a **single node** instead? See the [Advanced Keystore Patterns guide](/network/operation/keystore/advanced_patterns#multiple-sequencers)—that's a different use case from HA.

## Next Steps* Review the [Advanced Keystore Patterns guide](/network/operation/keystore/advanced_patterns) for multiple sequencers per node
* Set up [monitoring and observability](/network/operation/monitoring) for your HA infrastructure
* Learn about [governance participation](/network/operation/sequencer_management/creating_and_voting_on_proposals) as a sequencer
* Join the [Aztec Discord](https://discord.gg/aztec) for operator support and best practices

---


# Running a Prover

Source: https://docs.aztec.network/network/setup/running_a_prover

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide covers the steps required to run a prover on the Aztec network. Operating a prover is a resource-intensive role typically undertaken by experienced engineers due to its technical complexity and hardware requirements.

Aztec provers are critical infrastructure components. They generate cryptographic proofs attesting to transaction correctness, ultimately producing a single rollup proof submitted to Ethereum.

Prerequisites

Before proceeding, ensure you've reviewed and completed the [prerequisites](/network/prerequisites).

## Prover ArchitectureThe prover consists of three main components:

1. **Prover node**: Polls L1 for unproven epochs, creates prover jobs, distributes them to the broker, and submits the final rollup proof to the rollup contract.
2. **Prover broker**: Manages the job queue, distributing work to agents and collecting results.
3. **Prover agent(s)**: Executes proof generation jobs in a stateless manner.

## Minimum Requirements## Prover Node* 16 core / 32 vCPU (released in 2015 or later)
* 16 GB RAM
* 1 TB NVMe SSD
* 25 Mbps network connection

## Prover Broker* 8 core / 16 vCPU (released in 2015 or later)
* 16 GB RAM
* 10 GB SSD

## Prover Agents**For each agent:**

* 32 core / 64 vCPU (released in 2015 or later)
* 128 GB RAM
* 10 GB SSD

These requirements are subject to change as the network throughput increases. Prover agents require high-performance hardware, typically data center-grade infrastructure.

Running Multiple Agents

You can run multiple prover agents on a single machine by adjusting `PROVER_AGENT_COUNT`. Hardware requirements scale approximately linearly:

* **2 agents**: 64 cores, 256 GB RAM
* **3 agents**: 96 cores, 384 GB RAM
* **4 agents**: 128 cores, 512 GB RAM

## Generating KeysBefore setting up your prover, you need to generate the required Ethereum private key for the prover publisher.

## Prover Publisher Private KeyThe prover publisher key is used to submit proofs to L1. This account needs ETH funding to pay for L1 gas.

Generate an Ethereum private key using Foundry's `cast` tool:

```
# Generate a new wallet with a 24-word mnemonic  
cast wallet new-mnemonic --words 24  
  
# This outputs a mnemonic phrase, a derived address, and private key  
# Save these securely - you'll need the private key for PROVER_PUBLISHER_PRIVATE_KEY  
# and the address for PROVER_ID
```

**Important notes:**

* Save both the private key and the derived address securely
* The private key will be used for `PROVER_PUBLISHER_PRIVATE_KEY`
* The derived Ethereum address will be used for `PROVER_ID`

Account Funding Required

The publisher account needs to be funded with ETH to post proofs to L1. Ensure the account holds sufficient ETH for gas costs during operation.

tip

If you don't have Foundry installed, follow the installation guide at [getfoundry.sh](https://getfoundry.sh/).

## SetupThe prover components are distributed across multiple machines for better performance and resource utilization. This setup runs multiple prover agents on separate high-performance machines, isolates the broker for better job queue management, and separates network-facing components (prover node) from compute-intensive components (agents).

## Architecture* **Prover Node**: Runs on a machine with network access and L1 connectivity
* **Prover Broker**: Can run on the same machine as the prover node or separately (must be accessible from prover agents)
* **Prover Agents**: Run on separate high-performance machines (32+ cores each, scalable with `PROVER_AGENT_COUNT`)

Network Requirements

Prover agents must communicate with the prover broker over the network. Ensure that:

* The broker machine's port 8080 is accessible from all agent machines
* Firewall rules allow traffic between agents and broker
* Network connectivity is stable and low-latency between components

## Prover Node and Broker SetupOn the machine that will run the prover node and broker:

## Step 1: Set Up Directory Structure```
mkdir -p aztec-prover-node/prover-node-data aztec-prover-node/prover-broker-data  
cd aztec-prover-node  
touch .env
```

## Step 2: Configure Environment VariablesAdd to your `.env` file:

```
# Prover Node Configuration  
DATA_DIRECTORY=./prover-node-data  
P2P_IP=[your external IP address]  
P2P_PORT=40400  
ETHEREUM_HOSTS=[your L1 execution endpoint]  
L1_CONSENSUS_HOST_URLS=[your L1 consensus endpoint]  
LOG_LEVEL=info  
PROVER_BROKER_HOST=http://prover-broker:8080  
PROVER_PUBLISHER_PRIVATE_KEY=[your prover publisher private key, see prerequisites]  
AZTEC_PORT=8080  
AZTEC_ADMIN_PORT=8880  
  
# Prover Broker Configuration  
PROVER_BROKER_DATA_DIRECTORY=./prover-broker-data  
PROVER_BROKER_PORT=8080
```

## Step 3: Create Docker Compose FileCreate `docker-compose.yml`:

```
name: aztec-prover-node  
services:  
  prover-node:  
    image: aztecprotocol/aztec:2.1.9  
    entrypoint: >-  
      node  
      --no-warnings  
      /usr/src/yarn-project/aztec/dest/bin/index.js  
      start  
      --prover-node  
      --archiver  
      --network mainnet  
    depends_on:  
      prover-broker:  
        condition: service_started  
        required: true  
    environment:  
      DATA_DIRECTORY: /var/lib/data  
      ETHEREUM_HOSTS: ${ETHEREUM_HOSTS}  
      L1_CONSENSUS_HOST_URLS: ${L1_CONSENSUS_HOST_URLS}  
      LOG_LEVEL: ${LOG_LEVEL}  
      PROVER_BROKER_HOST: ${PROVER_BROKER_HOST}  
      PROVER_PUBLISHER_PRIVATE_KEY: ${PROVER_PUBLISHER_PRIVATE_KEY}  
      P2P_IP: ${P2P_IP}  
      P2P_PORT: ${P2P_PORT}  
      AZTEC_PORT: ${AZTEC_PORT}  
      AZTEC_ADMIN_PORT: ${AZTEC_ADMIN_PORT}  
    ports:  
      - ${AZTEC_PORT}:${AZTEC_PORT}  
      - ${P2P_PORT}:${P2P_PORT}  
      - ${P2P_PORT}:${P2P_PORT}/udp  
    volumes:  
      - ${DATA_DIRECTORY}:/var/lib/data  
    restart: unless-stopped  
  
  prover-broker:  
    image: aztecprotocol/aztec:2.1.9  
    entrypoint: >-  
      node  
      --no-warnings  
      /usr/src/yarn-project/aztec/dest/bin/index.js  
      start  
      --prover-broker  
      --network mainnet  
    environment:  
      DATA_DIRECTORY: /var/lib/data  
      ETHEREUM_HOSTS: ${ETHEREUM_HOSTS}  
      P2P_IP: ${P2P_IP}  
      LOG_LEVEL: ${LOG_LEVEL}  
    ports:  
      - ${PROVER_BROKER_PORT}:8080  
    volumes:  
      - ${PROVER_BROKER_DATA_DIRECTORY}:/var/lib/data  
    restart: unless-stopped
```

Security: Admin Port Not Exposed

The admin port (8880) is intentionally **not exposed** to the host machine for security reasons. The admin API provides sensitive operations like configuration changes and database rollbacks that should never be accessible from outside the container.

If you need to access admin endpoints, use `docker exec`:

```
docker exec -it prover-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_getConfig","params":[],"id":1}'
```

**Important:** The broker exposes port 8080 via `ports: - ${PROVER_BROKER_PORT}:8080`, making it accessible to external prover agents. Ensure this port is reachable from your agent machines.

This configuration includes only essential settings. The `--network mainnet` flag applies network-specific defaults—see the [CLI reference](/network/reference/cli_reference) for all available configuration options.

## Step 4: Start Node and Broker```
docker compose up -d
```

## Prover Agent SetupOn each machine that will run prover agents:

## Step 1: Set Up Directory```
mkdir aztec-prover-agent  
cd aztec-prover-agent  
touch .env
```

## Step 2: Configure Environment VariablesAdd to your `.env` file:

```
PROVER_AGENT_COUNT=1  
PROVER_AGENT_POLL_INTERVAL_MS=10000  
PROVER_BROKER_HOST=http://[BROKER_MACHINE_IP]:8080  
PROVER_ID=[address corresponding to PROVER_PUBLISHER_PRIVATE_KEY]
```

Replace `[BROKER_MACHINE_IP]` with the IP address of the machine running the prover broker.

**Agent configuration tips:**

* Set `PROVER_AGENT_COUNT` based on your machine's hardware (e.g., 64 cores/256 GB RAM = 2 agents, 96 cores/384 GB RAM = 3 agents, 128 cores/512 GB RAM = 4 agents)
* Test connectivity before starting: `curl http://[BROKER_MACHINE_IP]:8080`
* If the curl test fails, check your network configuration, firewall rules, and ensure the broker is running

## Step 3: Create Docker Compose FileCreate `docker-compose.yml`:

```
name: aztec-prover-agent  
services:  
  prover-agent:  
    image: aztecprotocol/aztec:2.1.9  
    entrypoint: >-  
      node  
      --no-warnings  
      /usr/src/yarn-project/aztec/dest/bin/index.js  
      start  
      --prover-agent  
      --network mainnet  
    environment:  
      PROVER_AGENT_COUNT: ${PROVER_AGENT_COUNT}  
      PROVER_AGENT_POLL_INTERVAL_MS: ${PROVER_AGENT_POLL_INTERVAL_MS}  
      PROVER_BROKER_HOST: ${PROVER_BROKER_HOST}  
      PROVER_ID: ${PROVER_ID}  
    restart: unless-stopped
```

## Step 4: Start Agent```
docker compose up -d
```

**Scaling your prover capacity:**

* **Horizontal scaling**: Add more agent machines by repeating the agent setup on additional high-performance machines
* **Vertical scaling**: Increase `PROVER_AGENT_COUNT` on existing machines (ensure adequate hardware)

All agents, regardless of which machine they're on, must be able to communicate with the broker at the configured `PROVER_BROKER_HOST`.

## VerificationOnce your prover is running, verify all components are working correctly:

## Check ServicesOn the prover node machine:

```
docker compose ps
```

On each agent machine:

```
docker compose ps
```

## View LogsOn prover node machine:

```
# Prover node logs  
docker compose logs -f prover-node  
  
# Broker logs  
docker compose logs -f prover-broker
```

On agent machines:

```
# Agent logs  
docker compose logs -f prover-agent
```

## Troubleshooting## Components not communicating**Issue**: Prover agent cannot connect to broker.

**Solutions**:

* Verify the broker IP address in `PROVER_BROKER_HOST` is correct
* Ensure port 8080 on the broker machine is accessible from agent machines
* Check firewall rules between machines allow traffic on port 8080
* Test connectivity from agent machine: `curl http://[BROKER_IP]:8080`
* Verify the broker container is running: `docker compose ps`
* Check if the broker port is exposed in docker-compose.yml
* Review broker logs for connection attempts: `docker compose logs prover-broker`

## Insufficient resources**Issue**: Prover agent crashes or performs poorly.

**Solutions**:

* Verify your hardware meets the minimum requirements (32 cores per agent, 128 GB RAM per agent)
* Check system resource usage: `docker stats`
* Reduce `PROVER_AGENT_COUNT` if running multiple agents per machine
* Ensure no other resource-intensive processes are running
* Monitor CPU and memory usage to verify resources match your configured agent count

## Agent not picking up jobs**Issue**: Agent logs show no job activity.

**Solutions**:

* Verify the broker is receiving jobs from the prover node
* Check broker logs for errors
* Confirm `PROVER_ID` matches your publisher address
* Ensure agent can reach the broker endpoint
* Test broker connectivity: `curl http://[BROKER_IP]:8080`

## Docker issues**Issue**: Containers won't start or crash repeatedly.

**Solutions**:

* Ensure Docker and Docker Compose are up to date
* Check disk space availability on all machines
* Verify `.env` files are properly formatted
* Review logs for specific error messages

## Common IssuesSee the [Operator FAQ](/network/operation/operator_faq) for additional common issues and resolutions.

## Next Steps* Monitor your prover's performance and proof submission rate
* Consider adding more prover agents for increased capacity (either by increasing `PROVER_AGENT_COUNT` or adding more machines)
* Join the [Aztec Discord](https://discord.gg/aztec) for operator support
* Review [creating and voting on proposals](/network/operation/sequencer_management/creating_and_voting_on_proposals) for participating in governance

---


# Building Node Software from Source

Source: https://docs.aztec.network/network/setup/building_from_source

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide shows you how to build the Aztec node Docker image from source, including all build tools and dependencies.

Building from source allows you to:

* Run a specific tagged version
* Verify the build process matches the official CI pipeline
* Customize the software for development or testing
* Audit the complete build chain

## Requirements**Hardware:**

* 4 core / 8 vCPU
* 16 GB RAM for Docker
* 150 GB free disk space
* Stable internet connection

**Software:**

* Git to clone the repository
* Docker version 20.10 or later with at least 16 GB RAM allocated

This guide assumes you're using a standard Linux distribution such as Debian or Ubuntu. While other operating systems may work, these instructions are tested and optimized for Linux environments.

These requirements are for building the software. Running a node has different requirements—see [Running a Full Node](/network/setup/running_a_node).

## Build Steps## Step 1: Clone the RepositoryClone the Aztec packages repository:

```
git clone https://github.com/AztecProtocol/aztec-packages.git  
cd aztec-packages
```

## Step 2: Check Out a Version TagCheck out the version tag you want to build. For example, to build version 2.1.9:

```
git checkout v2.1.9
```

tip

View all available release tags with:

```
git tag | grep "^v[0-9]"
```

## Step 3: Build the Container with Build ToolsBuild the container image with all necessary compilation tools:

```
cd build-images/src  
docker build --target build -t aztec-build-local:3.0 .  
cd ../..
```

tip

The tag `aztec-build-local:3.0` avoids conflicts with the official Docker Hub image and clearly indicates this is a locally-built version.

**What this does:**

* Builds the `build` stage from `build-images/src/Dockerfile`
* Installs Node.js 22.16.0 from NodeSource repository
* Installs Clang 16, 18, and 20 from LLVM
* Installs Rust 1.85.0 using the Rust toolchain installer with wasm32 targets
* Downloads and installs WASI SDK 27 from GitHub releases
* Builds Foundry v1.4.1 from source
* Installs CMake, Ninja, and other build essentials

note

This step builds all compilation tooling from scratch. The Dockerfile uses multi-stage builds—you only need the `build` target. Other targets (`devbox` and `sysbox`) are for development environments.

Verifying the Build Image

After the build completes, inspect the image to verify its contents:

```
# Run a shell in the container to explore  
docker run -it --rm aztec-build-local:3.0 /bin/bash  
  
# Check specific versions once inside:  
node --version        # Should show v22.16.0  
rustc --version       # Should show Rust 1.85.0  
clang-20 --version    # Should show clang 20.x  
forge --version       # Should show v1.4.1  
cmake --version       # Should show cmake 3.24+
```

You can review the Dockerfile at `build-images/src/Dockerfile` to see exactly what's installed and verify each step.

## Step 4: Compile the Source CodeRun the bootstrap script inside the build container to compile all source code:

```
docker run --rm \  
  -v $(pwd):/workspaces/aztec-packages \  
  -w /workspaces/aztec-packages \  
  aztec-build-local:3.0 \  
  ./bootstrap.sh full
```

**What this does:**

* Mounts your local repository into the container
* Compiles C++ code (Barretenberg proving system)
* Compiles Rust code (Noir compiler and ACVM)
* Builds TypeScript/JavaScript packages
* Writes compiled artifacts to your local filesystem (persist after container exits)
* Runs tests to verify the build

note

The bootstrap process is incremental—if interrupted, restart it to resume from where it left off. Git submodules for L1 contract dependencies are initialized automatically during the build.

## Step 5: Build the Runtime Base ImageBuild the runtime base image with Node.js dependencies. This image contains only runtime requirements—no build tools or compiled code:

```
docker build -f release-image/Dockerfile.base -t aztecprotocol/release-image-base .
```

note

The tag `aztecprotocol/release-image-base` must match exactly—the Dockerfile in Step 6 references this specific tag. This image is not published to Docker Hub; it exists only locally.

**What this does:**

* Installs production Node.js dependencies (no dev dependencies)
* Includes Node.js 22 runtime and system utilities
* Copies Foundry tools (anvil, cast) from the build container
* Creates a slim Ubuntu-based runtime environment without build tools

## Step 6: Build the Final Release ImageBuild the final node image, combining the runtime environment (Step 5) with your compiled code (Step 4):

```
docker build -f release-image/Dockerfile --build-arg VERSION=2.1.9 -t aztec-local:2.1.9 .
```

tip

The tag `aztec-local:2.1.9` avoids conflicts with the official Docker Hub image and clearly indicates this is a locally-built version.

**Build arguments:**

* `VERSION` - Sets the version string that appears in `aztec --version`

**What this does:**

* Starts from the `aztecprotocol/release-image-base` image (Step 5)
* Copies compiled source code from your local filesystem (Step 4)
* Sets up environment variables for Barretenberg and ACVM binaries
* Configures the entrypoint to run the Aztec node

## VerificationVerify your build completed successfully:

## Check Image Exists```
docker images aztec-local
```

You should see your image listed:

```
REPOSITORY      TAG       IMAGE ID       CREATED        SIZE  
aztec-local     2.1.9     abc123def456   2 minutes ago  2.5GB
```

## Verify Version```
docker run --rm aztec-local:2.1.9 --version
```

Should display version 2.1.9.

## Test Basic Functionality```
docker run --rm aztec-local:2.1.9 --help
```

Should display CLI help information without errors.

If all checks pass, your image is ready to use.

## Troubleshooting## Build fails with "no space left on device"**Issue**: Insufficient disk space.

**Solutions**:

* Clean up unused Docker images and build cache: `docker system prune -a`
* Free up at least 150 GB of disk space
* Ensure adequate storage for intermediate build artifacts

## Build image fails**Issue**: Errors during Step 3 when building `aztec-build-local:3.0`.

**Solutions**:

* Verify you're in the `build-images/src` directory
* Ensure the `--target build` flag is specified
* Retry the build if network issues occur while downloading Rust, LLVM, or WASI SDK
* Review `build-images/src/Dockerfile` to identify the failing stage

## Build fails with "failed to solve with frontend dockerfile.v0: failed to create LLB definition"**Issue**: The release image build cannot find the base image.

**Solutions**:

* Ensure you completed Step 5 and built the base image with the exact tag: `aztecprotocol/release-image-base`
* Verify the base image exists locally: `docker images aztecprotocol/release-image-base`
* If missing, return to Step 5 and rebuild the base image

## Bootstrap compilation fails**Issue**: Errors during `./bootstrap.sh` in Step 4.

**Solutions**:

* Verify you're using the correct build image: `aztec-build-local:3.0`
* Confirm you checked out a valid release tag (not a branch)
* Retry the build—the bootstrap script is incremental and resumes where it left off
* Review error messages for specifics—missing dependencies should not occur in the build container

## Docker runs out of memory**Issue**: Build crashes due to insufficient memory.

**Solutions**:

* Increase Docker's memory limit to at least 16 GB (Docker Desktop: Settings → Resources → Memory)
* Close other applications to free system memory
* Build on a machine with more RAM if possible

## Wrong version shows in `aztec --version`**Issue**: Version argument not passed correctly.

**Solutions**:

* Ensure you used `--build-arg VERSION=X.Y.Z` when building the release image
* The version should match the git tag without the 'v' prefix (e.g., `2.1.9` not `v2.1.9`)

## Using Your Custom Build## Running a NodeUse your locally-built image with any node setup method. For Docker Compose, update your `docker-compose.yml`:

```
services:  
  aztec-node:  
    image: "aztec-local:2.1.9"  
    # ... rest of configuration
```

See [Running a Full Node](/network/setup/running_a_node) for complete setup instructions.

## Using the CLIRun the Aztec CLI directly from your custom image:

```
docker run --rm aztec-local:2.1.9 --version
```

## Alternative Approaches## Using Pre-built Build ImageTo save time, skip Step 3 and pull the pre-built image from Docker Hub, then tag it locally:

```
docker pull aztecprotocol/build:3.0  
docker tag aztecprotocol/build:3.0 aztec-build-local:3.0
```

This approach is faster but requires trusting the published image. The official image is built from the same `build-images/src/Dockerfile`.

## Building Without DockerTo build without Docker, install all build dependencies locally and run `./bootstrap.sh` directly:

* Install all toolchains from the build image (Node.js 22, Rust 1.85.0, Clang 20, CMake, wasi-sdk)
* Run `bootstrap.sh check` to verify your environment
* See `build-images/README.md` for details

Using the build container is strongly recommended to ensure a consistent, tested environment.

## Understanding the Build ProcessThe build process uses these key files in the repository:

* **`build-images/src/Dockerfile`** - Defines the build container with all compilation tools
* **`bootstrap.sh`** - Main build script that compiles all source code (C++, Rust, TypeScript)
* **`release-image/Dockerfile.base`** - Multi-stage Dockerfile that creates a slim runtime base image
* **`release-image/Dockerfile`** - Final release image with compiled Aztec software
* **`release-image/bootstrap.sh`** - Build script used in CI for Docker images

The official CI pipeline follows a similar process. See `.github/workflows/ci3.yml` for how production images are built and deployed.

## Next Steps* Use your custom build to [run a full node](/network/setup/running_a_node)
* Set up [monitoring](/network/operation/monitoring) for your node
* Review the [CLI reference](/network/reference/cli_reference) for configuration options
* Join the [Aztec Discord](https://discord.gg/aztec) to discuss development and customization

---


# Using and running a bootnode

Source: https://docs.aztec.network/network/setup/bootnode_operation

Version: Ignition (v2.1.9)

On this page

## OverviewBootnodes facilitate peer discovery in the Aztec network by maintaining a list of active peers that new nodes can connect to. This guide covers how to connect your node to a bootnode and how to run your own bootnode.

## What is a bootnode?Nodes in the Aztec network must connect to peers to gossip transactions and propagate them across the network. Bootnodes help new nodes discover and connect to these peers, enabling them to join the peer-to-peer layer.

## PrerequisitesBefore proceeding, you should:

* Have the Aztec node software installed
* Understand basic command-line operations
* For running a bootnode: Have the necessary network infrastructure and port access

## Connecting to a bootnodeTo connect your node to a bootnode for peer discovery:

1. Obtain the bootnode's ENR (Ethereum Node Record)
2. Add the ENR to your node's `.env` file using the `BOOTSTRAP_NODES` environment variable

The variable accepts a comma-separated list of bootstrap node ENRs:

```
BOOTSTRAP_NODES=[ENR]
```

For multiple bootnodes:

```
BOOTSTRAP_NODES=[ENR1],[ENR2],[ENR3]
```

Then add the environment variable to your `docker-compose.yml`:

```
environment:  
  # ... other environment variables  
  BOOTSTRAP_NODES: ${BOOTSTRAP_NODES}
```

## Running a bootnodeTo run your own bootnode, create a dedicated Docker Compose configuration.

Create a `docker-compose.yml` file for your bootnode:

```
services:  
  aztec-bootnode:  
    image: "aztecprotocol/aztec:2.1.9"  
    container_name: "aztec-bootnode"  
    ports:  
      - ${P2P_PORT}:${P2P_PORT}  
      - ${P2P_PORT}:${P2P_PORT}/udp  
    volumes:  
      - ${DATA_DIRECTORY}:/var/lib/data  
    environment:  
      P2P_PORT: ${P2P_PORT}  
      P2P_BROADCAST_PORT: ${P2P_BROADCAST_PORT}  
      PEER_ID_PRIVATE_KEY_PATH: ${PEER_ID_PRIVATE_KEY_PATH}  
    entrypoint: >-  
      node  
      --no-warnings  
      /usr/src/yarn-project/aztec/dest/bin/index.js  
      start  
      --p2p-bootstrap  
    networks:  
      - aztec  
    restart: always  
  
networks:  
  aztec:  
    name: aztec
```

## Configuring the bootnode portBy default, the bootnode uses the `P2P_PORT` value. To customize the port, add to your `.env` file:

```
P2P_PORT=40400  
P2P_BROADCAST_PORT=[PORT]
```

## Persisting bootnode identityTo maintain a consistent bootnode identity across restarts, specify a private key location in your `.env` file:

```
DATA_DIRECTORY=./data  
PEER_ID_PRIVATE_KEY_PATH=/var/lib/data/bootnode-peer-id
```

**How it works:**

* If a private key exists at the path, the bootnode will use it for its identity
* If no private key exists, a new one will be generated and saved to that location
* This ensures your bootnode maintains the same ENR across restarts

## Obtaining your bootnode's ENRAfter starting your bootnode, obtain its ENR from the startup logs. You can share this ENR with node operators who want to connect to your bootnode.

## Adding your bootnode to the default setinfo

The process for adding bootnodes to Aztec's default bootnode list is currently being finalized. For now, share your bootnode ENR directly with node operators who
want to connect.

## VerificationTo verify your bootnode setup:

## For nodes connecting to a bootnode1. **Check logs**: Look for messages indicating successful peer discovery
2. **Verify peer count**: Confirm your node has connected to peers from the bootnode
3. **Monitor network activity**: Ensure transactions are being gossiped correctly

## For bootnode operators1. **Confirm bootnode is running**: Check that the process started successfully
2. **Verify port accessibility**: Ensure the configured port is open and accessible
3. **Monitor peer connections**: Check logs for incoming peer connection requests
4. **Validate ENR generation**: Confirm your bootnode's ENR is displayed in the logs

## Troubleshooting## Cannot connect to bootnode**Issue**: Your node fails to connect to the specified bootnode.

**Solutions**:

* Verify the ENR is correct and properly formatted
* Check network connectivity to the bootnode's address
* Ensure the bootnode is running and accessible
* Confirm firewall rules allow P2P connections

## Bootnode not discovering peers**Issue**: Your bootnode isn't discovering or storing peers.

**Solutions**:

* Verify the bootnode container is running with the correct configuration
* Check that the P2P port is properly configured and accessible
* Review logs for error messages or connection issues
* Ensure sufficient system resources are available

## Private key path errors**Issue**: Errors occur when specifying the peer ID private key path.

**Solutions**:

* Verify the path exists and is writable within the container
* Check file permissions for the directory and file
* Ensure the volume mount is correctly configured in docker-compose.yml
* Confirm the private key file format is correct (if reusing an existing key)

## Next Steps* Monitor your bootnode or node connections regularly
* Consider running multiple bootnodes for redundancy
* Join the Aztec community to share your bootnode ENR with other operators

---


# Using and uploading snapshots

Source: https://docs.aztec.network/network/setup/syncing_best_practices

Version: Ignition (v2.1.9)

On this page

## OverviewAll nodes on the Aztec network must download and synchronize the blockchain state before they can operate. This guide covers different sync modes, including how to use snapshots for faster synchronization and how to create your own snapshots.

Automatic Configuration

When using `--network [NETWORK_NAME]`, snapshot URLs are automatically configured for you. Most users don't need to manually set snapshot sources.

## Understanding sync modesNodes can synchronize state in two ways:

1. **L1 sync**: Queries the rollup and data availability layer for historical state directly from Layer 1
2. **Snapshot sync**: Downloads pre-built state snapshots from a storage location for faster synchronization

Since Aztec uses blobs, syncing from L1 requires an archive node that stores complete blob history from Aztec's deployment. Snapshot sync is significantly faster, doesn't require archive nodes, and reduces load on L1 infrastructure, making it the recommended approach for most deployments.

## PrerequisitesBefore proceeding, you should:

* Have the Aztec node software installed
* Understand basic node operation
* For uploading snapshots: Have access to cloud storage (Google Cloud Storage, Amazon S3, or Cloudflare R2) with appropriate permissions

## Using snapshots to sync your node## Configuring sync modeControl how your node synchronizes using the `SYNC_MODE` environment variable in your `.env` file:

```
aztec start --node --sync-mode [MODE]  
SYNC_MODE=[MODE]
```

Available sync modes:

* **`snapshot`**: Downloads and uses a snapshot only if no local data exists (default behavior)
* **`force-snapshot`**: Downloads and uses a snapshot even if local data exists, overwriting it
* **`l1`**: Syncs directly from Layer 1 without using snapshots

## Setting the snapshot sourceBy default, nodes use Aztec's official snapshot storage. To specify a custom snapshot location, add the `SNAPSHOTS_URL` environment variable to your `.env` file:

```
SYNC_MODE=snapshot  
SNAPSHOTS_URL=[BASE_URL]
```

The node searches for the snapshot index at:

```
[BASE_URL]/aztec-[L1_CHAIN_ID]-[VERSION]-[ROLLUP_ADDRESS]/index.json
```

**Supported storage backends**:

* **Google Cloud Storage** - `gs://bucket-name/path/`
* **Amazon S3** - `s3://bucket-name/path/`
* **Cloudflare R2** - `s3://bucket-name/path/?endpoint=https://[ACCOUNT_ID].r2.cloudflarestorage.com`
* **HTTP/HTTPS** - `https://host/path`
* **Local filesystem** - `file:///absolute/path`

**Default snapshot locations by network**:

* **Mainnet**: `https://aztec-labs-snapshots.com/mainnet/`
* **Testnet**: `https://aztec-labs-snapshots.com/testnet/`
* **Staging networks**: Configured via network metadata

## Using custom snapshot sourcesYou can configure your node to use custom snapshot sources for various use cases.

Add the following to your `.env` file:

**Google Cloud Storage:**

```
SYNC_MODE=force-snapshot  
SNAPSHOTS_URL=gs://my-snapshots/
```

**Cloudflare R2:**

```
SYNC_MODE=snapshot  
SNAPSHOTS_URL=s3://my-bucket/snapshots/?endpoint=https://[ACCOUNT_ID].r2.cloudflarestorage.com
```

Replace `[ACCOUNT_ID]` with your Cloudflare account ID.

**HTTP/HTTPS mirror:**

```
SYNC_MODE=snapshot  
SNAPSHOTS_URL=https://my-mirror.example.com/snapshots/
```

Then add the environment variables to your `docker-compose.yml`:

```
environment:  
  # ... other environment variables  
  SYNC_MODE: ${SYNC_MODE}  
  SNAPSHOTS_URL: ${SNAPSHOTS_URL}
```

## Creating and uploading snapshotsYou can create snapshots of your node's state for backup purposes or to share with other nodes. This is done by calling the `nodeAdmin_startSnapshotUpload` method on the node admin API.

## How snapshot upload worksWhen triggered, the upload process:

1. Pauses node syncing temporarily
2. Creates a backup of the archiver and world-state databases
3. Uploads the backup to the specified storage location
4. Resumes normal operation

## Uploading a snapshotUse the node admin API to trigger a snapshot upload. You can upload to Google Cloud Storage, Amazon S3, or Cloudflare R2 by specifying the appropriate storage URI.

**Example command**:

**Upload to Google Cloud Storage:**

```
docker exec -it aztec-node curl -XPOST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{  
    "method": "nodeAdmin_startSnapshotUpload",  
    "params": ["gs://your-bucket/snapshots/"],  
    "id": 1,  
    "jsonrpc": "2.0"  
  }'
```

**Upload to Amazon S3:**

```
docker exec -it aztec-node curl -XPOST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{  
    "method": "nodeAdmin_startSnapshotUpload",  
    "params": ["s3://your-bucket/snapshots/"],  
    "id": 1,  
    "jsonrpc": "2.0"  
  }'
```

**Upload to Cloudflare R2:**

```
docker exec -it aztec-node curl -XPOST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{  
    "method": "nodeAdmin_startSnapshotUpload",  
    "params": ["s3://your-bucket/snapshots/?endpoint=https://[ACCOUNT_ID].r2.cloudflarestorage.com"],  
    "id": 1,  
    "jsonrpc": "2.0"  
  }'
```

Replace `aztec-node` with your container name and `[ACCOUNT_ID]` with your Cloudflare account ID.

**Note**: Ensure your storage credentials are configured before uploading:

* **Google Cloud Storage**: Set up [Application Default Credentials](https://cloud.google.com/docs/authentication/application-default-credentials)
* **Amazon S3 / Cloudflare R2**: Set environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`

## Scheduling regular snapshotsFor continuous backup, schedule the upload command to run at regular intervals using cron or a similar scheduler. The frequency depends on how current you need your snapshots to be.

Once uploaded, other nodes can download these snapshots by configuring their `--snapshots-url` to point to your storage location.

## VerificationTo verify your sync configuration is working:

## For snapshot downloads1. **Check startup logs**: Look for messages indicating snapshot download progress
2. **Monitor sync time**: Snapshot sync should be significantly faster than L1 sync
3. **Verify state completeness**: Confirm your node has the expected block height after sync
4. **Check data directories**: Ensure the archiver and world-state databases are populated

## For snapshot uploads1. **Check API response**: The upload command should return a success response
2. **Monitor logs**: Watch for upload progress messages in the node logs
3. **Verify storage**: Check your storage bucket to confirm the snapshot files exist
4. **Validate index file**: Ensure the `index.json` file is created at the expected path
5. **Test download**: Try downloading the snapshot with another node to confirm it works

## Troubleshooting## Snapshot download fails**Issue**: Node cannot download snapshot from the specified URL.

**Solutions**:

* Verify the `--snapshots-url` is correct and accessible
* Check network connectivity to the storage location
* Confirm the snapshot index file exists at the expected path
* Review node logs for specific error messages
* Try using Aztec's default snapshot URL to isolate custom URL issues

## Snapshot upload fails**Issue**: The `nodeAdmin_startSnapshotUpload` command returns an error.

**Solutions**:

* Verify storage credentials are properly configured (Google Cloud, AWS, or Cloudflare R2)
* Check that the specified bucket exists and you have write permissions
* Confirm sufficient disk space is available for creating the backup
* Review node logs for detailed error messages
* For S3/R2: Ensure environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are set

## Storage space issues**Issue**: Running out of disk space during sync or snapshot creation.

**Solutions**:

* Ensure sufficient disk space (at least 2x the expected database size for snapshots)
* Clean up old snapshots or data if running recurring uploads
* Monitor disk usage and set up alerts
* Consider using a larger volume or adding storage

## Best practices* **Use snapshot sync for production**: Snapshot sync is significantly faster and more efficient than L1 sync
* **Choose the right storage backend**:
  + Google Cloud Storage for simplicity and GCP integration
  + Amazon S3 for AWS infrastructure integration
  + Cloudflare R2 for cost-effective public distribution (free egress)
* **Schedule regular snapshots**: Create snapshots at regular intervals if running critical infrastructure
* **Test snapshot restoration**: Periodically verify that your snapshots download and restore correctly
* **Monitor storage costs**: Implement retention policies to manage cloud storage costs
* **Keep snapshots current**: Older snapshots require more time to sync to the current state
* **Use `force-snapshot` sparingly**: Only use when you need to reset to a known state, as it overwrites local data

## Next Steps* Learn about [running bootnodes](/network/setup/bootnode_operation) for improved peer discovery
* Set up [monitoring](/network/operation/monitoring) to track your node's sync progress
* Check the [CLI reference](/network/reference/cli_reference) for additional sync-related options
* Join the [Aztec Discord](https://discord.gg/aztec) for sync optimization tips

---


# Monitoring

Source: https://docs.aztec.network/network/operation/monitoring

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide shows you how to set up monitoring and observability for your Aztec node using OpenTelemetry, Prometheus, and Grafana. Monitoring helps you maintain healthy node operations, diagnose issues quickly, and track performance over time.

Docker Compose Setup

This monitoring setup is designed to work with Docker Compose deployments of Aztec nodes.

## ArchitectureThe monitoring stack uses three components working together:

* **OpenTelemetry Collector**: Receives metrics from your Aztec node via OTLP protocol
* **Prometheus**: Stores and queries time-series metrics data
* **Grafana**: Visualizes metrics with dashboards and alerts

Your Aztec node exports metrics to the OpenTelemetry Collector, which processes and exposes them in a format Prometheus can scrape. Prometheus stores the metrics as time-series data, and Grafana queries Prometheus to create visualizations and alerts.

## Getting StartedFollow these guides in order to set up your complete monitoring stack:

1. [OpenTelemetry Collector Setup](/network/operation/otel_setup) - Configure OTEL to receive metrics from your node
2. [Prometheus Setup](/network/operation/prometheus_setup) - Set up Prometheus to store and query metrics
3. [Grafana Setup](/network/operation/grafana_setup) - Configure Grafana for visualization and alerting
4. [Key Metrics Reference](/network/operation/metrics_reference) - Understand the metrics your node exposes and create custom dashboards
5. [Complete Example and Troubleshooting](/network/operation/monitoring_example_troubleshooting) - Full Docker Compose configuration and troubleshooting help

## Available Metrics OverviewYour Aztec node exposes metrics through OpenTelemetry to help you monitor performance and health. The metrics available depend on your node type (full node, sequencer, or prover) and version.

## Metric CategoriesYour node exposes metrics in these categories:

* **Node Metrics**: Block height, sync status, peer count, and transaction processing
* **Sequencer Metrics**: Attestation activity, block proposals, and committee participation (sequencer nodes only)
* **Prover Metrics**: Job queue, proof generation, and agent utilization (prover nodes only)
* **System Metrics**: CPU, memory, disk I/O, and network bandwidth

For detailed information about each metric, PromQL queries, and dashboard creation, see the [Key Metrics Reference](/network/operation/metrics_reference).

## Next StepsOnce your monitoring stack is running:

* Review the [Key Metrics Reference](/network/operation/metrics_reference) to understand available metrics and PromQL queries
* Set up alerting rules in Prometheus for critical conditions
* Create custom dashboards tailored to your operational needs
* Configure notification channels (Slack, PagerDuty, email) in Grafana
* Join the [Aztec Discord](https://discord.gg/aztec) to share dashboards with the community

For troubleshooting common monitoring issues, see the [Complete Example and Troubleshooting](/network/operation/monitoring_example_troubleshooting) guide.

---


# Setup Guides

Source: https://docs.aztec.network/network/operation/otel_setup

Version: Ignition (v2.1.9)

On this page

## OverviewThe OpenTelemetry Collector receives metrics from your Aztec node and exports them to Prometheus for storage and analysis.

## Prerequisites* A running Aztec node with Docker Compose
* Basic understanding of Docker networking

## Setup Steps## Step 1: Create Configuration FileCreate an `otel-collector-config.yml` file in the same directory as your existing `docker-compose.yml`:

```
receivers:  
  otlp:  
    protocols:  
      http:  
        endpoint: 0.0.0.0:4318  
      grpc:  
        endpoint: 0.0.0.0:4317  
  
exporters:  
  prometheus:  
    endpoint: "0.0.0.0:8889"  
    metric_expiration: 5m  
  
processors:  
  batch:  
  
service:  
  pipelines:  
    metrics:  
      receivers: [otlp]  
      exporters:  
        - prometheus
```

This configuration:

* Receives metrics via OTLP (OpenTelemetry Protocol) on ports 4317 (gRPC) and 4318 (HTTP)
* Exports metrics to Prometheus format on port 8889
* Uses batch processing for efficiency

## Step 2: Add OTEL Collector to Docker ComposeAdd the following to your existing `docker-compose.yml` file:

```
services:  
  # ... existing services ...  
  otel-collector:  
    image: otel/opentelemetry-collector  
    container_name: aztec-otel  
    ports:  
      - 8888:8888  # OTEL collector metrics endpoint  
      - 8889:8889  # Prometheus exporter endpoint  
      - 4317:4317  # OTLP gRPC receiver  
      - 4318:4318  # OTLP HTTP receiver  
    volumes:  
      - ./otel-collector-config.yml:/etc/otel-collector-config.yml  
    command: >-  
      --config=/etc/otel-collector-config.yml  
    networks:  
      - aztec  
    restart: always
```

## Step 3: Configure Your Node to Export MetricsConfigure your Aztec node to export metrics to the OTEL collector.

**Step 3a: Add to .env file**

Add these variables to your `.env` file:

```
OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://otel-collector:4318/v1/metrics
```

**Step 3b: Update docker-compose.yml**

Add these environment variables to your node's service in `docker-compose.yml`:

```
services:  
  aztec-node:  # or aztec-sequencer, prover-node, etc.  
    # ... existing configuration ...  
    environment:  
      # ... existing environment variables ...  
      OTEL_EXPORTER_OTLP_METRICS_ENDPOINT: ${OTEL_EXPORTER_OTLP_METRICS_ENDPOINT}
```

**Network configuration:** Since your node and OTEL collector share the same Docker Compose file and `aztec` network, use the service name `otel-collector` in the endpoint URL as shown above.

## Step 4: Start Services```
# Start or restart all services  
docker compose up -d
```

## Step 5: Verify Metrics CollectionCheck that metrics are being collected:

```
# View OTEL collector logs  
docker compose logs -f otel-collector  
  
# Query Prometheus endpoint  
curl http://localhost:8889/metrics
```

You should see metrics in Prometheus format.

## Next Steps* Proceed to [Prometheus Setup](/network/operation/prometheus_setup) to configure metric storage and querying
* Return to [Monitoring Overview](/network/operation/monitoring)

---


# Prometheus Setup

Source: https://docs.aztec.network/network/operation/prometheus_setup

Version: Ignition (v2.1.9)

On this page

## OverviewPrometheus scrapes and stores the metrics exposed by the OTEL collector, providing a time-series database for querying and analysis.

## Prerequisites* Completed [OpenTelemetry Collector Setup](/network/operation/otel_setup)
* OTEL collector running and exposing metrics on port 8889

## Setup Steps## Step 1: Create Prometheus ConfigurationCreate a `prometheus.yml` file:

```
global:  
  scrape_interval: 15s  
  evaluation_interval: 15s  
  
scrape_configs:  
  - job_name: 'aztec-node'  
    static_configs:  
      - targets: ['otel-collector:8889']  
        labels:  
          instance: 'aztec-node-1'
```

If you're running multiple nodes, adjust the `instance` label to uniquely identify each node.

## Step 2: Add Prometheus to Docker ComposeAdd Prometheus to your `docker-compose.yml`:

```
services:  
  # ... existing services (otel-collector, etc.) ...  
  
  prometheus:  
    image: prom/prometheus:latest  
    container_name: aztec-prometheus  
    ports:  
      - 9090:9090  
    volumes:  
      - ./prometheus.yml:/etc/prometheus/prometheus.yml  
      - prometheus-data:/prometheus  
    command:  
      - '--config.file=/etc/prometheus/prometheus.yml'  
      - '--storage.tsdb.path=/prometheus'  
      - '--storage.tsdb.retention.time=30d'  
    networks:  
      - aztec  
    restart: always  
  
volumes:  
  prometheus-data:
```

## Step 3: Start Prometheus```
docker compose up -d
```

## Step 4: Verify PrometheusAccess Prometheus UI at `http://localhost:9090` and verify:

1. Go to **Status → Targets** to check that the `aztec-node` target is up
2. Go to **Graph** and query a metric (e.g., `aztec_archiver_block_height`)

## Using Prometheus## Query MetricsUse the Prometheus UI to explore and query metrics:

1. Navigate to `http://localhost:9090/graph`
2. Enter a metric name in the query box (use autocomplete to discover available metrics)
3. Click **Execute** to see the results
4. Switch between **Table** and **Graph** views

## Example Queries```
# Current block height  
aztec_archiver_block_height  
  
# Blocks synced over time window  
increase(aztec_archiver_block_height[5m])  
  
# Memory usage  
process_resident_memory_bytes  
  
# CPU usage rate  
rate(process_cpu_seconds_total[5m])
```

## Next Steps* Proceed to [Grafana Setup](/network/operation/grafana_setup) to configure visualization and alerting
* Return to [Monitoring Overview](/network/operation/monitoring)

---


# Grafana Setup

Source: https://docs.aztec.network/network/operation/grafana_setup

Version: Ignition (v2.1.9)

On this page

## OverviewGrafana provides visualization and alerting for your metrics, allowing you to create custom dashboards and receive notifications when issues arise.

## Prerequisites* Completed [Prometheus Setup](/network/operation/prometheus_setup)
* Prometheus running and accessible at `http://prometheus:9090`

## Setup Steps## Step 1: Add Grafana to Docker ComposeAdd Grafana to your `docker-compose.yml`:

```
services:  
  # ... existing services (otel-collector, prometheus, etc.) ...  
  grafana:  
    image: grafana/grafana:latest  
    container_name: aztec-grafana  
    ports:  
      - 3000:3000  
    volumes:  
      - grafana-data:/var/lib/grafana  
    environment:  
      - GF_SECURITY_ADMIN_PASSWORD=admin  
      - GF_USERS_ALLOW_SIGN_UP=false  
    networks:  
      - aztec  
    restart: always  
  
volumes:  
  # ... existing volumes  ...  
  grafana-data:  
  
networks:  
  aztec:  
    name: aztec
```

Admin Password Security

Change the default admin password (`GF_SECURITY_ADMIN_PASSWORD`) to a secure value for production deployments.

## Step 2: Start Grafana```
docker compose up -d grafana
```

## Step 3: Access Grafana1. Navigate to `http://localhost:3000`
2. Login with username `admin` and the password you set (default: `admin`)
3. You'll be prompted to change the password on first login

## Step 4: Add Prometheus Data Source1. In the left sidebar, click **Connections** → **Data sources**
2. Click **Add data source**
3. Search for and select **Prometheus**
4. Configure:
   * **Name**: Aztec Prometheus
   * **URL**: `http://prometheus:9090`
5. Click **Save & Test**

You should see a green success message confirming Grafana can connect to Prometheus.

## Creating Dashboards## Option 1: Create a Basic Dashboard1. In the left sidebar, click **Dashboards**
2. Click **New** → **New Dashboard**
3. Click **Add visualization**
4. Select your **Aztec Prometheus** data source
5. In the query editor, enter a metric (explore available metrics using the autocomplete)
6. Customize the visualization type and settings
7. Click **Apply**
8. Click **Save dashboard** icon (top right)
9. Give your dashboard a name and click **Save**

## Option 2: Import a Pre-built DashboardIf the Aztec community has created shared dashboards:

1. Click **+** → **Import**
2. Enter dashboard ID or upload JSON file
3. Select **Aztec Prometheus** as the data source
4. Click **Import**

## Recommended Dashboard PanelsExample panels you can create (adjust metric names based on what's actually available):

1. **Block Height Over Time**: Line graph tracking block sync progress
2. **Sync Rate**: Line graph showing blocks synced over time window (use `increase()` function)
3. **Peer Count**: Gauge showing P2P connections
4. **Memory Usage**: Line graph of `process_resident_memory_bytes`
5. **CPU Usage**: Line graph of `rate(process_cpu_seconds_total[5m])`

## Setting Up AlertsConfigure alerts to notify you of issues:

## Step 1: Create an Alert Rule1. In the left sidebar, click **Alerting** (bell icon)
2. Click **Alert rules** → **New alert rule**
3. Configure your alert:
   * **Query**: Select your Prometheus data source and metric (e.g., `aztec_archiver_block_height`)
   * **Condition**: Define the threshold (e.g., `increase(aztec_archiver_block_height[15m]) == 0` to alert if no blocks in 15 minutes)
   * **Evaluation interval**: How often to check (e.g., 1m)
4. Click **Save**

## Step 2: Configure Contact Points1. Under **Alerting**, click **Contact points**
2. Click **Add contact point**
3. Choose your notification method:
   * **Email**: Configure SMTP settings
   * **Slack**: Add webhook URL
   * **PagerDuty**: Add integration key
   * **Webhook**: Custom HTTP endpoint
4. Click **Save**

## Step 3: Create Notification Policies1. Under **Alerting**, click **Notification policies**
2. Click **New notification policy**
3. Define routing rules to send alerts to specific contact points
4. Click **Save**

## Example Alert Rules## Node Sync AlertAlert if the node stops syncing blocks:

* **Query**: `increase(aztec_archiver_block_height[15m])`
* **Condition**: `== 0`
* **Description**: Node has not synced any blocks in the last 15 minutes

## High Memory Usage AlertAlert if memory usage exceeds threshold:

* **Query**: `process_resident_memory_bytes`
* **Condition**: `> 8000000000` (8GB)
* **Description**: Node memory usage exceeds 8GB

## Peer Connection AlertAlert if peer count drops too low:

* **Query**: `aztec_peer_manager_peer_count_peers`
* **Condition**: `< 5`
* **Description**: Node has fewer than 5 peer connections

## Next Steps* Explore the [Monitoring Overview](/network/operation/monitoring) for troubleshooting and metrics reference
* Join the [Aztec Discord](https://discord.gg/aztec) to share dashboards with the community
* Configure additional notification channels for your alerts

---


# Key Metrics Reference

Source: https://docs.aztec.network/network/operation/metrics_reference

Version: Ignition (v2.1.9)

On this page

## OverviewYour Aztec node exposes metrics through OpenTelemetry to help you monitor performance, health, and operational status. This guide covers key metrics across node types and how to use them effectively.

Discovering Metrics

Once your monitoring stack is running, you can discover available metrics in the Prometheus UI at `http://localhost:9090/graph`. Start typing in the query box to see autocomplete suggestions for metrics exposed by your node.

## Prerequisites* Complete monitoring stack setup following the [Monitoring Overview](/network/operation/monitoring)
* Ensure Prometheus is running and scraping metrics from your OTEL collector
* Verify access to Prometheus UI at `http://localhost:9090`

Metric Names May Vary

The exact metric names and labels in this guide depend on your node type, version, and configuration. Always verify the actual metrics exposed by your node using the Prometheus UI metrics explorer at `http://localhost:9090/graph`. Common prefixes: `aztec_archiver_*`, `aztec_sequencer_*`, `aztec_prover_*`, `process_*`.

## Querying with PromQLUse Prometheus Query Language (PromQL) to query and analyze your metrics. Understanding these basics will help you read the alert rules throughout this guide.

## Basic Queries```
# Instant vector - current value  
aztec_archiver_block_height  
  
# Range vector - values over time  
aztec_archiver_block_height[5m]
```

## Rate and Increase```
# Rate of change per second (for counters)  
rate(process_cpu_seconds_total[5m])  
  
# Blocks synced over time window (for gauges)  
increase(aztec_archiver_block_height[1h])  
  
# Derivative - per-second change rate of gauges  
deriv(process_resident_memory_bytes[30m])
```

## Arithmetic OperationsCalculate derived metrics using basic math operators:

```
# Calculate percentage (block proposal failure rate)  
(increase(aztec_sequencer_slot_count[15m]) - increase(aztec_sequencer_slot_filled_count[15m]))  
/ increase(aztec_sequencer_slot_count[15m])  
  
# Convert to percentage scale  
rate(process_cpu_seconds_total[5m]) * 100
```

## Comparison OperatorsFilter and alert based on thresholds:

```
# Greater than  
rate(process_cpu_seconds_total[5m]) > 2.8  
  
# Less than  
aztec_peer_manager_peer_count_peers < 5  
  
# Equal to  
increase(aztec_archiver_block_height[15m]) == 0  
  
# Not equal to  
aztec_sequencer_current_state != 1
```

## Time WindowsChoose time windows based on metric behavior and alert sensitivity:

* **Short windows** (`[5m]`, `[10m]`) - Detect immediate issues, sensitive to spikes
* **Medium windows** (`[15m]`, `[30m]`) - Balance between responsiveness and stability, recommended for most alerts
* **Long windows** (`[1h]`, `[2h]`) - Trend analysis, capacity planning, smooth out temporary fluctuations

Example: `increase(aztec_archiver_block_height[15m])` checks if blocks were processed in the last 15 minutes - long enough to avoid false alarms from brief delays, short enough to catch real problems quickly.

## Core Node MetricsYour node exposes these foundational metrics for monitoring blockchain synchronization and network health. Configure immediate alerting for these metrics in all deployments.

## L2 Block Height ProgressTrack whether your node is actively processing new L2 blocks:

* **Metric**: `aztec_archiver_block_height`
* **Description**: Current L2 block number the node has synced to

**Alert rule**:

```
- alert: L2BlockHeightNotIncreasing  
  expr: increase(aztec_archiver_block_height{aztec_status=""}[15m]) == 0  
  for: 5m  
  labels:  
    severity: critical  
  annotations:  
    summary: "Aztec node not processing L2 blocks"  
    description: "No L2 blocks processed in the last 15 minutes. Node may be stuck or out of sync."
```

## Peer ConnectivityTrack the number of active P2P peers connected to your node:

* **Metric**: `aztec_peer_manager_peer_count_peers`
* **Description**: Number of outbound peers currently connected to the node

**Alert rule**:

```
- alert: LowPeerCount  
  expr: aztec_peer_manager_peer_count_peers < 5  
  for: 10m  
  labels:  
    severity: warning  
  annotations:  
    summary: "Low peer count detected"  
    description: "Node has only {{ $value }} peers connected. Risk of network isolation."
```

## L1 Block Height ProgressMonitor whether your node is seeing new L1 blocks:

* **Metric**: `aztec_l1_block_height`
* **Description**: Latest L1 (Ethereum) block number seen by the node

**Alert rule**:

```
- alert: L1BlockHeightNotIncreasing  
  expr: increase(aztec_l1_block_height[15m]) == 0  
  for: 10m  
  labels:  
    severity: warning  
  annotations:  
    summary: "Node not seeing new L1 blocks"  
    description: "No L1 block updates in 15 minutes. Check L1 RPC connection."
```

## Sequencer MetricsIf you're running a sequencer node, monitor these metrics for consensus participation, block production, and L1 publishing. Configure alerting for critical operations.

## L1 Publisher ETH BalanceMonitor the ETH balance used for publishing to L1 to prevent transaction failures:

* **Metric**: `aztec_l1_publisher_balance_eth`
* **Description**: Current ETH balance of the L1 publisher account

**Alert rule**:

```
- alert: LowL1PublisherBalance  
  expr: aztec_l1_publisher_balance_eth < 0.5  
  for: 5m  
  labels:  
    severity: critical  
  annotations:  
    summary: "L1 publisher ETH balance critically low"  
    description: "Publisher balance is {{ $value }} ETH. Refill immediately to avoid transaction failures."
```

## Sequencer StateMonitor the operational state of the sequencer module:

* **Metric**: `aztec_sequencer_current_state`
* **Description**: Current state of the sequencer module (1 = OK/running, 0 = stopped/error)

**Alert rule**:

```
- alert: SequencerNotHealthy  
  expr: aztec_sequencer_current_state != 1  
  for: 2m  
  labels:  
    severity: critical  
  annotations:  
    summary: "Sequencer module not in healthy state"  
    description: "Sequencer state is {{ $value }} (expected 1). Check sequencer logs immediately."
```

## Block Proposal FailuresTrack failed block proposals by comparing slots to filled slots:

* **Metrics**: `aztec_sequencer_slot_count` and `aztec_sequencer_slot_filled_count`
* **Description**: Tracks slots assigned to your sequencer versus slots successfully filled. Alert triggers when the failure rate exceeds 5% over 15 minutes.

**Alert rule**:

```
- alert: HighBlockProposalFailureRate  
  expr: |  
    (increase(aztec_sequencer_slot_count[15m]) - increase(aztec_sequencer_slot_filled_count[15m]))  
    / increase(aztec_sequencer_slot_count[15m]) > 0.05  
  for: 5m  
  labels:  
    severity: warning  
  annotations:  
    summary: "High block proposal failure rate"  
    description: "{{ $value | humanizePercentage }} of block proposals are failing in the last 15 minutes."
```

## Blob Publishing FailuresTrack failures when publishing blobs to L1:

* **Metric**: `aztec_l1_publisher_blob_tx_failure`
* **Description**: Number of failed blob transaction submissions to L1

**Alert rule**:

```
- alert: BlobPublishingFailures  
  expr: increase(aztec_l1_publisher_blob_tx_failure[15m]) > 0  
  for: 5m  
  labels:  
    severity: warning  
  annotations:  
    summary: "Blob publishing failures detected"  
    description: "{{ $value }} blob transaction failures in the last 15 minutes. Check L1 gas prices and publisher balance."
```

## Attestation ActivityTrack your sequencer's participation in the consensus protocol:

* **Metrics**: Attestations submitted, attestation success rate, attestation timing
* **Use cases**:
  + Verify your sequencer is actively participating
  + Monitor attestation success rate
  + Detect missed attestation opportunities

## Block ProposalsMonitor block proposal activity and success:

* **Metrics**: Blocks proposed, proposal success rate, proposal timing
* **Use cases**:
  + Track block production performance
  + Identify proposal failures and causes
  + Monitor proposal timing relative to slot schedule

## Committee ParticipationTrack your sequencer's involvement in consensus committees:

* **Metrics**: Committee assignments, participation rate, duty execution
* **Use cases**:
  + Verify your sequencer is assigned to committees
  + Monitor duty execution completion rate
  + Track committee participation over time

## Performance MetricsMeasure block production efficiency:

* **Metrics**: Block production time, validation latency, processing throughput
* **Use cases**:
  + Optimize block production pipeline
  + Identify performance bottlenecks
  + Compare performance against network averages

## Prover MetricsIf you're running a prover node, track these metrics for proof generation workload and resource utilization.

## Job QueueMonitor pending proof generation work:

* **Metrics**: Queue depth, queue wait time, job age
* **Use cases**:
  + Detect proof generation backlogs
  + Capacity planning for prover resources
  + Monitor job distribution across agents

## Proof GenerationTrack proof completion metrics:

* **Metrics**: Proofs completed, completion time, success rate, failure reasons
* **Use cases**:
  + Monitor proof generation throughput
  + Identify failing proof types
  + Track generation time trends

## Agent UtilizationMonitor resource usage per proof agent:

* **Metrics**: CPU usage per agent, memory allocation, GPU utilization (if applicable)
* **Use cases**:
  + Optimize agent allocation
  + Detect resource constraints
  + Load balancing across agents

## ThroughputMeasure proof generation capacity:

* **Metrics**: Jobs completed per time period, proofs per second, utilization rate
* **Use cases**:
  + Capacity planning
  + Performance optimization
  + SLA monitoring

## System MetricsYour node exposes standard infrastructure metrics through OpenTelemetry and the runtime environment.

## CPU UsageMonitor process and system CPU utilization:

* **Metric**: `process_cpu_seconds_total`
* **Description**: Cumulative CPU time consumed by the process in seconds

**Alert rules**:

```
# Note: Adjust thresholds based on your system's CPU core count.  
# Example below assumes a 4-core system (70% = 2.8 cores, 85% = 3.4 cores)  
- alert: HighCPUUsage  
  expr: rate(process_cpu_seconds_total[5m]) > 2.8  
  for: 10m  
  labels:  
    severity: warning  
  annotations:  
    summary: "High CPU usage detected"  
    description: "Node using {{ $value }} CPU cores (above 2.8 threshold). Consider scaling resources."
```

## Memory UsageTrack RAM consumption:

* **Metric**: `process_resident_memory_bytes`
* **Description**: Resident memory size in bytes

**Alert rules**:

```
- alert: HighMemoryUsage  
  expr: process_resident_memory_bytes > 8000000000  
  for: 5m  
  labels:  
    severity: warning  
  annotations:  
    summary: "High memory usage detected"  
    description: "Memory usage is {{ $value | humanize1024 }}B. Consider increasing available RAM or investigating memory leaks."
```

**Additional monitoring**:

* Track memory growth rate to detect leaks
* Monitor garbage collection metrics for runtime efficiency

## Disk I/OMonitor storage operations:

* **Metrics**: Disk read/write rates, I/O latency, disk utilization
* **Use cases**:
  + Identify I/O bottlenecks
  + Plan storage upgrades
  + Detect disk performance degradation

## Network BandwidthTrack network throughput:

* **Metrics**: Bytes sent/received, packet rates, connection counts
* **Use cases**:
  + Monitor P2P bandwidth usage
  + Capacity planning for network resources
  + Detect unusual traffic patterns

## Creating Dashboards in GrafanaOrganize your Grafana dashboards by operational focus to make monitoring efficient and actionable. For specific panel configurations and queries, see the [Grafana Setup](/network/operation/grafana_setup) guide.

## Dashboard Organization Strategy**Overview Dashboard** - At-a-glance health check

* L2 and L1 block height progression
* Peer connectivity status
* Critical alerts summary
* Resource utilization (CPU, memory)
* Use stat panels and gauges for current values
* Include time-series graphs for trends

**Performance Dashboard** - Deep-dive into operational metrics

* Block processing rates and latencies
* Transaction throughput
* Network bandwidth utilization
* Query response times
* Use percentile graphs (p50, p95, p99) for latency metrics
* Compare current performance against historical baselines

**Resource Dashboard** - Infrastructure monitoring

* CPU usage per core
* Memory allocation and garbage collection
* Disk I/O rates and latency
* Network packet rates
* Set threshold warning lines at 70-80% utilization
* Include growth trend projections

**Role-Specific Dashboards** - Specialized metrics by node type

* **Sequencer Dashboard**: Block proposals, attestations, committee participation, L1 publisher balance
* **Prover Dashboard**: Job queue depth, proof generation rates, agent utilization, success rates
* Focus on metrics unique to the role's responsibilities
* Include SLA tracking and performance benchmarks

## Best Practices## Metric Collection1. **Appropriate Scrape Intervals**: Balance data granularity against storage costs

   * Standard: 15s for most metrics
   * High-frequency: 5s for critical real-time metrics
   * Low-frequency: 60s for slow-changing metrics
2. **Retention Policy**: Configure based on operational needs

   * Short-term: 7-15 days for detailed troubleshooting
   * Long-term: 30-90 days for trend analysis
   * Archive: Consider downsampling for longer retention
3. **Label Cardinality**: Avoid high-cardinality labels that explode metric storage

   * Good: `instance`, `node_type`, `region`
   * Avoid: `user_id`, `transaction_hash`, `timestamp`

## Monitoring Strategy1. **Layered Monitoring**: Monitor at multiple levels

   * Infrastructure: CPU, memory, disk, network
   * Application: Block height, peers, throughput
   * Business: Transaction success rate, user activity
2. **Proactive Alerts**: Set alerts before problems become critical

   * Use warning and critical thresholds
   * Alert on trends, not just absolute values
   * Reduce alert fatigue with proper tuning
3. **Dashboard Discipline**: Keep dashboards focused and actionable

   * Separate dashboards by role and concern
   * Include relevant context in panel titles
   * Add threshold lines and annotations

## Next Steps* Explore advanced PromQL queries in the [Prometheus documentation](https://prometheus.io/docs/prometheus/latest/querying/basics/)
* Set up alerting rules following the [Prometheus alerting guide](https://prometheus.io/docs/alerting/latest/overview/)
* Configure notification channels in [Grafana](/network/operation/grafana_setup)
* Return to [Monitoring Overview](/network/operation/monitoring)
* Join the [Aztec Discord](https://discord.gg/aztec) to share dashboards with the community

---


# Complete Example and Troubleshooting

Source: https://docs.aztec.network/network/operation/monitoring_example_troubleshooting

Version: Ignition (v2.1.9)

On this page

## Complete Docker Compose ExampleHere's a complete example with all monitoring components integrated with your Aztec node:

```
services:  
  # Your Aztec node (example for full node)  
  aztec-node:  
    image: "aztecprotocol/aztec:2.1.9"  
    container_name: "aztec-node"  
    ports:  
      - ${AZTEC_PORT}:${AZTEC_PORT}  
      - ${P2P_PORT}:${P2P_PORT}  
      - ${P2P_PORT}:${P2P_PORT}/udp  
    volumes:  
      - ${DATA_DIRECTORY}:/var/lib/data  
    environment:  
      DATA_DIRECTORY: /var/lib/data  
      LOG_LEVEL: ${LOG_LEVEL}  
      ETHEREUM_HOSTS: ${ETHEREUM_HOSTS}  
      L1_CONSENSUS_HOST_URLS: ${L1_CONSENSUS_HOST_URLS}  
      P2P_IP: ${P2P_IP}  
      P2P_PORT: ${P2P_PORT}  
      AZTEC_PORT: ${AZTEC_PORT}  
      OTEL_EXPORTER_OTLP_METRICS_ENDPOINT: http://otel-collector:4318/v1/metrics  
    entrypoint: >-  
      node  
      --no-warnings  
      /usr/src/yarn-project/aztec/dest/bin/index.js  
      start  
      --node  
      --archiver  
      --network mainnet  
    networks:  
      - aztec  
    restart: always  
  
  # OpenTelemetry Collector  
  otel-collector:  
    image: otel/opentelemetry-collector  
    container_name: aztec-otel  
    ports:  
      - 8888:8888  
      - 8889:8889  
      - 4317:4317  
      - 4318:4318  
    volumes:  
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml  
    command: >-  
      --config=/etc/otel-collector-config.yaml  
    networks:  
      - aztec  
    restart: always  
  
  # Prometheus  
  prometheus:  
    image: prom/prometheus:latest  
    container_name: aztec-prometheus  
    ports:  
      - 9090:9090  
    volumes:  
      - ./prometheus.yml:/etc/prometheus/prometheus.yml  
      - prometheus-data:/prometheus  
    command:  
      - "--config.file=/etc/prometheus/prometheus.yml"  
      - "--storage.tsdb.path=/prometheus"  
      - "--storage.tsdb.retention.time=30d"  
    networks:  
      - aztec  
    restart: always  
  
  # Grafana  
  grafana:  
    image: grafana/grafana:latest  
    container_name: aztec-grafana  
    ports:  
      - 3000:3000  
    volumes:  
      - grafana-data:/var/lib/grafana  
    environment:  
      - GF_SECURITY_ADMIN_PASSWORD=your-secure-password  
      - GF_USERS_ALLOW_SIGN_UP=false  
    networks:  
      - aztec  
    restart: always  
  
volumes:  
  prometheus-data:  
  grafana-data:  
  
networks:  
  aztec:  
    name: aztec
```

This configuration includes:

* Your Aztec node configured to export metrics to the OTEL collector
* OpenTelemetry Collector to receive and process metrics
* Prometheus to store time-series data with 30-day retention
* Grafana for visualization and alerting
* Persistent volumes for Prometheus and Grafana data
* All services on the same Docker network for easy communication

## Troubleshooting## Metrics not appearing**Issue**: No metrics showing in Prometheus or Grafana.

**Solutions**:

* Verify OTEL collector is running: `docker compose ps otel-collector`
* Check OTEL collector logs: `docker compose logs otel-collector`
* Verify node is configured with correct OTEL endpoints
* Test OTEL collector endpoint: `curl http://localhost:8889/metrics`
* Ensure all containers are on the same Docker network

## Prometheus target down**Issue**: Prometheus shows target as "down" in Status → Targets.

**Solutions**:

* Verify OTEL collector is running and exposing port 8889
* Check Prometheus configuration in `prometheus.yml`
* Ensure target address is correct (use service name if in same Docker network)
* Review Prometheus logs: `docker compose logs prometheus`

## Grafana cannot connect to Prometheus**Issue**: Grafana shows "Bad Gateway" or cannot query Prometheus.

**Solutions**:

* Verify Prometheus is running: `docker compose ps prometheus`
* Check data source URL in Grafana (should be `http://prometheus:9090`)
* Test Prometheus endpoint: `curl http://localhost:9090/api/v1/query?query=up`
* Ensure Grafana and Prometheus are on the same Docker network

## Next Steps* Set up alerting rules in Prometheus for critical conditions
* Create custom dashboards for your specific monitoring needs
* Configure notification channels (Slack, PagerDuty, email) in Grafana
* Explore advanced PromQL queries for deeper insights
* Join the [Aztec Discord](https://discord.gg/aztec) to share dashboards with the community

---


# Keystore Usage

Source: https://docs.aztec.network/network/operation/keystore

Version: Ignition (v2.1.9)

On this page

## OverviewThe keystore manages private keys and addresses for your Aztec sequencer or prover. This guide covers advanced keystore configurations including secure key storage methods, multi-account setups, and production deployment patterns.

## PrerequisitesBefore proceeding, you should:

* Be familiar with running a sequencer or prover node
* Understand the basic keystore structure from the [sequencer setup guide](/network/setup/sequencer_management)
* Have access to appropriate key management infrastructure (if using remote signers)

## Understanding Keystore RolesThe keystore manages different types of keys depending on your node type. Understanding these roles helps you configure the right keys for your needs.

## Sequencer KeysWhen running a sequencer, you configure these keys and addresses:

* **Attester** (required): Your sequencer's identity. This key signs block proposals and attestations. The corresponding Ethereum address uniquely identifies your sequencer on the network.
* **Publisher** (optional): Submits block proposals to L1. Defaults to using the attester key if not specified. Must be funded with at least 0.1 ETH.
* **Coinbase** (optional): Ethereum address that receives L2 block rewards on L1. Defaults to the attester address if not set.
* **Fee Recipient** (required): Aztec address that receives unburnt L2 transaction fees from blocks you produce.

## Prover KeysProver nodes use a simpler configuration:

* **Prover ID**: Ethereum address identifying your prover and receiving rewards.
* **Publisher**: Submits proof transactions to L1. Must be funded with ETH for gas costs.

## Slasher KeysIf you're running a slasher to monitor the network:

* **Slasher**: Key used to create slash payloads on L1 when detecting sequencer misbehavior.

## What This Guide CoversThis guide walks you through advanced keystore configurations in three parts:

## 1. Key Storage MethodsLearn about different ways to store and access private keys:

* Inline private keys (for testing)
* Remote signers with Web3Signer (recommended for production Ethereum keys)
* JSON V3 encrypted keystores
* BIP44 mnemonic derivation

See [Key Storage Methods](/network/operation/keystore/storage_methods) for detailed instructions.

## 2. Advanced Configuration PatternsExplore complex deployment scenarios:

* Using multiple publisher accounts for load distribution
* Running multiple sequencers on a single node
* Infrastructure provider configurations
* High availability setups

See [Advanced Configuration Patterns](/network/operation/keystore/advanced_patterns) for examples.

## 3. TroubleshootingGet help with common issues:

* Keystore loading failures
* Key format validation
* Security best practices
* Permission problems

See [Troubleshooting](/network/operation/keystore/troubleshooting) for solutions.

## Getting Started**First time creating a keystore?** Start with the [Creating Validator Keystores guide](/network/operation/keystore/creating_keystores) to learn how to use the Aztec CLI to generate keystores for sequencers and provers.

Once you have a basic keystore, explore the [Key Storage Methods](/network/operation/keystore/storage_methods) guide to understand advanced options like remote signers and encrypted keystores. Then check out [Advanced Configuration Patterns](/network/operation/keystore/advanced_patterns) for complex deployment scenarios.

For production deployments, we strongly recommend using remote signers or encrypted keystores instead of inline private keys.

---


# Creating Sequencer Keystores

Source: https://docs.aztec.network/network/operation/keystore/creating_keystores

Version: Ignition (v2.1.9)

On this page

## OverviewKeystores are configuration files that store the cryptographic keys and addresses your sequencer node needs to operate on the Aztec network. This guide shows you how to create keystores using the Aztec CLI's `validator-keys` commands.

A keystore contains:

* **Attester keys**: Your sequencer's identity (Ethereum and BLS keys for signing proposals and attestations)
* **Publisher keys**: Keys used to submit blocks to L1 (requires ETH for gas)
* **Fee recipient**: Aztec address for L2 transaction fees (currently not used)
* **Coinbase address**: Ethereum address receiving L1 block rewards (optional, defaults to attester address)

## PrerequisitesBefore creating keystores, ensure you have:

* Basic understanding of Ethereum addresses and private keys
* Access to an Ethereum L1 RPC endpoint
* Foundry toolkit installed (for creating publisher addresses)

## Installing the Aztec CLIFirst, install the Aztec CLI using the official installer:

```
bash -i <(curl -s https://install.aztec.network)
```

Then install the correct version for the current network:

```
aztec-up 2.1.7
```

Verify your CLI installation:

```
aztec --version
```

## Recommended Setup: Multiple Validators with Shared PublisherThis approach creates multiple sequencer identities (validators) that share a single publisher address for submitting transactions to L1. This is the recommended configuration for production deployments.

## Step 1: Create Publisher Address and Set RPC EndpointFirst, set your Ethereum L1 RPC endpoint:

```
export ETH_RPC=https://ethereum-rpc.publicnode.com
```

Or use your preferred Ethereum RPC provider (Infura, Alchemy, etc.).

Then generate a separate address for publishing transactions to L1 using the Foundry toolkit:

```
cast wallet new-mnemonic --words 24
```

**Example output:**

```
Successfully generated a new mnemonic.  
Phrase:  
word1 word2 word3 word4 word5 word6 word7 word8 word9 word10 word11 word12 word13 word14 word15 word16 word17 word18 word19 word20 word21 word22 word23 word24  
  
Accounts:  
- Account 0:  
Address:     0xE434A95e816991E66bF7052955FD699aEf8a286b  
Private key: 0x7988a4a7...79f058a0
```

Critical: Save Your Publisher Mnemonic

The 24-word mnemonic is the **only way** to recover your publisher private key. Store it securely offline (not on the server running the node).

**Save from the output:**

* ✅ The 24-word mnemonic (for recovery)
* ✅ The private key (you'll use this in the next step)
* ✅ The address (you'll fund this with ETH)

## Step 2: Generate Your Keystores with PublisherGenerate 5 validators with the publisher private key from Step 1:

```
aztec validator-keys new \  
  --fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000 \  
  --staker-output \  
  --gse-address 0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f \  
  --l1-rpc-urls $ETH_RPC \  
  --count 5 \  
  --publishers 0x7988a4a779f058a0
```

Replace `0x7988a4a779f058a0` with your actual publisher private key from Step 1.

**What this command does:**

* Generates a new mnemonic for your validator keys (save this securely!)
* Creates 5 sequencer identities (validators) with Ethereum and BLS keys
* Configures all validators to use the same publisher address for L1 submissions
* Generates public keystore data for the staking dashboard
* Saves files to `~/.aztec/keystore/`

**Example output:**

```
No mnemonic provided, generating new one...  
Using new mnemonic:  
  
absent city nephew garment million badge front text memory grape two lizard  
  
Wrote validator keystore to /Users/your-name/.aztec/keystore/key1.json  
Wrote staker output for 5 validator(s) to /Users/your-name/.aztec/keystore/key1_staker_output.json  
  
acc1:  
  attester:  
    eth: 0x8E76a8B8D66E0A56E241F2768fD2ad4eba07E565  
    bls: 0x29eaf46e4699e33a1abe7300258567c624a7304a2134e31aa2609437f281d81d  
  publisher:  
    - 0x7988a4a779f058a0  
acc2:  
  attester:  
    eth: 0x2037b472537a4246B1A7325f327028EF450ba0Ef  
    bls: 0x8d7eb7d9436ac6cb9b8f1c211673ea228c7f438882e6438b2caefca753df28e8  
  publisher:  
    - 0x7988a4a779f058a0  
acc3:  
  attester:  
    eth: 0x0c14593f7465DeDbb86d68982374BB05F4C60386  
    bls: 0xad1cccf512d2f180238af795831344445f7ac47e2d623f3dac854e93e5b1e76d  
  publisher:  
    - 0x7988a4a779f058a0  
acc4:  
  attester:  
    eth: 0x4D213928988f0123f6b3B4A377F856812F08E831  
    bls: 0xa90f5889dddd4cd6bc5a28db5e0db60d3cbf5147eb6e82b313024b2d0634110e  
  publisher:  
    - 0x7988a4a779f058a0  
acc5:  
  attester:  
    eth: 0x29f147Da38d5F66bB84e791969b365c796829c92  
    bls: 0x0d683001c2ce866e322f0c7509f087a909508787d125336931aa9168d2a1f95b  
  publisher:  
    - 0x7988a4a779f058a0  
  
Note: The publisher value shown is the private key (truncated in this example). All validators share the same publisher private key.  
  
Staker outputs:  
[  
  {  
    "attester": "0x8E76a8B8D66E0A56E241F2768fD2ad4eba07E565",  
    "publicKeyG1": { "x": "0x...", "y": "0x..." },  
    "publicKeyG2": { "x0": "0x...", "x1": "0x...", "y0": "0x...", "y1": "0x..." },  
    "proofOfPossession": { "x": "0x...", "y": "0x..." }  
  },  
  ... (4 more validators)  
]
```

Critical: Save Both Mnemonics

You now have **two separate mnemonics** to secure:

1. **Validator mnemonic** (shown above, 12 words) - Regenerates your attester keys
2. **Publisher mnemonic** (from Step 1, 24 words) - Regenerates your publisher key

Both must be stored securely offline. Losing either mnemonic means losing access to those keys.

**Files created:**

* `~/.aztec/keystore/key1.json` - Private keystore with all 5 validators and publisher configured
* `~/.aztec/keystore/key1_staker_output.json` - Public keystore for staking dashboard

## Step 3: Fund the Publisher AddressYour publisher address needs ETH to pay for L1 gas when submitting proposals.

**Funding requirement:** At least **0.3 ETH** for 5 validators (rule of thumb: 0.1 ETH per validator)

Transfer ETH to the publisher address from Step 1. You can check the balance with:

```
cast balance 0xE434A95e816991E66bF7052955FD699aEf8a286b --rpc-url $ETH_RPC
```

Replace the address with your actual publisher address.

Monitor Publisher Balance

Set up monitoring to alert when the publisher balance falls below 0.5 ETH to prevent failed block publications.

## Step 4: Upload Keystore to Your NodeNow you're ready to spin up your sequencer node!

**Upload the private keystore to your server:**

The `key1.json` file contains your private keys and must be uploaded to your sequencer node.

**For standard server deployments:**

```
# Upload to your server's keystore directory  
scp ~/.aztec/keystore/key1.json user@your-server:/path/to/aztec-sequencer/keys/keystore.json
```

**For dAppNode deployments:**

* Upload `key1.json` to the dAppNode keystore folder
* Rename it to `keystore.json`

Keep the Public Keystore Local

Keep `key1_staker_output.json` on your local machine - you'll need it for registration on the staking dashboard. **Do not upload this to your server.**

## Step 5: Start Your NodeStart your sequencer node following the [Sequencer Management guide](/network/setup/sequencer_management).

When your node starts successfully, you'll see output similar to:

```
Started validator with addresses: 0x8E76a8B8D66E0A56E241F2768fD2ad4eba07E565, 0x2037b472537a4246B1A7325f327028EF450ba0Ef, 0x0c14593f7465DeDbb86d68982374BB05F4C60386, 0x4D213928988f0123f6b3B4A377F856812F08E831, 0x29f147Da38d5F66bB84e791969b365c796829c92
```

These are your validator attester addresses - they match the addresses shown when you generated your keys.

## Step 6: Register Your ValidatorsUse the public keystore (`key1_staker_output.json`) to register your validators on the staking dashboard. See [Registering a Sequencer](/network/setup/sequencer_management#next-steps-registering-your-sequencer) for details.

---

## Quick Setup SummaryBy following the recommended setup, you've accomplished:

✅ **Generated a dedicated publisher address** with its own 24-word mnemonic
✅ **Created 5 validator identities** with a separate 12-word mnemonic
✅ **Configured all validators** to use the shared publisher for L1 transactions
✅ **Funded the publisher** with at least 0.3 ETH for gas costs
✅ **Uploaded the private keystore** (`key1.json`) to your sequencer node
✅ **Started your node** and verified validator addresses in the output
✅ **Ready to register** using the public keystore (`key1_staker_output.json`)

**Two mnemonics to keep secure:**

1. **Publisher mnemonic** (24 words) - Recovers publisher private key
2. **Validator mnemonic** (12 words) - Recovers all 5 validator attester keys

## Alternative: Single Validator SetupFor testing or simpler setups, you can create a single validator that uses its attester key as the publisher.

## Basic Single Validator```
aztec validator-keys new \  
  --fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000 \  
  --staker-output \  
  --gse-address 0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f \  
  --l1-rpc-urls $ETH_RPC
```

This creates:

* One validator with attester keys
* No separate publisher (attester key used for publishing)
* Private keystore at `~/.aztec/keystore/keyN.json`
* Public keystore at `~/.aztec/keystore/keyN_staker_output.json`

When to Use Single Validator

Use single validator setup for:

* Testing and development
* Simple deployments with one sequencer identity
* When you don't need to isolate attester and publisher keys

## Understanding Keystore Structure## Private Keystore FormatThe private keystore (`key1.json`) contains sensitive private keys:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0x...",  // Ethereum private key - sequencer identifier  
        "bls": "0x..."   // BLS private key - signs proposals and attestations  
      },  
      "publisher": ["0x..."],  // Publisher private key(s) for L1 submissions  
      "feeRecipient": "0x0000000000000000000000000000000000000000000000000000000000000000",  
      "coinbase": "0x..."  // Optional: custom address for L1 rewards  
    }  
  ]  
}
```

**Field descriptions:**

* **attester.eth**: Derives the address that serves as your sequencer's unique identifier
* **attester.bls**: Signs proposals and attestations, used for staking operations
* **publisher**: Array of private keys for submitting signed messages to L1 (pays gas)
* **feeRecipient**: L2 fee recipient (not currently used, set to all zeros)
* **coinbase**: L1 block reward recipient (optional, defaults to attester address)

## Public Keystore FormatThe public keystore (`key1_staker_output.json`) contains only public information safe to share:

```
[  
  {  
    "attester": "0xYOUR_ATTESTER_ADDRESS",  
    "publicKeyG1": {  
      "x": "0x...",  
      "y": "0x..."  
    },  
    "publicKeyG2": {  
      "x0": "0x...",  
      "x1": "0x...",  
      "y0": "0x...",  
      "y1": "0x..."  
    },  
    "proofOfPossession": {  
      "x": "0x...",  
      "y": "0x..."  
    }  
  }  
]
```

This file is used for registration on the staking dashboard and contains no private keys.

## Advanced Options## Providing Your Own MnemonicFor deterministic key generation or to recreate keys from an existing mnemonic:

```
aztec validator-keys new \  
  --fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000 \  
  --staker-output \  
  --gse-address 0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f \  
  --l1-rpc-urls $ETH_RPC \  
  --mnemonic "your existing twelve word mnemonic phrase here" \  
  --count 5 \  
  --publishers 0x7988a4a779f058a0
```

This regenerates the same validators if you've used this mnemonic before, or creates new ones at the next derivation indices.

## Custom Output LocationSpecify custom directory and filename:

```
aztec validator-keys new \  
  --fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000 \  
  --staker-output \  
  --gse-address 0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f \  
  --l1-rpc-urls $ETH_RPC \  
  --count 5 \  
  --publishers 0x7988a4a779f058a0 \  
  --data-dir ~/my-sequencer/keys \  
  --file sequencer1.json
```

This creates keystores at:

* `~/my-sequencer/keys/sequencer1.json` (private keystore)
* `~/my-sequencer/keys/sequencer1_staker_output.json` (public keystore)

**Default behavior** (if you don't specify `--data-dir` or `--file`):

* **Directory**: `~/.aztec/keystore/`
* **Filename**: `key1.json`, `key2.json`, etc. (auto-increments)

## Verifying Your Keystore[  ​](#verifying-your-keystore "Direct link to Verifying Your Keystore")

Verify the keystore is valid JSON:

```
cat ~/.aztec/keystore/key1.json | jq .
```

Check validator count:

```
jq '.validators | length' ~/.aztec/keystore/key1.json
```

Verify BLS keys are present:

```
jq '.validators[0].attester.bls' ~/.aztec/keystore/key1.json
```

Extract attester addresses:

```
# Get attester ETH private key (to derive address)  
jq -r '.validators[0].attester.eth' ~/.aztec/keystore/key1.json
```

## TroubleshootingIf you encounter issues during keystore creation or management, see the **[Troubleshooting and Best Practices Guide](/network/operation/keystore/troubleshooting)** for:

* Keystore creation issues (RPC, permissions, invalid JSON, legacy BLS keys)
* Runtime and operational issues (node startup, remote signers, nonce conflicts)
* Comprehensive security best practices
* Complete CLI reference

## Next StepsNow that you've created your keystores:

## For Sequencer Operators1. **Fund publisher addresses** - At least 0.1 ETH per validator
2. **Set up your node** - See [Sequencer Management](/network/setup/sequencer_management)
3. **Register validators** - Use the public keystore with the staking dashboard
4. **Monitor operations** - Track attestations and publisher balance

## Advanced Configurations* **[Advanced Keystore Patterns](/network/operation/keystore/advanced_patterns)** - Multiple validators, high availability, remote signers
* **[Key Storage Methods](/network/operation/keystore/storage_methods)** - Encrypted keystores, HSMs, key management systems
* **[Troubleshooting and Best Practices](/network/operation/keystore/troubleshooting)** - Common issues, security best practices, and CLI reference

## Getting Help* Review the [Operator FAQ](/network/operation/operator_faq) for common questions
* Join the [Aztec Discord](https://discord.gg/aztec) for operator support
* Check the [CLI reference](/network/reference/cli_reference) for all available commands

---


# Key storage methods

Source: https://docs.aztec.network/network/operation/keystore/storage_methods

Version: Ignition (v2.1.9)

On this page

## OverviewThe keystore supports four methods for storing and accessing private keys. These methods can be mixed within a single configuration.

## Private keys (inline)")

The simplest method is to include private keys directly in the keystore. The `validator-keys new` command generates keystores in this format by default:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xef17bcb86452f3f6a73678c01bee757e9d46d1cd0050f043c10cfc953b17bad2",  
        "bls": "0x20f2f5989b66462b39229900948c7846403768fec5b76d1c2937d64e04aac4b9"  
      },  
      "feeRecipient": "0x0000000000000000000000000000000000000000000000000000000000000000"  
    }  
  ]  
}
```

Note that the attester field now contains both Ethereum (`eth`) and BLS (`bls`) private keys. Both are required for sequencer operation.

Not for Production Use

Inline private keys are convenient for testing but should be avoided in production. Use remote signers or encrypted keystores for production deployments.

## Remote signers (Web3Signer)")

Remote signers keep private keys in a separate, secure signing service. This is the recommended approach for production environments for Ethereum keys.

The keystore supports [Web3Signer](https://docs.web3signer.consensys.io/) endpoints for Ethereum keys. The keystore automatically detects whether a value is a private key or an address based on string length:

* **66 characters** (`0x` + 64 hex characters): Interpreted as a private key (stored inline)
* **42 characters** (`0x` + 40 hex characters): Interpreted as an address and uses the nearest `remoteSignerUrl`

BLS Keys Do Not Support Remote Signers

BLS keys must always be stored as private keys directly in the keystore. The keystore does not check `remoteSignerUrl` for BLS keys. Web3Signer's BLS support is designed for Ethereum consensus layer operations and is not compatible with Aztec's BLS key requirements.

Remote signers can be configured at three levels:

**Global level** (applies to all ETH keys):

```
{  
  "schemaVersion": 1,  
  "remoteSigner": "https://signer.example.com:8080",  
  "validators": [  
    {  
      "attester": {  
        "eth": "0x1234567890123456789012345678901234567890",  
        "bls": "0x20f2f5989b66462b39229900948c7846403768fec5b76d1c2937d64e04aac4b9"  
      },  
      "feeRecipient": "0x1234567890123456789012345678901234567890123456789012345678901234"  
    }  
  ]  
}
```

In this example, the Ethereum attester address (42 characters) is managed by the remote signer, while the BLS key (66 characters) is a private key stored directly in the keystore.

**Validator (sequencer) block level** (applies to all ETH keys in a sequencer configuration):

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0x1234567890123456789012345678901234567890",  
        "bls": "0x20f2f5989b66462b39229900948c7846403768fec5b76d1c2937d64e04aac4b9"  
      },  
      "feeRecipient": "0x1234567890123456789012345678901234567890123456789012345678901234",  
      "remoteSigner": "https://signer.example.com:8080"  
    }  
  ]  
}
```

**Account level** (applies to a specific ETH key):

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": {  
          "address": "0x1234567890123456789012345678901234567890",  
          "remoteSignerUrl": "https://signer.example.com:8080"  
        },  
        "bls": "0x20f2f5989b66462b39229900948c7846403768fec5b76d1c2937d64e04aac4b9"  
      },  
      "feeRecipient": "0x1234567890123456789012345678901234567890123456789012345678901234"  
    }  
  ]  
}
```

## Client certificate authenticationFor remote signers requiring client certificates:

```
{  
  "schemaVersion": 1,  
  "remoteSigner": {  
    "remoteSignerUrl": "https://signer.example.com:8080",  
    "certPath": "/path/to/client-cert.p12",  
    "certPass": "certificate-password"  
  },  
  "validators": [...]  
}
```

## JSON V3 encrypted keystoresJSON V3 keystores provide standard Ethereum-compatible encrypted key storage.

**Single file:**

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "path": "/path/to/keystore.json",  
        "password": "keystore-password"  
      },  
      "feeRecipient": "0x1234567890123456789012345678901234567890123456789012345678901234"  
    }  
  ]  
}
```

**Directory of keystores:**

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0x1234567890123456789012345678901234567890123456789012345678901234",  
        "bls": "0x2345678901234567890123456789012345678901234567890123456789012345"  
      },  
      "publisher": {  
        "path": "/path/to/keystores/",  
        "password": "shared-password"  
      },  
      "feeRecipient": "0x1234567890123456789012345678901234567890123456789012345678901234"  
    }  
  ]  
}
```

All `.json` files in the directory will be loaded using the provided password.

## Mnemonics (BIP44 derivation)")

Mnemonics derive multiple keys from a single seed phrase using [BIP44](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki) paths.

**Single key** (default path `m/44'/60'/0'/0/0`):

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0x1234567890123456789012345678901234567890123456789012345678901234",  
        "bls": "0x2345678901234567890123456789012345678901234567890123456789012345"  
      },  
      "publisher": {  
        "mnemonic": "test test test test test test test test test test test junk"  
      },  
      "feeRecipient": "0x1234567890123456789012345678901234567890123456789012345678901234"  
    }  
  ]  
}
```

**Multiple sequential keys:**

```
{  
  "publisher": {  
    "mnemonic": "test test test test test test test test test test test junk",  
    "addressCount": 4  
  }  
}
```

Generates 4 keys at paths `m/44'/60'/0'/0/0` through `m/44'/60'/0'/0/3`.

**Custom derivation paths:**

```
{  
  "publisher": {  
    "mnemonic": "test test test test test test test test test test test junk",  
    "accountIndex": 5,  
    "addressIndex": 3,  
    "addressCount": 2  
  }  
}
```

Not for Production Use

Mnemonics are convenient for testing but should be avoided in production. Use remote signers or encrypted keystores for production deployments.

## Next steps* Learn about [Advanced Configuration Patterns](/network/operation/keystore/advanced_patterns)
* See [Troubleshooting](/network/operation/keystore/troubleshooting) if you encounter issues

---


# Sample configuration patterns

Source: https://docs.aztec.network/network/operation/keystore/advanced_patterns

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide covers advanced keystore configuration patterns for complex deployments, including multi-publisher setups, running multiple sequencers, and infrastructure provider scenarios.

## Multiple publishersMultiple publisher accounts provide:

* **Load distribution**: Spread L1 transaction costs across accounts
* **Parallelization**: Submit multiple transactions simultaneously
* **Resilience**: Continue operating if one publisher runs out of gas

**Array of publishers:**

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xATTESTER_ETH_PRIVATE_KEY",  
        "bls": "0xATTESTER_BLS_PRIVATE_KEY"  
      },  
      "publisher": [  
        "0xPUBLISHER_1_PRIVATE_KEY",  
        "0xPUBLISHER_2_PRIVATE_KEY",  
        "0xPUBLISHER_3_PRIVATE_KEY"  
      ],  
      "feeRecipient": "0x1234567890123456789012345678901234567890123456789012345678901234"  
    }  
  ]  
}
```

**Mixed storage methods:**

```
{  
  "schemaVersion": 1,  
  "remoteSigner": "https://signer1.example.com:8080",  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xATTESTER_ETH_PRIVATE_KEY",  
        "bls": "0xATTESTER_BLS_PRIVATE_KEY"  
      },  
      "publisher": [  
        "0xLOCAL_PRIVATE_KEY",  
        "0xREMOTE_SIGNER_ADDRESS_1",  
        {  
          "address": "0xREMOTE_SIGNER_ADDRESS_2",  
          "remoteSignerUrl": "https://signer2.example.com:8080"  
        },  
        {  
          "mnemonic": "test test test test test test test test test test test junk",  
          "addressCount": 2  
        }  
      ],  
      "feeRecipient": "0x1234567890123456789012345678901234567890123456789012345678901234"  
    }  
  ]  
}
```

This creates 5 publishers:

1. Local private key
2. Address in default remote signer (signer1.example.com)
3. Address in alternative remote signer (signer2.example.com)
4. Two mnemonic-derived addresses

Publisher Funding Required

All publisher accounts must be funded with ETH. Monitor balances to avoid missed proposals or proofs.

## Multiple sequencersRun multiple sequencer identities in a single node. This is useful when you operate multiple sequencers but want to consolidate infrastructure.

High Availability Across Nodes

If you want to run the **same** sequencer across multiple nodes for redundancy and high availability, see the [High Availability Sequencers guide](/network/setup/high_availability_sequencers). That guide covers running one sequencer identity on multiple physical nodes.

This section covers running **multiple different sequencer identities** on a single node.

**When to use multiple sequencers per node:**

* You have multiple sequencer identities (different attester addresses)
* You want to consolidate infrastructure and reduce operational overhead
* You're running sequencers for multiple entities or clients
* You want to simplify management of several sequencers

**Use two approaches:**

**Option 1: Shared configuration**

Multiple attesters sharing the same publisher, coinbase, and fee recipient:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": [  
        {  
          "eth": "0xSEQUENCER_1_ETH_KEY",  
          "bls": "0xSEQUENCER_1_BLS_KEY"  
        },  
        {  
          "eth": "0xSEQUENCER_2_ETH_KEY",  
          "bls": "0xSEQUENCER_2_BLS_KEY"  
        }  
      ],  
      "publisher": ["0xSHARED_PUBLISHER"],  
      "coinbase": "0xSHARED_COINBASE",  
      "feeRecipient": "0xSHARED_FEE_RECIPIENT"  
    }  
  ]  
}
```

**Option 2: Separate configurations**

Each sequencer with its own publisher, coinbase, and fee recipient:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xSEQUENCER_1_ETH_KEY",  
        "bls": "0xSEQUENCER_1_BLS_KEY"  
      },  
      "publisher": ["0xPUBLISHER_1"],  
      "coinbase": "0xCOINBASE_1",  
      "feeRecipient": "0xFEE_RECIPIENT_1"  
    },  
    {  
      "attester": {  
        "eth": "0xSEQUENCER_2_ETH_KEY",  
        "bls": "0xSEQUENCER_2_BLS_KEY"  
      },  
      "publisher": ["0xPUBLISHER_2"],  
      "coinbase": "0xCOINBASE_2",  
      "feeRecipient": "0xFEE_RECIPIENT_2"  
    }  
  ]  
}
```

For high availability configurations where you run the same sequencer across multiple nodes, see the [High Availability Sequencers guide](/network/setup/high_availability_sequencers).

## Infrastructure provider scenarios## Scenario 1: Multiple sequencers with isolationFor sequencers requiring complete separation, use separate keystore files:

**keystore-sequencer-a.json:**

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xSEQUENCER_A_ETH_KEY",  
        "bls": "0xSEQUENCER_A_BLS_KEY"  
      },  
      "feeRecipient": "0xFEE_RECIPIENT_A"  
    }  
  ]  
}
```

**keystore-sequencer-b.json:**

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xSEQUENCER_B_ETH_KEY",  
        "bls": "0xSEQUENCER_B_BLS_KEY"  
      },  
      "feeRecipient": "0xFEE_RECIPIENT_B"  
    }  
  ]  
}
```

Point `KEY_STORE_DIRECTORY` to the directory containing both files.

## Scenario 2: Shared publisher infrastructureMultiple sequencers sharing a publisher pool for simplified gas management:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "0xSEQUENCER_1_ETH_KEY",  
        "bls": "0xSEQUENCER_1_BLS_KEY"  
      },  
      "publisher": ["0xPUBLISHER_1", "0xPUBLISHER_2"],  
      "feeRecipient": "0xFEE_RECIPIENT_1"  
    },  
    {  
      "attester": {  
        "eth": "0xSEQUENCER_2_ETH_KEY",  
        "bls": "0xSEQUENCER_2_BLS_KEY"  
      },  
      "publisher": ["0xPUBLISHER_1", "0xPUBLISHER_2"],  
      "feeRecipient": "0xFEE_RECIPIENT_2"  
    }  
  ]  
}
```

Both sequencers share publishers while maintaining separate identities and fee recipients.

## Prover configurations**Simple prover** (uses same key for identity and publishing):

```
{  
  "schemaVersion": 1,  
  "prover": "0xPROVER_PRIVATE_KEY"  
}
```

**Prover with dedicated publishers:**

```
{  
  "schemaVersion": 1,  
  "prover": {  
    "id": "0xPROVER_IDENTITY_ADDRESS",  
    "publisher": [  
      "0xPUBLISHER_1_PRIVATE_KEY",  
      "0xPUBLISHER_2_PRIVATE_KEY"  
    ]  
  }  
}
```

The `id` receives prover rewards while `publisher` accounts submit proofs.

## Complete Configuration Examples## High Availability Sequencer SetupCreating keystores for running the same sequencer across multiple nodes:

```
# Step 1: Generate a base keystore with your attester and multiple publishers  
aztec validator-keys new \  
  --fee-recipient [YOUR_FEE_RECIPIENT] \  
  --mnemonic "your shared mnemonic..." \  
  --address-index 0 \  
  --publisher-count 3 \  
  --data-dir ~/keys-temp  
  
# This generates ONE keystore with:  
# - Attester keys (ETH and BLS) at derivation index 0  
# - Three publisher keys at indices 1, 2, and 3
```

After generation, you'll have a keystore with one attester and multiple publishers. Create separate keystores for each node by copying the base keystore and editing each to use only one publisher:

**Node 1** - Uses publisher at index 1
**Node 2** - Uses publisher at index 2
**Node 3** - Uses publisher at index 3

Each node's keystore will have the **same attester keys** (both ETH and BLS) but a **different publisher key**.

For detailed step-by-step HA setup instructions, see the [High Availability Sequencers guide](/network/setup/high_availability_sequencers).

## Next steps* See [Troubleshooting](/network/operation/keystore/troubleshooting) for common issues
* Return to [Key Storage Methods](/network/operation/keystore/storage_methods) for more options
* Start with basics at [Creating Keystores](/network/operation/keystore/creating_keystores)

---


# Troubleshooting and Best Practices

Source: https://docs.aztec.network/network/operation/keystore/troubleshooting

Version: Ignition (v2.1.9)

On this page

## Keystore Creation Issues## Missing fee-recipient Flag**Error message:**

```
error: required option '--fee-recipient <address>' not specified
```

**Solution:** The CLI requires the `--fee-recipient` flag. Use the zero address:

```
--fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000
```

## RPC Connection Issues**Error message:**

```
Error: HTTP request failed
```

**Solutions:**

* Verify `$ETH_RPC` is set correctly: `echo $ETH_RPC`
* Test RPC connectivity: `cast block-number --rpc-url $ETH_RPC`
* Try a different RPC provider if the current one is rate-limited

## Permission Denied**Error message:**

```
Error: permission denied
```

**Solution:** Ensure you have write permissions for the target directory:

```
mkdir -p ~/.aztec/keystore  
chmod 755 ~/.aztec/keystore
```

## Invalid Keystore JSON**Error:** Node fails to load keystore or CLI rejects keystore file

**Solutions:**

* Validate JSON syntax: `jq . ~/.aztec/keystore/key1.json`
* Ensure all required fields are present
* Check that publisher is an array: `["0x..."]` not `"0x..."`
* Verify private keys are 64-character hex strings (with or without `0x` prefix)

## Legacy BLS Key Derivation (2.1.4 Users)")

**Issue:** Need to regenerate keys that were created with CLI version 2.1.4 or earlier

Version 2.1.5 changed the BLS key derivation path, which means keys generated from the same mnemonic produce different results. This affects users who:

* Generated keys with version 2.1.4 using `--count` parameter
* Used `--account-index` explicitly in version 2.1.4
* Need to regenerate keys from mnemonic that are already registered in the GSE contract

**The derivation path change:**

* **2.1.4**: `m/12381/3600/0/0/0`, `m/12381/3600/1/0/0`, `m/12381/3600/2/0/0`
* **2.1.5+**: `m/12381/3600/0/0/0`, `m/12381/3600/0/0/1`, `m/12381/3600/0/0/2`

**Solution: Use the --legacy flag**

If you generated keys with version 2.1.4 and need to regenerate them from your mnemonic, use the `--legacy` flag:

```
aztec validator-keys new \  
  --fee-recipient 0x0000000000000000000000000000000000000000000000000000000000000000 \  
  --staker-output \  
  --gse-address 0xa92ecFD0E70c9cd5E5cd76c50Af0F7Da93567a4f \  
  --l1-rpc-urls $ETH_RPC \  
  --mnemonic "your twelve word mnemonic phrase here" \  
  --count 5 \  
  --legacy
```

The `--legacy` flag uses the 2.1.4 derivation path to reproduce your original keys.

When NOT to Use --legacy

Do NOT use the `--legacy` flag if:

* You're generating keys for the first time
* You generated keys with version 2.1.5 or later
* You didn't use `--count` or `--account-index` in version 2.1.4

Using `--legacy` unnecessarily will create keys with the old derivation path that won't match your newer registrations.

Why This Matters

BLS keys are registered in the GSE (Governance Staking Escrow) contract and cannot be easily updated. If you regenerate keys with a different derivation path, they won't match what's registered on chain, and your sequencer won't be able to attest properly.

## Runtime and Operational Issues## "No validators found in keystore"**Symptoms**: Node fails to start with no sequencer configurations loaded

**Causes**:

* Keystore file not found at specified path
* Invalid JSON syntax
* Missing required fields
* File permissions prevent reading

**Solutions**:

1. Verify keystore path:

```
ls -la $KEY_STORE_DIRECTORY
```

2. Validate JSON syntax:

```
cat keystore.json | jq .
```

3. Check required fields:

```
{  
  "schemaVersion": 1,  
  "validators": [  
    {  
      "attester": {  
        "eth": "REQUIRED - Ethereum private key",  
        "bls": "REQUIRED - BLS private key"  
      },  
      "feeRecipient": "REQUIRED - Aztec address"  
    }  
  ]  
}
```

4. Fix file permissions:

```
chmod 600 keystore.json  
chown aztec:aztec keystore.json
```

## "Failed to connect to remote signer"**Symptoms**: Node cannot reach Web3Signer endpoint

**Causes**:

* Incorrect URL or port
* Network connectivity issues
* Certificate validation failures
* Remote signer not running

**Solutions**:

1. Test connectivity:

```
curl https://signer.example.com:8080/upcheck
```

2. Verify certificate:

```
openssl s_client -connect signer.example.com:8080 -showcerts
```

3. Check remote signer logs for authentication errors
4. For self-signed certificates, ensure proper certificate configuration in keystore

## "Insufficient funds for gas"**Symptoms**: Transactions fail with insufficient balance errors

**Causes**:

* Publisher accounts not funded
* ETH balance depleted

**Solutions**:

1. Check publisher balances:

```
cast balance 0xPUBLISHER_ADDRESS --rpc-url $ETHEREUM_HOST
```

2. Fund publisher accounts with ETH
3. Set up automated balance monitoring and alerts

## "Nonce too low" or "Replacement transaction underpriced"**Symptoms**: Transaction submission failures related to nonces

**Causes**:

* Multiple nodes using same publisher key
* Publisher key reused across keystores
* Transaction pool issues

**Solutions**:

1. **Never share publisher keys across multiple running nodes**
2. If you must use the same key, ensure only one node is active at a time
3. Clear pending transactions if safe to do so

## "Keystore file not loaded"**Symptoms**: Only some keystores load from a directory

**Causes**:

* Invalid JSON in some files
* Incorrect file extensions
* Schema version mismatch

**Solutions**:

1. Check all files in directory:

```
for file in /path/to/keystores/*.json; do  
  echo "Checking $file"  
  jq . "$file" || echo "Invalid JSON in $file"  
done
```

2. Ensure all files use `.json` extension
3. Verify `schemaVersion: 1` in all keystores

## "Cannot decrypt JSON V3 keystore"**Symptoms**: Failed to load encrypted keystore files

**Causes**:

* Incorrect password
* Corrupted keystore file
* Unsupported encryption algorithm

**Solutions**:

1. Verify password is correct
2. Test decryption manually:

```
# Using ethereumjs-wallet or similar tool
```

3. Re-generate keystore if corrupted
4. Ensure keystore was generated using standard tools (geth, web3.py, ethers.js)

## Security Best Practices## Protecting Private Keys1. **Never commit keystores to version control**

   * Add `keystore.json` to `.gitignore`
   * Store keystores outside your project directory
2. **Backup your mnemonic securely**

   * Write it down offline
   * Store in a secure location (not on the server)
   * Consider using a hardware wallet or password manager
3. **Limit keystore access**

   ```
   chmod 600 ~/.aztec/keystore/key1.json
   ```
4. **Separate publisher from attester**

   * Use dedicated publisher keys
   * Keep attester keys offline when possible
   * Use remote signers for production

## Key Storage**DO:**

* Use remote signers (Web3Signer) for production deployments
* Store keystores in encrypted volumes
* Use JSON V3 keystores with strong passwords
* Restrict file permissions to 600 (owner read/write only)
* Keep backups of keystores in secure, encrypted locations

**DON'T:**

* Commit keystores or private keys to version control
* Store unencrypted private keys on disk
* Share private keys between nodes
* Use the same keys across test and production environments
* Log private keys or keystore passwords

## Publisher Key Management**DO:**

* Use separate publisher keys for each sequencer if possible
* Monitor publisher account balances with alerting
* Rotate publisher keys periodically
* Maintain multiple funded publishers for resilience
* Keep publisher keys separate from attester keys

**DON'T:**

* Reuse publisher keys across multiple nodes
* Run out of gas in publisher accounts
* Use sequencer attester keys as publishers if avoidable
* Share publisher keys between sequencers

## Remote Signer Security**DO:**

* Use TLS/HTTPS for all remote signer connections
* Implement client certificate authentication
* Run remote signers on isolated networks
* Monitor remote signer access logs
* Use firewall rules to restrict access

**DON'T:**

* Use unencrypted HTTP connections
* Expose remote signers to the public internet
* Share remote signer endpoints between untrusted parties
* Disable certificate verification

## Operational Security**DO:**

* Implement principle of least privilege for file access
* Use hardware security modules (HSMs) for high-value sequencers
* Maintain audit logs of key access and usage
* Test keystore configurations in non-production environments first
* Document your key management procedures

**DON'T:**

* Run nodes as root user
* Store passwords in shell history or scripts
* Share attester keys between sequencers
* Neglect monitoring and alerting

## Production DeploymentsFor production, consider:

* **Hardware Security Modules (HSMs)** for key storage
* **Remote signers** to keep keys off the node
* **Encrypted keystores** with password protection
* **Key management systems** (HashiCorp Vault, AWS Secrets Manager)

See [Key Storage Methods](/network/operation/keystore/storage_methods) for advanced security patterns.

## CLI Reference## validator-keys newCreate a new keystore with validators:

```
aztec validator-keys new [options]
```

**Common Options:**

| Option | Description | Default |
| --- | --- | --- |
| `--fee-recipient <address>` | L2 fee recipient (required) | None |
| `--mnemonic <phrase>` | 12 or 24 word mnemonic | Auto-generated |
| `--count <number>` | Number of validators to create | `1` |
| `--publisher-count <number>` | Publishers per validator | `0` |
| `--staker-output` | Generate public keystore for staking | `false` |
| `--gse-address <address>` | GSE contract address (required with --staker-output) | None |
| `--l1-rpc-urls <urls>` | L1 RPC endpoints (required with --staker-output) | None |
| `--legacy` | Use 2.1.4 BLS derivation path (only for regenerating old keys) | `false` |
| `--data-dir <path>` | Output directory | `~/.aztec/keystore` |
| `--file <name>` | Keystore filename | `key1.json` |

For the complete list:

```
aztec validator-keys new --help
```

## validator-keys addAdd validators to an existing keystore:

```
aztec validator-keys add <keystore-path> [options]
```

## validator-keys stakerGenerate staker output from an existing keystore:

```
aztec validator-keys staker \  
  --from <keystore-path> \  
  --gse-address <address> \  
  --l1-rpc-urls <url> \  
  --output <output-file>
```

## Getting HelpIf you encounter issues not covered here:

* Review the [Operator FAQ](/network/operation/operator_faq) for common questions
* Join the [Aztec Discord](https://discord.gg/aztec) for operator support
* Check the [CLI reference](/network/reference/cli_reference) for all available commands
* Review node logs for specific error messages (redact private keys!)
* When asking for help, provide:
  + Error messages (with private keys redacted)
  + Keystore structure (anonymized)
  + Node version and deployment environment

## Related Documentation* **[Creating Keystores](/network/operation/keystore/creating_keystores)** - Main guide for generating keystores
* **[Advanced Keystore Patterns](/network/operation/keystore/advanced_patterns)** - Multiple validators, high availability, remote signers
* **[Key Storage Methods](/network/operation/keystore/storage_methods)** - Encrypted keystores, HSMs, key management systems
* **[Sequencer Management](/network/setup/sequencer_management)** - Operational guidance for running sequencers

---


# Sequencer Management

Source: https://docs.aztec.network/network/operation/sequencer_management

Version: Ignition (v2.1.9)

On this page

## OverviewOnce your sequencer is running, you need to manage its ongoing operations. This guide covers sequencer management tasks including participating in governance, running with delegated stake, and querying contract state to monitor your sequencer's health and performance.

## PrerequisitesBefore proceeding, you should:

* Have a running sequencer node (see [Sequencer Setup Guide](/network/setup/sequencer_management))
* Be familiar with basic sequencer operations
* Have access to Foundry's `cast` tool for contract queries
* Understand your sequencer's role in the network

## Understanding Sequencer OperationsAs a sequencer operator, your responsibilities extend beyond simply running a node. You participate in network governance, manage your stake (whether self-funded or delegated), and monitor your sequencer's performance and status on the network.

## Key Management Areas**Governance Participation**: Sequencers play a crucial role in protocol governance. You signal support for protocol upgrades, vote on proposals, and help shape the network's evolution. Active participation ensures your voice is heard in decisions that affect the protocol.

**Stake Management**: Whether you're using your own stake or operating with delegated stake from others, you need to understand how staking works, monitor your balances, and ensure you maintain sufficient funds for operations.

**Operational Monitoring**: Regular monitoring of your sequencer's status, performance metrics, and onchain state helps you catch issues early and maintain optimal operations.

## What This Guide CoversThis guide walks you through sequencer management in four parts:

## 1. Governance and Proposal ProcessLearn how to participate in protocol governance:

* Understanding payloads and the governance lifecycle
* Signaling support for protocol upgrades
* Creating and voting on proposals
* Executing approved changes
* Upgrading your node after governance changes

See [Governance and Proposal Process](/network/operation/sequencer_management/creating_and_voting_on_proposals) for detailed instructions.

## 2. Running Delegated StakeIf you're operating a sequencer with delegated stake:

* Understanding the delegated stake model
* Registering as a provider with the Staking Registry
* Managing sequencer identities for delegation
* Updating provider configuration and commission rates
* Monitoring delegator relationships

See [Running Delegated Stake](/network/operation/sequencer_management/become_a_staking_provider) for setup instructions.

## 3. Claiming RewardsLearn how to claim your sequencer rewards:

* Understanding how rewards accumulate in the Rollup contract
* Checking reward claimability status and pending rewards
* Claiming rewards to your coinbase address
* Troubleshooting common claiming issues

See [Claiming Rewards](/network/operation/sequencer_management/claiming_rewards) for detailed instructions.

## 4. Useful CommandsEssential contract query commands for operators:

* Finding contract addresses (Registry, Rollup, Governance)
* Querying the sequencer set and individual sequencer status
* Checking governance signals and proposal counts
* Monitoring stake balances and voting power
* Troubleshooting common query issues

See [Useful Commands](/network/operation/sequencer_management/useful_commands) for a complete reference.

## Getting StartedStart with the [Useful Commands](/network/operation/sequencer_management/useful_commands) guide to learn how to query your sequencer's status and verify it's operating correctly. This helps you establish a baseline for monitoring.

If you're participating in governance, review the [Governance and Proposal Process](/network/operation/sequencer_management/creating_and_voting_on_proposals) guide to understand how to signal, vote, and execute proposals.

For operators running with delegated stake, the [Running Delegated Stake](/network/operation/sequencer_management/become_a_staking_provider) guide walks you through provider registration and management.

## Best Practices**Monitor Regularly**: Check your sequencer's status, balance, and attestation activity regularly. Set up alerts for critical thresholds like low balances or missed attestations.

**Participate in Governance**: Stay informed about governance proposals and participate in votes that affect your operations. Join the community discussions on Discord to understand proposed changes.

**Maintain Adequate Balances**: Ensure your publisher account always has sufficient ETH (at least 0.1 ETH) to avoid being slashed. Monitor balances and set up automated top-ups if possible.

**Keep Your Node Updated**: When governance proposals pass that require node upgrades, prepare during the execution delay period. Have a plan for coordinated upgrades to minimize downtime.

**Communicate with Delegators**: If you're running with delegated stake, maintain open communication with your delegators about performance, commission changes, and planned maintenance.

## Next Steps* Query your sequencer status using the [Useful Commands](/network/operation/sequencer_management/useful_commands)
* Learn about [governance participation](/network/operation/sequencer_management/creating_and_voting_on_proposals) to vote on protocol changes
* Set up [monitoring](/network/operation/monitoring) to track your sequencer's performance
* Join the [Aztec Discord](https://discord.gg/aztec) for operator support and community discussions

---


# Governance and Proposal Process

Source: https://docs.aztec.network/network/operation/sequencer_management/creating_and_voting_on_proposals

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide shows you how to participate in protocol governance as a sequencer. You'll learn how to signal support for protocol upgrades, create proposals, and vote on governance decisions that shape the Aztec network.

## PrerequisitesBefore proceeding, you should:

* Have a running sequencer node (see [Sequencer Setup Guide](/network/setup/sequencer_management))
* Have a basic understanding of Aztec's governance model and voting mechanisms

## Understanding Governance Components## PayloadsProtocol upgrades consist of a series of commands that execute on protocol contracts or replace contract references. You define these steps in a contract called a **payload** that you deploy on Ethereum.

This guide assumes the payload already exists at a known address. You'll participate in the payload's journey through signaling, proposal creation, voting, and execution.

Always Verify Payloads

Before signaling support or voting, always:

1. Verify the payload address on Etherscan or your preferred block explorer
2. Review the `getActions()` function to understand what changes the payload will make
3. Check if the payload has been audited (if applicable)
4. Discuss the proposal with the community on [Aztec Discord](https://discord.gg/aztec)

Never signal or vote for a payload you haven't personally verified.

Here's an example payload structure:

```
contract UpgradePayload is IPayload {  
  IRegistry public immutable REGISTRY;  
  address public NEW_ROLLUP = address(new FakeRollup());  
  
  constructor(IRegistry _registry) {  
    REGISTRY = _registry;  
  }  
  
  function getActions() external view override(IPayload) returns (IPayload.Action[] memory) {  
    IPayload.Action[] memory res = new IPayload.Action[](1);  
  
    res[0] = Action({  
      target: address(REGISTRY),  
      data: abi.encodeWithSelector(REGISTRY.addRollup.selector, NEW_ROLLUP)  
    });  
  
    return res;  
  }  
  
  function getURI() external pure override(IPayload) returns (string memory) {  
    return "UpgradePayload";  
  }  
}
```

If this payload's proposal passes governance voting, the governance contract executes `addRollup` on the `Registry` contract.

## Contract AddressesKey contracts you'll use:

* **Governance Proposer**: Handles payload signaling and proposal creation
* **Governance Staking Escrow (GSE)**: Manages stake delegation and voting
* **Governance**: Executes approved proposals
* **Rollup**: Your sequencer stakes here and defaults to delegating voting power here

**To obtain these contract addresses:** Check your sequencer logs at startup for the line beginning with `INFO: node Aztec Node started on chain...`

## Governance Lifecycle OverviewThe governance process follows these stages:

1. **Signaling**: Sequencers signal support for a payload when proposing blocks. A payload needs a quorum of support to be promoted to a proposal. Signaling can start any time from the moment a payload is deployed.
2. **Proposal Creation**: After reaching quorum, anyone can submit the payload as an official proposal.
3. **Voting Delay** (3 days): A mandatory waiting period before voting opens (allows time for community review).
4. **Voting Period** (7 days): Users who hold stake in the network vote on the proposal using their staked tokens. A proposal passes if it receives at least 20% quorum, 2/3 of votes are "yea", and a minimum of 500 validators' worth of voting power is cast.
5. **Execution Delay** (7 days): After passing the vote, another mandatory delay before execution (allows time for node upgrades).
6. **Execution**: Anyone can execute the proposal, which applies the changes. There is a 7-day grace period after the execution delay during which the proposal can still be executed.

## Signaling Support for a PayloadAs a sequencer, you initiate proposals through signaling. When you propose a block, you can automatically signal support for a specific payload. Once enough sequencers signal support within a round, the payload qualifies to become an official proposal.

## How Signaling Works* Only you can signal during slots when you're the block proposer
* Your sequencer node automatically calls `signal` on the `GovernanceProposer` contract when proposing a block (if you've configured a payload address)
* Rounds consist of 1000 slots each (20 hours at 72 seconds per slot). At every 1000-slot boundary, the system checks if any payload has received 600 or more signals (the quorum threshold, which is 60% of the round size)
* Payloads that reach quorum can be submitted as official proposals by anyone

## Configure Your Signaling PreferenceUse the `setConfig` method on your node's admin interface to specify which payload address you want to signal support for.

```
docker exec -it aztec-sequencer curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{  
    "jsonrpc":"2.0",  
    "method":"nodeAdmin_setConfig",  
    "params":[{"governanceProposerPayload":"0x1234567890abcdef1234567890abcdef12345678"}],  
    "id":1  
  }'
```

Replace `0x1234567890abcdef1234567890abcdef12345678` with your actual payload contract address and `aztec-sequencer` with your container name.

Expected response:

```
{"jsonrpc":"2.0","id":1}
```

## Verify Your ConfigurationUse the `getConfig` method to verify the payload address:

```
docker exec -it aztec-sequencer curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{  
    "jsonrpc":"2.0",  
    "method":"nodeAdmin_getConfig",  
    "id":1  
  }'
```

Search for `governanceProposerPayload` in the response to confirm it matches your configured address.

Once configured, your sequencer automatically signals support for this payload each time you propose a block. Each signal counts toward the quorum requirement.

## Creating a ProposalOnce a payload receives the required quorum (600 signals in a 1000-slot round), you or any user can call `submitRoundWinner` on the `GovernanceProposer` contract to officially create the proposal.

## Submit the Payload```
cast send [GOVERNANCE_PROPOSER_ADDRESS] \  
  "submitRoundWinner(uint256)" [ROUND_NUMBER] \  
  --rpc-url [YOUR_RPC_URL] \  
  --private-key [YOUR_PRIVATE_KEY]
```

To find the current round number:

```
# Get the current round from the GovernanceProposer contract  
cast call [GOVERNANCE_PROPOSER_ADDRESS] \  
  "getCurrentRound()" \  
  --rpc-url [YOUR_RPC_URL]
```

## Verify the Created ProposalAfter creation, you can query the proposal in the governance contract:

```
# Get the total proposal count  
cast call [GOVERNANCE_CONTRACT_ADDRESS] \  
  "proposalCount()" \  
  --rpc-url [YOUR_RPC_URL]  
  
# Query the latest proposal (count - 1, since proposals are zero-indexed)  
cast call [GOVERNANCE_CONTRACT_ADDRESS] \  
  "proposals(uint256)" $((PROPOSAL_COUNT - 1)) \  
  --rpc-url [YOUR_RPC_URL]
```

This returns the `CompressedProposal` struct data, which includes:

* The payload address
* Creation timestamp
* Voting start and end times
* Current vote tallies

## Voting on ProposalsOnce a payload becomes a proposal, there's a mandatory waiting period before voting opens. You can vote in two ways: through default delegation to the rollup contract, or by delegating to an address you control for custom voting.

## Default Voting Through the RollupBy default, when you stake as a sequencer, you delegate your voting power to the rollup contract through the GSE (Governance Staking Escrow). The rollup automatically votes "yea" on proposals created through the `GovernanceProposer` using **all** delegated stake from **all** sequencers in that rollup.

**Key points:**

* If you signaled for a payload, your stake votes "yea" automatically—no additional action needed
* If you didn't signal but other sequencers did, your stake still votes "yea" when the rollup votes
* To vote differently, you must change your delegation before voting opens (see Custom Voting below)

Anyone can trigger the rollup vote:

```
cast send [ROLLUP_ADDRESS] \  
  "vote(uint256)" [PROPOSAL_ID] \  
  --rpc-url [YOUR_RPC_URL] \  
  --private-key [YOUR_PRIVATE_KEY]
```

## Custom Voting: Delegating to Your Own AddressIf you want to vote differently on a proposal (for example, to vote "nay" or to split your voting power), you can delegate your stake to an address you control. This removes your stake's voting power from the rollup's control and gives it to your chosen address.

Voting Power Timestamp

Voting power is timestamped at the moment a proposal becomes "active" (when the voting period opens). You must complete delegation **before** the voting period begins to use your voting power for that proposal.

Check the proposal's voting start time and delegate well in advance.

## Step 1: Delegate Your StakeUse the GSE contract to delegate to an address you control:

```
cast send [GSE_ADDRESS] \  
  "delegate(address,address,address)" \  
  [ROLLUP_ADDRESS] \  
  [YOUR_ATTESTER_ADDRESS] \  
  [YOUR_DELEGATEE_ADDRESS] \  
  --rpc-url [YOUR_RPC_URL] \  
  --private-key [YOUR_WITHDRAWER_PRIVATE_KEY]
```

* `[ROLLUP_ADDRESS]`: The rollup contract where you staked
* `[YOUR_ATTESTER_ADDRESS]`: Your sequencer's attester address
* `[YOUR_DELEGATEE_ADDRESS]`: The address that will vote (often the same as your attester address, or another address you control)
* You must sign this transaction with your **withdrawer** private key (the withdrawer that you specified when you initially deposited to the rollup)

## Step 2: Vote Through GSEOnce you've delegated to an address you control, that address can vote directly on proposals:

```
# Vote "yea" with your voting power  
cast send [GSE_ADDRESS] \  
  "vote(uint256,uint256,bool)" \  
  [PROPOSAL_ID] \  
  [AMOUNT] \  
  true \  
  --rpc-url [YOUR_RPC_URL] \  
  --private-key [YOUR_DELEGATEE_PRIVATE_KEY]
```

* `[AMOUNT]`: The amount of voting power to use (can be your full stake or a partial amount)
* You can vote multiple times with different amounts to split your voting power between "yea" and "nay" if desired
* To vote "nay" with your voting power, set the boolean in the code above to false

## Step 3: Verify Your VoteCheck that your vote was recorded:

```
# Check vote counts for a proposal  
# Note: This returns the proposal's vote tallies from the Governance contract, not GSE  
cast call [GOVERNANCE_CONTRACT_ADDRESS] \  
  "getProposal(uint256)" [PROPOSAL_ID] \  
  --rpc-url [YOUR_RPC_URL]
```

This returns the current "yea" and "nay" vote tallies.

## Executing ProposalsWhen a proposal receives sufficient support, it passes. After passing, there's another mandatory delay before the proposal becomes executable. Once executable, anyone can trigger execution.

## Execute the ProposalOnce the proposal state is Executable, anyone can execute it:

```
cast send [GOVERNANCE_CONTRACT_ADDRESS] \  
  "execute(uint256)" [PROPOSAL_ID] \  
  --rpc-url [YOUR_RPC_URL] \  
  --private-key [YOUR_PRIVATE_KEY]
```

After execution, the governance contract performs all actions defined in the payload. The protocol changes become effective immediately.

## Upgrade Your Node**Critical**: Once a proposal executes, you must upgrade your node software to track the protocol changes.

Monitor proposals closely from the signaling stage through execution. When a vote passes, prepare to upgrade your node software during the execution delay period, so you're ready when the proposal becomes effective. In practice, this often means running multiple nodes, with one node being on the version upgraded from, and one being on the version being upgraded to.

## Troubleshooting## My Signal Isn't Being Recorded**Symptoms**: You configured a payload address, but the signal count isn't increasing.

**Solutions**:

1. Verify you're actually proposing blocks in slots assigned to you
2. Check your node logs for errors related to governance signaling
3. Verify the payload address is correct and matches the format (0x...)
4. Confirm the `GovernanceProposer` contract address is correct for your network

## I Can't Delegate My Voting Power**Symptoms**: Delegation transaction fails or reverts.

**Solutions**:

1. Verify you're using your **withdrawer** private key, not your attester key
2. Confirm you have stake deposited in the rollup
3. Check that the addresses are correct (rollup, attester, delegatee)
4. Ensure the rollup address matches where you actually staked

## My Vote Transaction Fails**Symptoms**: Vote transaction reverts or fails.

**Solutions**:

1. Check the proposal is in the "Active" state (voting period is open)
2. Verify you delegated before the voting period started (voting power is timestamped)
3. Confirm you have sufficient voting power (check your stake amount)
4. Ensure you're not trying to vote with more power than you have
5. Check you're using the correct private key (delegatee key, not withdrawer)

## How Do I Check When Voting Opens?Query the proposal to see the voting timeline:

```
cast call [GOVERNANCE_CONTRACT_ADDRESS] \  
  "proposals(uint256)" [PROPOSAL_ID] \  
  --rpc-url [YOUR_RPC_URL]
```

The returned data includes timestamps for:

* Voting start time
* Voting end time

## SummaryAs a sequencer participating in governance:

1. **Signal support**: Configure your node with a payload address. Your node automatically signals when proposing blocks.
2. **Vote**: Your delegated stake automatically votes "yea" on proposals created through sequencer signaling. You don't need to take additional action if you support the proposal. To vote differently, delegate your stake to an address you control before voting opens, then vote directly through the GSE contract.
3. **Upgrade promptly**: Monitor proposals and upgrade your node software after execution to stay in sync with protocol changes.

## Next Steps* Learn about [sequencer management](/network/setup/sequencer_management) for operating your node
* Join the [Aztec Discord](https://discord.gg/aztec) to participate in governance discussions and stay informed about upcoming proposals

---


# Slashing and Offenses

Source: https://docs.aztec.network/network/operation/sequencer_management/slashing_and_offenses

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide explains how the Aztec network's slashing mechanism works and how your sequencer automatically participates in detecting and voting on validator offenses. You'll learn about the Tally Model of slashing, the types of offenses that are automatically detected, and how to configure your sequencer's slashing behavior.

## PrerequisitesBefore proceeding, you should:

* Have a running sequencer node (see [Sequencer Setup Guide](/network/setup/sequencer_management))
* Understand that slashing actions are executed automatically when you propose blocks
* Have the Sentinel enabled if you want to detect inactivity offenses

## Understanding the Tally Model of SlashingThe Aztec network uses a consensus-based slashing mechanism where validators vote on individual validator offenses during block proposal.

## How Slashing Works**Automatic Detection**: Your sequencer runs watchers that continuously monitor the network and automatically detect slashable offenses committed by other validators.

**Voting Through Proposals**: Time is divided into slashing rounds (typically 128 L2 slots per round). When you propose a block during round N, your sequencer automatically votes on which validators from round N-2 should be slashed. This 2-round offset gives the network time to detect offenses before voting.

**Vote Encoding**: Votes are encoded as bytes where each validator's vote is represented by 2 bits indicating the slash amount (0-3 slash units). The L1 contract tallies these votes and slashes validators that reach quorum.

**Execution**: After a round ends, there's an execution delay period (approximately 3 days) during which the slashing vetoer can pause execution if needed. Once the delay passes, anyone can execute the round to apply the slashing.

## Slashing Rounds and Offsets```
Round 1 (Grace Period): No voting happens  
Round 2 (Grace Period): No voting happens  
Round 3: Proposers vote on offenses from Round 1 (which are typically forgiven due to grace period)  
Round 4: Proposers vote on offenses from Round 2  
Round N: Proposers vote on offenses from Round N-2
```

**Key parameters**:

* **Round Size**: 128 L2 slots (approximately 1.28 hours at 36 seconds per slot)
* **Slashing Offset**: 2 rounds (proposers in round N vote on offenses from round N-2)
* **Execution Delay**: 28 rounds (~3 days)
* **Grace Period**: First 128 slots (configurable per node)

## Slashing AmountsThe L1 contract defines three fixed slashing tiers that can be configured for different offenses. These amounts are set on L1 deployment and can only be changed via governance.

Network Configuration

On the current network, **all offenses are currently configured to slash 2,000 tokens (1% of the Activation Threshold - the minimum stake required to join the validator set)**. With the ejection threshold at 98%, validators can be slashed a maximum of **3 times** (totaling 3% of their Activation Threshold) before being automatically ejected from the validator set.

## Slashable OffensesYour sequencer automatically detects and votes to slash the following offenses:

## 1. Inactivity**What it is**: A validator fails to attest to block proposals when selected for committee duty, or fails to propose a block when selected as proposer.

**Detection criteria**:

* Measured **per epoch** for validators on the committee during that epoch (committees are assigned per epoch and remain constant for all slots in that epoch)
* The Sentinel calculates: `(missed_proposals + missed_attestations) / (total_proposals + total_attestations)`
* A validator is considered inactive for an epoch if this ratio meets or exceeds `SLASH_INACTIVITY_TARGET_PERCENTAGE` (e.g., 0.8 = 80% or more duties missed)
* Requires **consecutive committee participation with inactivity**: Must be inactive for N consecutive epochs where they were on the committee (configured via `SLASH_INACTIVITY_CONSECUTIVE_EPOCH_THRESHOLD=2`). Epochs where the validator was not on the committee are not counted, so a validator inactive in epochs 1, 3, and 5 meets the threshold for 3 consecutive inactive epochs even though epochs 2 and 4 are skipped.

**Proposed penalty**: 1% of stake

**Note**: Requires the Sentinel to be enabled (`SENTINEL_ENABLED=true`). The Sentinel tracks attestation and proposal activity for all validators.

## 2. Valid Epoch Not Proven**What it is**: An epoch was not proven within the proof submission window, even though all data was available and the epoch was valid.

**Detection criteria**:

* An epoch gets pruned (removed from the chain)
* Your node can re-execute all transactions from that epoch
* The state roots match the original epoch (indicating it could have been proven)

**Proposed penalty**: 0% (disabled for initial deployment)

**Responsibility**: The entire committee of the pruned epoch is slashed.

## 3. Data Withholding**What it is**: The committee failed to make transaction data publicly available, preventing the epoch from being proven.

**Detection criteria**:

* An epoch gets pruned
* Your node cannot obtain all the transactions needed to re-execute the epoch
* The data was not propagated to the sequencer set before the proof submission window ended

**Proposed penalty**: 0% (disabled for initial deployment)

**Responsibility**: The entire committee from the pruned epoch is slashed for failing to propagate data.

## 4. Proposed Insufficient Attestations**What it is**: A proposer submitted a block to L1 without collecting enough valid committee attestations.

**Detection criteria**:

* Block published to L1 has fewer than 2/3 + 1 attestations from the committee
* Your node detects this through L1 block validation

**Proposed penalty**: 1% of stake

## 5. Proposed Incorrect Attestations**What it is**: A proposer submitted a block with invalid signatures or signatures from non-committee members.

**Detection criteria**:

* Block contains attestations with invalid ECDSA signatures
* Block contains signatures from addresses not in the committee

**Proposed penalty**: 1% of stake

## 6. Attested to Descendant of Invalid Block**What it is**: A validator attested to a block that builds on top of an invalid block.

**Detection criteria**:

* A validator attests to block B
* Block B's parent block has invalid or insufficient attestations
* Your node has previously identified the parent as invalid

**Proposed penalty**: 1% of stake

**Note**: Validators should only attest to blocks that build on valid chains with proper attestations.

## Configuring Your Sequencer for SlashingThe slashing module runs automatically when your sequencer is enabled. You can configure its behavior using environment variables or the node's admin API. Remember to enable the Sentinel if you want to detect inactivity offenses.

## Environment VariablesYour sequencer comes pre-configured with default slashing settings. You can optionally override these defaults by setting environment variables before starting your node.

**Default configuration:**

```
# Grace period - offenses during the first N slots are not slashed  
SLASH_GRACE_PERIOD_L2_SLOTS=128  # Default: first round is grace period  
  
# Inactivity detection (requires SENTINEL_ENABLED=true)  
SLASH_INACTIVITY_TARGET_PERCENTAGE=0.8  # Slash if missed proposals + attestations >= 80%  
SLASH_INACTIVITY_CONSECUTIVE_EPOCH_THRESHOLD=2  # Must be inactive for 2+ epochs  
SLASH_INACTIVITY_PENALTY=2000000000000000000000  # 2000 tokens (1%)  
  
# Sentinel configuration (required for inactivity detection)  
SENTINEL_ENABLED=true  # Must be true to detect inactivity offenses  
SENTINEL_HISTORY_LENGTH_IN_EPOCHS=100  # Track 100 epochs of history  
  
# Epoch prune and data withholding penalties (disabled by default)  
SLASH_PRUNE_PENALTY=0  # Set to >0 to enable  
SLASH_DATA_WITHHOLDING_PENALTY=0  # Set to >0 to enable  
  
# Invalid attestations and blocks  
SLASH_PROPOSE_INVALID_ATTESTATIONS_PENALTY=2000000000000000000000  # 2000 tokens  
SLASH_ATTEST_DESCENDANT_OF_INVALID_PENALTY=2000000000000000000000  # 2000 tokens  
SLASH_INVALID_BLOCK_PENALTY=2000000000000000000000  # 2000 tokens  
  
# Offense expiration  
SLASH_OFFENSE_EXPIRATION_ROUNDS=4  # Offenses older than 4 rounds are dropped  
  
# Execution behavior  
SLASH_EXECUTE_ROUNDS_LOOK_BACK=4  # Check 4 rounds back for executable slashing rounds
```

## Runtime Configuration via APIYou can update slashing configuration while your node is running using the `nodeAdmin_setConfig` method:

**CLI Method**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{  
    "jsonrpc":"2.0",  
    "method":"nodeAdmin_setConfig",  
    "params":[{  
      "slashInactivityPenalty":"2000000000000000000000",  
      "slashInactivityTargetPercentage":0.9  
    }],  
    "id":1  
  }'
```

**Docker Method**:

```
docker exec -it aztec-sequencer curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{  
    "jsonrpc":"2.0",  
    "method":"nodeAdmin_setConfig",  
    "params":[{  
      "slashInactivityPenalty":"2000000000000000000000",  
      "slashInactivityTargetPercentage":0.9  
    }],  
    "id":1  
  }'
```

## Excluding Validators from SlashingYou can configure your node to always or never slash specific validators:

```
# Always slash these validators (regardless of detected offenses)  
SLASH_VALIDATORS_ALWAYS=0x1234...,0x5678...  
  
# Never slash these validators (even if offenses are detected)  
SLASH_VALIDATORS_NEVER=0xabcd...,0xef01...
```

**Note**: Validators in `SLASH_VALIDATORS_NEVER` take priority. If a validator appears in both lists, they won't be slashed.

**Automatic protection**: Your own validator addresses (from your keystore) are automatically added to `SLASH_VALIDATORS_NEVER` unless you set `slashSelfAllowed=true` via the node admin API.

## Verify Your ConfigurationCheck your current slashing configuration:

**CLI Method**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{  
    "jsonrpc":"2.0",  
    "method":"nodeAdmin_getConfig",  
    "id":1  
  }'
```

**Docker Method**:

```
docker exec -it aztec-sequencer curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{  
    "jsonrpc":"2.0",  
    "method":"nodeAdmin_getConfig",  
    "id":1  
  }'
```

Look for fields starting with `slash` in the response to verify your settings.

## How Automatic Slashing WorksOnce configured, your sequencer handles slashing automatically:

## 1. Continuous Offense DetectionWatchers run in the background, monitoring:

* Block attestations via the Sentinel (when enabled)
* Invalid blocks from the P2P network
* Chain prunes and epoch validation
* L1 block data for attestation validation

## 2. Offense StorageWhen a watcher detects an offense, it's automatically stored with:

* Validator address
* Offense type
* Epoch or slot number
* Penalty amount

Offenses are kept until they're voted on or expire after the configured number of rounds.

## 3. Automatic VotingWhen you're selected as a block proposer:

1. Your sequencer retrieves offenses from 2 rounds ago (the slashing offset)
2. It filters out validators in your `SLASH_VALIDATORS_NEVER` list
3. It adds synthetic offenses for validators in your `SLASH_VALIDATORS_ALWAYS` list
4. Votes are encoded as a byte array, with each validator's vote represented by two bits specifying the proposed slash amount (0–3 units)
5. The votes are submitted to L1 as part of your proposal transaction

**You don't need to take any manual action** - this happens automatically during block proposal.

## 4. Round ExecutionWhen slashing rounds become executable (after the execution delay):

* Your sequencer checks if there are rounds ready to execute
* If you're the proposer and a round is ready, your node includes the execution call in your proposal
* This triggers the L1 contract to tally votes and slash validators that reached quorum

## Understanding the Slashing VetoerThe slashing vetoer is an independent security group that can pause slashing to protect validators from unfair slashing due to software bugs.

**Execution Delay**: All slashing proposals have a ~3 day execution delay (28 rounds on testnet) during which the vetoer can review and potentially block execution.

**Temporary Disable**: The vetoer can disable all slashing for up to 3 days if needed, with the ability to extend this period.

**Purpose**: This failsafe protects sequencers from being unfairly slashed due to client software bugs or network issues that might cause false positives in offense detection.

## Ejection from the Validator SetIf a validator's stake falls below the ejection threshold after being slashed, they are automatically exited from the validator set.

**Ejection Threshold**: 98% of Activation Threshold

This means a validator can be slashed up to **3 times** (at 1% per slash, totaling 3%) before being automatically ejected. Their remaining stake is sent to their registered withdrawer address.

## Monitoring Slashing Activity## Check Pending OffensesMonitor offenses your node has detected but not yet voted on by checking your node logs:

```
# Look for these log messages  
grep "Adding pending offense" /path/to/node/logs  
grep "Voting to slash" /path/to/node/logs
```

## View Executed Slashing RoundsYour node logs when slashing rounds are executed:

```
grep "Slashing round.*has been executed" /path/to/node/logs
```

## Query L1 Contract StateYou can query the TallySlashingProposer contract to see voting activity:

```
# Get current round information  
cast call [TALLY_SLASHING_PROPOSER_ADDRESS] \  
  "getCurrentRound()" \  
  --rpc-url [YOUR_RPC_URL]  
  
# Check a specific round's vote count  
cast call [TALLY_SLASHING_PROPOSER_ADDRESS] \  
  "getRound(uint256)" [ROUND_NUMBER] \  
  --rpc-url [YOUR_RPC_URL]
```

## Troubleshooting## Slashing Module Not Running**Symptom**: No slashing-related logs appear in your node output.

**Solutions**:

1. Verify your node is running as a validator (not just an observer)
2. Check that `disableValidator` is not set to `true` in your config
3. Confirm the rollup contract has a slashing proposer configured
4. Restart your node and check for errors during slasher initialization

## Inactivity Offenses Not Detected**Symptom**: Your node doesn't detect inactivity offenses even when validators miss attestations.

**Solutions**:

1. Enable the Sentinel: Set `SENTINEL_ENABLED=true`
2. Verify Sentinel is tracking data: Check logs for "Sentinel" messages
3. Ensure `SLASH_INACTIVITY_PENALTY` is greater than 0
4. Check that `SENTINEL_HISTORY_LENGTH_IN_EPOCHS` is configured appropriately (see configuration section)
5. Remember: Validators need to be inactive for consecutive epochs (threshold: 2 by default)

## Own Validators Being Slashed**Symptom**: Your node is voting to slash your own validators.

**Solutions**:

1. Verify that `slashSelfAllowed` is not set to `true`
2. Check that your validator addresses from the keystore are being automatically added to `SLASH_VALIDATORS_NEVER`
3. Manually add your addresses to `SLASH_VALIDATORS_NEVER` as a safeguard:

   ```
   SLASH_VALIDATORS_NEVER=0xYourAddress1,0xYourAddress2
   ```

## Penalty Amounts Not Matching L1**Symptom**: Your configured penalties don't result in slashing on L1.

**Solutions**:

1. For the current network, all penalties should be set to `2000000000000000000000` (2000 tokens, 1%)
2. Verify your penalty configuration matches the default values shown in the Environment Variables section

## Best Practices**Enable the Sentinel**: If you want to participate in inactivity slashing, make sure `SENTINEL_ENABLED=true`. This is the only way to detect validators who go offline.

**Use Grace Periods**: Set `SLASH_GRACE_PERIOD_L2_SLOTS` to avoid slashing validators during the initial network bootstrap period when issues are more likely.

**Monitor Your Offenses**: Regularly check your logs to see what offenses your node is detecting and voting on. This helps you verify your slashing configuration is working as expected.

**Don't Disable Default Protections**: Unless you explicitly want to slash your own validators, keep `slashSelfAllowed` at its default (`false`) to avoid accidentally voting against yourself.

**Understand the Impact**: Remember that slashing is permanent and affects validators' stake. Only configure `SLASH_VALIDATORS_ALWAYS` for validators you have strong evidence of malicious behavior.

**Stay Updated**: Monitor Aztec Discord and governance proposals for changes to slashing parameters or new offense types being added to the protocol.

## SummaryAs a sequencer operator:

1. **Slashing is automatic**: Your sequencer detects offenses and votes during block proposals without manual intervention
2. **Configuration is flexible**: Use environment variables or runtime API calls to adjust penalties and behavior
3. **Safety mechanisms exist**: Grace periods, vetoer controls, and automatic self-protection prevent unfair slashing
4. **Monitoring is important**: Check logs and L1 state to ensure your slasher is operating as expected

## Next Steps* Review [Governance and Proposal Process](/network/operation/sequencer_management/creating_and_voting_on_proposals) to understand how slashing parameters can be changed
* Set up [monitoring](/network/operation/monitoring) to track your sequencer's slashing activity
* Join the [Aztec Discord](https://discord.gg/aztec) to discuss slashing behavior and network health with other operators

---


# Claiming Rewards

Source: https://docs.aztec.network/network/operation/sequencer_management/claiming_rewards

Version: Ignition (v2.1.9)

On this page

## OverviewSequencer rewards accumulate in the Rollup contract but are not automatically distributed. You must manually claim them by calling the Rollup contract. This guide shows you how to check pending rewards and claim them using Foundry's `cast` command.

## PrerequisitesBefore proceeding, you should:

* Have a running sequencer that earned rewards (see [Sequencer Setup Guide](/network/setup/sequencer_management))
* Have Foundry installed with the `cast` command available ([installation guide](https://book.getfoundry.sh/getting-started/installation))
* Know your Rollup contract address (see [Useful Commands](/network/operation/sequencer_management/useful_commands#get-the-rollup-contract-address))
* Have your sequencer's coinbase address
* Have an Ethereum RPC endpoint for the network you're querying

## Understanding Reward Claiming## How Rewards AccumulateWhen your sequencer proposes blocks and participates in consensus, rewards accumulate in the Rollup contract under your coinbase address. These rewards come from:

* Block rewards distributed by the protocol
* Transaction fees from processed transactions

Rewards are tracked per coinbase address in the Rollup contract's storage but remain in the contract until you claim them.

## Manual vs AutomaticRewards are not automatically sent to your coinbase address. You must explicitly claim them by calling the `claimSequencerRewards` function on the Rollup contract.

## Claim RequirementsBefore claiming, verify these conditions:

1. **Rewards must be claimable**: A governance vote must pass to enable the claiming of rewards (only possible after a minimum configured timestamp) and governance must have called `setRewardsClaimable(true)` on the rollup contract.
2. **Rewards have accumulated**: Query your pending rewards before attempting to claim.
3. **Sufficient gas**: Ensure you have ETH to pay transaction gas costs.

## Checking Reward Status## Set Up Your EnvironmentFor convenience, set your RPC URL as an environment variable:

```
export RPC_URL="https://your-ethereum-rpc-endpoint.com"  
export ROLLUP_ADDRESS="[YOUR_ROLLUP_CONTRACT_ADDRESS]"
```

Replace `[YOUR_ROLLUP_CONTRACT_ADDRESS]` with your actual Rollup contract address.

## Check if Rewards Are ClaimableVerify reward claiming is enabled before attempting to claim:

```
cast call $ROLLUP_ADDRESS "isRewardsClaimable()" --rpc-url $RPC_URL
```

**Expected output:**

* `0x0000000000000000000000000000000000000000000000000000000000000001` - Rewards are claimable (true)
* `0x0000000000000000000000000000000000000000000000000000000000000000` - Rewards are not yet claimable (false)

If rewards are not claimable, check when they will become claimable:

```
cast call $ROLLUP_ADDRESS "getEarliestRewardsClaimableTimestamp()" --rpc-url $RPC_URL
```

This returns a Unix timestamp indicating the earliest time when governance can enable reward claiming.

## Query Your Pending RewardsCheck accumulated rewards:

```
cast call $ROLLUP_ADDRESS "getSequencerRewards(address)" [COINBASE_ADDRESS] --rpc-url $RPC_URL
```

Replace `[COINBASE_ADDRESS]` with your sequencer's coinbase address.

**Example:**

```
# Query and convert to decimal tokens (assuming 18 decimals)  
cast call $ROLLUP_ADDRESS "getSequencerRewards(address)" 0x742d35Cc6634C0532925a3b844Bc9e7595f0bEb --rpc-url $RPC_URL | cast --to-dec | cast --from-wei  
# Output: 0.1
```

## Claiming Your RewardsThe `claimSequencerRewards` function is permissionless - anyone can call it for any address. Rewards are always sent to the `coinbase` address, regardless of who submits the transaction.

## Basic Claim CommandUse `cast send` to claim rewards:

```
cast send $ROLLUP_ADDRESS \  
  "claimSequencerRewards(address)" \  
  [COINBASE_ADDRESS] \  
  --rpc-url $RPC_URL \  
  --private-key [YOUR_PRIVATE_KEY]
```

Replace:

* `[COINBASE_ADDRESS]` - The coinbase address whose rewards you want to claim
* `[YOUR_PRIVATE_KEY]` - The private key of the account paying for gas

**Example:**

```
cast send $ROLLUP_ADDRESS \  
  "claimSequencerRewards(address)" \  
  0x742d35Cc6634C0532925a3b844Bc9e7595f0bEb \  
  --rpc-url $RPC_URL \  
  --private-key 0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80
```

## Using a Keystore FileFor better security, use a keystore file instead of exposing your private key:

```
cast send $ROLLUP_ADDRESS \  
  "claimSequencerRewards(address)" \  
  [COINBASE_ADDRESS] \  
  --rpc-url $RPC_URL \  
  --keystore [PATH_TO_KEYSTORE] \  
  --password [KEYSTORE_PASSWORD]
```

## Using a Hardware WalletIf you're using a Ledger wallet:

```
cast send $ROLLUP_ADDRESS \  
  "claimSequencerRewards(address)" \  
  [COINBASE_ADDRESS] \  
  --rpc-url $RPC_URL \  
  --ledger
```

This will prompt you to confirm the transaction on your Ledger device.

## Verifying Your ClaimCheck that the transaction succeeded and your pending rewards were reset to zero:

```
# Check transaction succeeded (look for status: 1)  
cast receipt [TRANSACTION_HASH] --rpc-url $RPC_URL  
  
# Verify pending rewards are now zero  
cast call $ROLLUP_ADDRESS "getSequencerRewards(address)" [COINBASE_ADDRESS] --rpc-url $RPC_URL
```

## Troubleshooting## "Rewards not claimable" Error**Symptom**: Transaction reverts with "Rewards not claimable" error.

**Solution**:

1. Check if rewards are claimable using `isRewardsClaimable()`
2. If `false`, wait until governance enables claiming via `setRewardsClaimable(true)`
3. Check the earliest claimable timestamp using `getEarliestRewardsClaimableTimestamp()`

## No Pending Rewards**Symptom**: `getSequencerRewards()` returns zero.

**Possible causes**:

1. Your sequencer has not proposed any blocks yet
2. You already claimed all available rewards
3. Your coinbase address is configured incorrectly

**Solutions**:

1. Verify your sequencer is active and proposing blocks (check [monitoring](/network/operation/monitoring))
2. Check your sequencer logs for block proposals
3. Verify the coinbase address in your sequencer configuration matches the address you're querying
4. Check if blocks you proposed have been proven (rewards are distributed after proof submission)

## Transaction Fails with "Out of Gas"**Symptom**: Transaction reverts due to insufficient gas.

**Solution**:

1. Increase the gas limit when sending the transaction using `--gas-limit`:

   ```
   cast send $ROLLUP_ADDRESS \  
     "claimSequencerRewards(address)" \  
     [COINBASE_ADDRESS] \  
     --rpc-url $RPC_URL \  
     --private-key [YOUR_PRIVATE_KEY] \  
     --gas-limit 200000
   ```
2. Ensure your account has sufficient ETH to cover gas costs

## Insufficient Funds for Gas**Symptom**: Transaction fails because the sending account has insufficient ETH.

**Solution**:

1. Check your account balance:

   ```
   cast balance [YOUR_ADDRESS] --rpc-url $RPC_URL
   ```
2. Send ETH to your account to cover gas costs (recommended: at least 0.005 ETH)

## Wrong Network**Symptom**: Transaction fails or contract calls return unexpected results.

**Solution**:

1. Verify your RPC URL points to the correct network (Ethereum mainnet)
2. Verify the Rollup contract address matches your target network
3. Check your account has ETH on the correct network

## Best Practices**Claim Regularly**: Claim rewards periodically to reduce accumulated balances in the Rollup contract. This minimizes risk and simplifies accounting.

**Monitor Pending Rewards**: Set up automated scripts to query pending rewards and alert you when they exceed a threshold.

**Use Keystore Files**: Avoid exposing private keys in command history. Use keystore files or hardware wallets for production operations.

**Verify Before Claiming**: Check pending rewards before claiming to ensure the transaction justifies the gas cost.

**Track Claim History**: Keep records of claim transactions for accounting purposes using transaction hashes on blockchain explorers.

**Coordinate with Delegators**: If operating with delegated stake, communicate with delegators about claiming and distribution schedules.

## Next Steps* Set up [monitoring](/network/operation/monitoring) to track reward accumulation automatically
* Learn about [delegated stake management](/network/operation/sequencer_management/become_a_staking_provider) if operating with delegators
* Review [useful commands](/network/operation/sequencer_management/useful_commands) for other sequencer queries
* Join the [Aztec Discord](https://discord.gg/aztec) for operator support and community discussions

---


# Useful Commands

Source: https://docs.aztec.network/network/operation/sequencer_management/useful_commands

Version: Ignition (v2.1.9)

On this page

## OverviewThis reference provides commands for common sequencer operator tasks. You'll use Foundry's `cast` command to query onchain contract state, check sequencer status, and monitor governance processes.

If you need help with something not covered here, visit the [Aztec Discord](https://discord.gg/aztec) in the `#operator-faq` channel.

## PrerequisitesBefore using these commands, ensure you have:

* **Foundry installed** with the `cast` command available ([installation guide](https://book.getfoundry.sh/getting-started/installation))
* **Aztec CLI tool** installed (see [prerequisites guide](/network/prerequisites#aztec-toolchain))
* **Ethereum RPC endpoint** (execution layer) for the network you're querying
* **Contract addresses** for your deployment (Registry, Rollup, Governance)

## Getting Started## Set Up Your EnvironmentFor convenience, set your RPC URL as an environment variable:

```
export RPC_URL="https://your-ethereum-rpc-endpoint.com"
```

All examples below use `--rpc-url $RPC_URL`. In production, always include this flag with your actual RPC endpoint.

## Understanding DeploymentsAssume there are multiple deployments of Aztec, such as `mainnet` and `ignition-mainnet`. Each deployment has a unique Registry contract address that remains constant across upgrades. If a governance upgrade deploys a new rollup contract, the Registry contract address stays the same.

## Find the Registry Contract AddressThe Registry contract is your entrypoint to all other contracts for a specific deployment. You'll need this address to discover other contract addresses.

Contact the Aztec team or check the documentation for the Registry contract address for your target network (mainnet, ignition-mainnet, etc.).

## Get the Rollup Contract AddressOnce you have the Registry address, retrieve the Rollup contract:

```
cast call [REGISTRY_CONTRACT_ADDRESS] "getCanonicalRollup()" --rpc-url $RPC_URL
```

Replace `[REGISTRY_CONTRACT_ADDRESS]` with your actual Registry contract address.

**Example:**

```
cast call 0x1234567890abcdef1234567890abcdef12345678 "getCanonicalRollup()" --rpc-url $RPC_URL
```

This returns the Rollup contract address in hexadecimal format.

## Query the Sequencer Set## Get the GSE Contract AddressThe GSE (Governance Staking Escrow) contract manages sequencer registrations and balances. Get its address from the Rollup contract:

```
cast call [ROLLUP_ADDRESS] "getGSE()" --rpc-url $RPC_URL
```

This returns the GSE contract address, which you'll need for some queries below.

## Count Active SequencersGet the total number of active sequencers in the set:

```
cast call [ROLLUP_ADDRESS] "getActiveAttesterCount()" --rpc-url $RPC_URL
```

This returns the count of currently active sequencers as a hexadecimal number.

## List Sequencers by IndexRetrieve individual sequencer addresses by their index (0-based):

```
cast call [ROLLUP_ADDRESS] "getAttesterAtIndex(uint256)" [INDEX] --rpc-url $RPC_URL
```

Replace:

* `[ROLLUP_ADDRESS]` - Your Rollup contract address
* `[INDEX]` - The index of the sequencer (starting from 0)

**Example:**

```
# Get the first sequencer (index 0)  
cast call 0xabcdef1234567890abcdef1234567890abcdef12 "getAttesterAtIndex(uint256)" 0 --rpc-url $RPC_URL  
  
# Get the second sequencer (index 1)  
cast call 0xabcdef1234567890abcdef1234567890abcdef12 "getAttesterAtIndex(uint256)" 1 --rpc-url $RPC_URL
```

## Check Sequencer StatusQuery the complete status and information for a specific sequencer:

```
cast call [ROLLUP_ADDRESS] "getAttesterView(address)" [ATTESTER_ADDRESS] --rpc-url $RPC_URL
```

Replace:

* `[ROLLUP_ADDRESS]` - Your Rollup contract address
* `[ATTESTER_ADDRESS]` - The sequencer's attester address you want to check

**Example:**

```
cast call 0xabcdef1234567890abcdef1234567890abcdef12 "getAttesterView(address)" 0x742d35Cc6634C0532925a3b844Bc9e7595f0bEb --rpc-url $RPC_URL
```

## Interpret the ResponseThe `getAttesterView` command returns an `AttesterView` struct containing:

1. **status** - The sequencer's current status code (see Status Codes below)
2. **effectiveBalance** - The sequencer's effective stake balance
3. **exit** - Exit information struct (if the sequencer is exiting):
   * `withdrawalId` - Withdrawal ID in the GSE contract
   * `amount` - Amount being withdrawn
   * `exitableAt` - Timestamp when withdrawal can be finalized
   * `recipientOrWithdrawer` - Address that receives funds or can initiate withdrawal
   * `isRecipient` - Whether the exit has a recipient set
   * `exists` - Whether an exit exists
4. **config** - Attester configuration struct:
   * `publicKey` - BLS public key (G1 point with x and y coordinates)
   * `withdrawer` - Address authorized to withdraw stake

## Get Individual Sequencer InformationQuery specific pieces of information using the GSE contract:

```
# Check if a sequencer is registered  
cast call [GSE_ADDRESS] "isRegistered(address,address)" [ROLLUP_ADDRESS] [ATTESTER_ADDRESS] --rpc-url $RPC_URL  
  
# Get sequencer's balance on this rollup instance  
cast call [GSE_ADDRESS] "balanceOf(address,address)" [ROLLUP_ADDRESS] [ATTESTER_ADDRESS] --rpc-url $RPC_URL  
  
# Get sequencer's effective balance (includes bonus if latest rollup)  
cast call [GSE_ADDRESS] "effectiveBalanceOf(address,address)" [ROLLUP_ADDRESS] [ATTESTER_ADDRESS] --rpc-url $RPC_URL  
  
# Get sequencer's configuration (withdrawer and public key)  
cast call [ROLLUP_ADDRESS] "getConfig(address)" [ATTESTER_ADDRESS] --rpc-url $RPC_URL  
  
# Get only the status  
cast call [ROLLUP_ADDRESS] "getStatus(address)" [ATTESTER_ADDRESS] --rpc-url $RPC_URL
```

## Status Codes| Status | Name | Meaning |
| --- | --- | --- |
| 0 | NONE | The sequencer does not exist in the sequencer set |
| 1 | VALIDATING | The sequencer is currently active and participating in consensus |
| 2 | ZOMBIE | The sequencer is not active (balance fell below ejection threshold, possibly due to slashing) but still has funds in the system |
| 3 | EXITING | The sequencer has initiated withdrawal and is in the exit delay period |

## Governance Operations## Get Governance Contract AddressesFirst, get the Governance contract from the Registry, then query it for the GovernanceProposer contract:

```
# Get the Governance contract  
cast call [REGISTRY_ADDRESS] "getGovernance()" --rpc-url $RPC_URL  
  
# Get the GovernanceProposer contract  
cast call [GOVERNANCE_ADDRESS] "governanceProposer()" --rpc-url $RPC_URL
```

Replace `[REGISTRY_ADDRESS]` and `[GOVERNANCE_ADDRESS]` with your actual addresses.

## Check Governance Quorum RequirementsQuery the quorum parameters for the governance system:

```
# Get the signaling round size (in L2 blocks)  
cast call [GOVERNANCE_PROPOSER_ADDRESS] "M()" --rpc-url $RPC_URL  
  
# Get the number of signals required for quorum in any single round  
cast call [GOVERNANCE_PROPOSER_ADDRESS] "N()" --rpc-url $RPC_URL
```

**What these values mean:**

* **M()** - The size of any signaling round, measured in L2 blocks (e.g., 1000 blocks)
* **N()** - The number of signals needed within a round for a payload to reach quorum (e.g., 750 signals, which is 75% of M)

## Find the Current Round NumberCalculate which governance round corresponds to a specific L2 slot:

```
cast call [GOVERNANCE_PROPOSER_ADDRESS] "computeRound(uint256)" [SLOT_NUMBER] --rpc-url $RPC_URL
```

Replace:

* `[GOVERNANCE_PROPOSER_ADDRESS]` - Your GovernanceProposer contract address
* `[SLOT_NUMBER]` - The L2 slot number you want to check

This returns the round number in hexadecimal format. Convert it to decimal for use in the next command.

**Example:**

```
# Check which round slot 5000 belongs to  
cast call 0x9876543210abcdef9876543210abcdef98765432 "computeRound(uint256)" 5000 --rpc-url $RPC_URL  
  
# Output: 0x0000000000000000000000000000000000000000000000000000000000000005 (round 5)
```

## Check Signal Count for a PayloadCheck how many sequencers have signaled support for a specific payload in a given round:

```
cast call [GOVERNANCE_PROPOSER_ADDRESS] "yeaCount(address,uint256,address)" [ROLLUP_ADDRESS] [ROUND_NUMBER] [PAYLOAD_ADDRESS] --rpc-url $RPC_URL
```

Replace:

* `[GOVERNANCE_PROPOSER_ADDRESS]` - Your GovernanceProposer contract address
* `[ROLLUP_ADDRESS]` - Your Rollup contract address
* `[ROUND_NUMBER]` - The round number as a decimal integer (not hex)
* `[PAYLOAD_ADDRESS]` - The address of the payload contract you're checking

**Example:**

```
cast call 0x9876543210abcdef9876543210abcdef98765432 "yeaCount(address,uint256,address)" 0xabcdef1234567890abcdef1234567890abcdef12 5 0x1111111111111111111111111111111111111111 --rpc-url $RPC_URL
```

This returns the number of signals the payload has received in that round. Compare this to the quorum threshold (N) to determine if the payload can be promoted to a proposal.

## Get Current Proposal CountCheck how many governance proposals exist:

```
cast call [GOVERNANCE_CONTRACT_ADDRESS] "proposalCount()" --rpc-url $RPC_URL
```

## Query a Specific ProposalGet details about a specific proposal:

```
cast call [GOVERNANCE_CONTRACT_ADDRESS] "proposals(uint256)" [PROPOSAL_ID] --rpc-url $RPC_URL
```

Replace:

* `[GOVERNANCE_CONTRACT_ADDRESS]` - Your Governance contract address
* `[PROPOSAL_ID]` - The proposal ID (zero-indexed, so the first proposal is 0)

This returns the proposal struct containing:

* Payload address
* Creation timestamp
* Voting start and end times
* Current vote tallies

## Tips and Best Practices## Using EtherscanYou can also query these contracts through Etherscan's "Read Contract" interface:

1. Navigate to the contract address on Etherscan
2. Go to the "Contract" tab
3. Click "Read Contract" or "Read as Proxy"
4. Find the function you want to call and enter parameters

This provides a user-friendly interface without requiring command-line tools.

## Monitoring AutomationConsider creating scripts that regularly query sequencer status and governance signals. This helps you:

* Track your sequencer's health
* Monitor governance proposals you care about
* Receive alerts when action is needed

## Decoding Hex OutputSome commands return hexadecimal values. Use `cast` to convert them:

```
# Convert hex to decimal  
cast --to-dec 0x03e8  
  
# Convert hex to address format  
cast --to-address 0x000000000000000000000000742d35Cc6634C0532925a3b844Bc9e7595f0bEb
```

## Troubleshooting## "Invalid JSON RPC response"**Issue**: Command fails with JSON RPC error.

**Solutions**:

* Verify your RPC endpoint is accessible and correct
* Check that you're connected to the right network (Ethereum mainnet)
* Ensure your RPC provider supports the `eth_call` method
* Try a different RPC endpoint

## "Reverted" or "Execution reverted"**Issue**: Contract call reverts.

**Solutions**:

* Verify the contract address is correct
* Check that the function signature matches the contract's ABI
* Ensure you're passing the correct parameter types
* Verify the contract is deployed on the network you're querying

## "Could not find function"**Issue**: Function not found in contract.

**Solutions**:

* Verify the function name spelling and capitalization
* Check that you're querying the correct contract
* Ensure the contract version matches the function you're calling
* Try querying through Etherscan to verify the contract ABI

## Next Steps* [Learn about sequencer management](/network/setup/sequencer_management) to operate your sequencer node
* [Participate in governance](/network/operation/sequencer_management/creating_and_voting_on_proposals) by signaling, voting, and creating proposals
* [Monitor your node](/network/operation/monitoring) with metrics and observability tools
* Join the [Aztec Discord](https://discord.gg/aztec) for operator support and community discussions

---


# FAQs & Common Issues

Source: https://docs.aztec.network/network/operation/operator_faq

Version: Ignition (v2.1.9)

On this page

## OverviewThis guide addresses common issues node operators encounter when running Aztec nodes. Each entry includes the issue symptoms, possible causes, and step-by-step solutions.

If your issue isn't listed here, visit the [Aztec Discord](https://discord.gg/aztec) in the `#operator-faq` channel for community support.

## Node Sync Issues## SYNC\_BLOCK Failed Error**Symptom**: You see this error in your node logs:

```
ERROR: world-state:database Call SYNC_BLOCK failed: Error: Can't synch block: block state does not match world state
```

**Cause**: Your local database state is corrupted or out of sync with the network.

**Solution**:

1. Stop your node:

   ```
   docker compose down
   ```
2. Remove the archiver data directory:

   ```
   rm -rf ~/.aztec/v2.1.9/data/archiver
   ```
3. Restart your node:

   ```
   docker compose up -d
   ```

Data Loss and Resync

This process removes local state and requires full resynchronization. Consider using snapshot sync mode (`SYNC_MODE=snapshot`) to speed up recovery. See the [syncing best practices guide](/network/setup/syncing_best_practices) for more information.

## Error Getting Slot Number**Symptom**: Your logs show "Error getting slot number" related to beacon or execution endpoints.

**Cause**:

* **Beacon-related errors**: Failed to connect to your L1 consensus (beacon) RPC endpoint
* **Execution-related errors**: Failed to connect to your L1 execution RPC endpoint or reporting routine issue

**Solutions**:

1. **Verify L1 endpoint configuration**:

   * Check your `L1_CONSENSUS_HOST_URLS` setting points to your beacon node
   * Check your `ETHEREUM_HOSTS` setting points to your execution client
   * Ensure URLs are formatted correctly (e.g., `http://localhost:5052` for beacon)
2. **Test endpoint connectivity**:

   ```
   # Test beacon endpoint  
   curl [YOUR_BEACON_ENDPOINT]/eth/v1/beacon/headers  
     
   # Test execution endpoint  
   curl -X POST -H "Content-Type: application/json" \  
     --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1}' \  
     [YOUR_EXECUTION_ENDPOINT]
   ```
3. **Verify L1 clients are synced**:

   * Check that your beacon node is fully synced
   * Check that your execution client is fully synced
   * Use `docker compose logs` or check L1 client logs for sync status
4. **Check for rate limiting** (if using third-party RPC):

   * See the "RPC and Rate Limiting" section below
   * Consider using your own L1 node for better reliability

## RPC and Rate Limiting## RPC Rate Limit or Quota Exceeded**Symptom**: Your logs show errors like:

```
Error: quota limit exceeded  
Error: rate limit exceeded  
Error: too many requests
```

**Cause**: Your RPC provider is throttling requests due to rate limits or quota restrictions.

**Solutions**:

1. **Register for an API key with your RPC provider**:

   * Most providers (Infura, Alchemy, QuickNode) offer higher limits with authenticated requests
   * Update your configuration to include the API key in your RPC URL
   * Example: `https://mainnet.infura.io/v3/YOUR_API_KEY`
2. **Use your own L1 node** (recommended for sequencers):

   * Running your own Ethereum node eliminates rate limits entirely
   * Provides better performance, reliability, and privacy
   * See [Eth Docker's guide](https://ethdocker.com/Usage/QuickStart) for setup instructions
   * Ensure you're running both execution and consensus clients
3. **Configure multiple RPC endpoints for failover**:

   * Aztec nodes support comma-separated RPC URLs
   * Example: `ETHEREUM_HOSTS=https://rpc1.example.com,https://rpc2.example.com`
   * The node will automatically fail over if one endpoint is unavailable

Run Your Own L1 Infrastructure

Sequencer operators should always run their own L1 infrastructure to ensure reliability, avoid rate limits, and maintain optimal performance. Third-party RPC providers are suitable for testing but not recommended for production sequencer operations.

## Blob Retrieval Errors**Symptom**: Your logs show errors like:

```
Error: No blob bodies found  
Error: Unable to get blob sidecar, Gateway Time-out (504)
```

**Cause**: Your beacon node endpoint is slow, overloaded, rate-limited, or not synced properly.

**Solutions**:

1. **Verify beacon endpoint configuration**:

   ```
   # Check L1_CONSENSUS_HOST_URLS in your configuration  
   # Should point to your beacon node's API endpoint
   ```
2. **Test beacon endpoint health**:

   ```
   # Check if beacon node is responding  
   curl [YOUR_BEACON_ENDPOINT]/eth/v1/node/health  
     
   # Check sync status  
   curl [YOUR_BEACON_ENDPOINT]/eth/v1/node/syncing
   ```
3. **Ensure beacon node is fully synced**:

   * Check your beacon client logs
   * Verify the sync status shows as synced
   * Blob data is only available for recent blocks (typically 18 days)
4. **Run your own beacon node** (recommended):

   * Using a third-party beacon endpoint may have rate limits
   * Running your own provides better reliability and eliminates timeouts
   * See the [prerequisites guide](/network/prerequisites) for L1 infrastructure setup

## Funding and Resources## Insufficient L1 Funds**Symptom**: Your sequencer cannot publish blocks, and logs show:

```
Error: Insufficient L1 funds  
Error: insufficient funds for gas * price + value
```

**Cause**: Your publisher address doesn't have enough Sepolia ETH to pay for L1 gas fees.

**Solutions**:

1. **Get Sepolia ETH from a faucet**:

   * [Sepolia Faucet](https://sepoliafaucet.com/)
   * [Alchemy Sepolia Faucet](https://www.alchemy.com/faucets/ethereum-sepolia)
   * [Infura Sepolia Faucet](https://www.infura.io/faucet/sepolia)
2. **Maintain sufficient balance**:

   * Keep at least **0.1 ETH** in your publisher account at all times
   * Monitor your balance regularly to avoid running out
   * Falling below the minimum balance may result in slashing
3. **Set up balance monitoring**:

   ```
   # Check your publisher balance  
   cast balance [YOUR_PUBLISHER_ADDRESS] --rpc-url [YOUR_RPC_URL]
   ```
4. **Configure alerts**:

   * Set up monitoring to alert you when balance drops below 0.15 ETH
   * This gives you time to top up before hitting the critical threshold

Slashing Risk

Sequencers with insufficient funds in their publisher account risk being slashed. Always maintain at least 0.1 ETH to ensure uninterrupted operation and avoid penalties.

## Updates and Maintenance## Version-Specific Updates:To update to a specific version:

```
# Change the image tag from:  
image: "aztecprotocol/aztec:latest"  
# To:  
image: "aztecprotocol/aztec:2.1.9"
```

Then run:

```
docker compose pull  
docker compose down  
docker compose up -d
```

Stay Informed About Updates

Join the [Aztec Discord](https://discord.gg/aztec) and follow the announcements channel to stay informed about new releases and required updates.

## Network and Connectivity## Port Forwarding Not Working**Symptom**: Your node cannot discover peers or shows "0 peers connected" in logs.

**Cause**: Firewall rules or router configuration are blocking P2P connections.

**Solutions**:

1. **Verify your external IP address**:

   ```
   curl ipv4.icanhazip.com
   ```

   Confirm this matches your `P2P_IP` configuration.
2. **Test port connectivity**:

   ```
   # From another machine, test if your P2P port is accessible  
   nc -zv [YOUR_EXTERNAL_IP] 40400
   ```
3. **Configure router port forwarding**:

   * Log into your router's admin interface
   * Forward port 40400 (TCP and UDP) to your node's local IP address
   * Save and restart router if needed
4. **Check local firewall rules**:

   ```
   # Linux: Allow P2P port through firewall  
   sudo ufw allow 40400/tcp  
   sudo ufw allow 40400/udp  
     
   # Verify rules  
   sudo ufw status
   ```
5. **Verify Docker network settings**:

   * Ensure ports are properly mapped in docker-compose.yml
   * Check that `P2P_PORT` environment variable matches the exposed ports

## Other Common Issues## CodeError: Stream Reset**Symptom**: You occasionally see this error in logs:

```
CodeError: stream reset
```

**Cause**: Temporary P2P connection disruption. This is normal network behavior and occurs when peer connections are interrupted.

**Impact**: This is safe to ignore. Your node automatically reconnects to peers and maintains network connectivity.

**Action Required**: None. This is expected behavior in P2P networks.

## Keystore Not Loading**Symptom**: Your sequencer fails to start with errors about invalid keys or missing keystore.

**Cause**: Keystore file is improperly formatted, missing, or has incorrect permissions.

**Solutions**:

1. **Verify keystore.json format**:

   ```
   {  
     "schemaVersion": 1,  
     "validators": [  
       {  
         "attester": {  
           "eth": "0xYOUR_ETH_PRIVATE_KEY_HERE",  
           "bls": "0xYOUR_BLS_PRIVATE_KEY_HERE"  
         },  
         "publisher": ["0xYOUR_PUBLISHER_KEY_HERE"],  
         "coinbase": "0xYOUR_COINBASE_ADDRESS",  
         "feeRecipient": "0xYOUR_AZTEC_ADDRESS"  
       }  
     ]  
   }
   ```
2. **Validate private key format**:

   * Keys should start with `0x`
   * Keys should be 64 hexadecimal characters (plus the `0x` prefix)
   * No spaces or extra characters
   * The attester must contain both `eth` and `bls` keys
3. **Check file permissions**:

   ```
   # Ensure keystore is readable  
   chmod 600 ~/.aztec/keys/keystore.json  
     
   # Verify ownership  
   ls -la ~/.aztec/keys/
   ```
4. **Verify keystore directory path**:

   * Ensure `KEY_STORE_DIRECTORY` environment variable is set in your `.env` file
   * Verify the volume mount in `docker-compose.yml` points to the correct directory

For more information on keystore configuration and creation, see the [Creating Validator Keystores guide](/network/operation/keystore/creating_keystores) and the [Advanced Keystore Usage guide](/network/operation/keystore).

## Docker Container Won't Start**Symptom**: Docker container crashes immediately after starting or won't start at all.

**Cause**: Various issues including configuration errors, insufficient resources, or port conflicts.

**Solutions**:

1. **Check container logs**:

   ```
   docker compose logs aztec-sequencer
   ```

   Look for specific error messages that indicate the problem.
2. **Verify Docker resources**:

   * Ensure sufficient disk space: `df -h`
   * Check Docker has adequate memory allocated (16GB+ recommended)
   * Verify CPU resources are available
3. **Check environment file format**:

   ```
   # Verify .env file exists and is properly formatted  
   cat .env  
     
   # No spaces around = signs  
   # No quotes around values (unless necessary)
   ```
4. **Verify port availability**:

   ```
   # Check if ports are already in use  
   lsof -i :8080  
   lsof -i :40400
   ```
5. **Update Docker and Docker Compose**:

   ```
   # Check versions  
   docker --version  
   docker compose version  
     
   # Update if needed  
   sudo apt-get update && sudo apt-get upgrade docker-ce docker-compose-plugin
   ```
6. **Try a clean restart**:

   ```
   docker compose down  
   docker compose pull  
   docker compose up -d
   ```

## Getting Additional HelpIf you've tried the solutions above and are still experiencing issues:

1. **Gather diagnostic information**:

   * Recent log output from your node
   * Your configuration (remove private keys!)
   * Aztec version you're running
   * Operating system and hardware specs
2. **Check existing issues**:

   * Browse the [Aztec GitHub issues](https://github.com/AztecProtocol/aztec-packages/issues)
   * Search for similar problems and solutions
3. **Ask for help**:

   * Join the [Aztec Discord](https://discord.gg/aztec)
   * Post in the `#operator-faq` or `#operator-support` channel
   * Include your diagnostic information
   * Be specific about what you've already tried

## Next Steps* Review [monitoring setup](/network/operation/monitoring) to catch issues early with metrics and alerts
* Check the [CLI reference](/network/reference/cli_reference) for all configuration options
* Join the [Aztec Discord](https://discord.gg/aztec) for real-time operator support

---


# Changelog

Source: https://docs.aztec.network/network/reference/changelog

Version: Ignition (v2.1.9)

On this page

## OverviewThis changelog documents all configuration changes, new features, and breaking changes across Aztec node versions. Each version has a dedicated page with detailed migration instructions.

## Version history## [v2.0.2 (from v1.2.1)](/network/reference/changelog/v2.0.2)Major release with significant configuration simplification, keystore integration, and feature updates.

**Key changes:**

* Simplified L1 contract address configuration (registry-only)
* Integrated keystore system for key management
* Removed component-specific settings in favor of global configuration
* Enhanced P2P transaction collection capabilities
* New invalidation controls for sequencers

**Migration difficulty**: Moderate to High

[View full changelog →](/network/reference/changelog/v2.0.2)

---

## Migration guidesWhen upgrading between versions:

1. Review the version-specific changelog for breaking changes
2. Follow the migration checklist for your node type
3. Test in a non-production environment first
4. Check the troubleshooting section for common upgrade issues
5. Join [Aztec Discord](https://discord.gg/aztec) for upgrade support

## Related resources* [CLI Reference](/network/reference/cli_reference) - Current command-line options
* [Node API Reference](/network/reference/node_api_reference) - API documentation
* [Ethereum RPC Reference](/network/reference/ethereum_rpc_reference) - L1 RPC usage

---


# v2.0.2 (from v1.2.1)

Source: https://docs.aztec.network/network/reference/changelog/v2.0.2

Version: Ignition (v2.1.9)

On this page

## OverviewVersion 2.0.2 introduces significant configuration simplification, an integrated keystore system, and enhanced P2P capabilities. This release includes breaking changes that require migration from v1.2.1.

**Migration difficulty**: Moderate to High

## Breaking changes## L1 contract addresses**v1.2.1:**

```
--rollup-address <value>                    ($ROLLUP_CONTRACT_ADDRESS)  
--inbox-address <value>                     ($INBOX_CONTRACT_ADDRESS)  
--outbox-address <value>                    ($OUTBOX_CONTRACT_ADDRESS)  
--fee-juice-address <value>                 ($FEE_JUICE_CONTRACT_ADDRESS)  
--staking-asset-address <value>             ($STAKING_ASSET_CONTRACT_ADDRESS)  
--fee-juice-portal-address <value>          ($FEE_JUICE_PORTAL_CONTRACT_ADDRESS)  
--registry-address <value>                  ($REGISTRY_CONTRACT_ADDRESS)
```

**v2.0.2:**

```
--registry-address <value>                  ($REGISTRY_CONTRACT_ADDRESS)  
--rollup-version <value>                    ($ROLLUP_VERSION)  # Default: canonical
```

**Migration**: Only registry address is required. All other contract addresses are derived automatically.

## Keystore integration**v1.2.1:**

```
--sequencer.publisherPrivateKey <value>     ($SEQ_PUBLISHER_PRIVATE_KEY)  
--proverNode.publisherPrivateKey <value>    ($PROVER_PUBLISHER_PRIVATE_KEY)
```

**v2.0.2:**

```
--proverNode.keyStoreDirectory <value>      ($KEY_STORE_DIRECTORY)  
  
# Multiple publishers supported  
--sequencer.publisherPrivateKeys <value>    ($SEQ_PUBLISHER_PRIVATE_KEYS)  
--sequencer.publisherAddresses <value>      ($SEQ_PUBLISHER_ADDRESSES)  
  
--proverNode.publisherPrivateKeys <value>   ($PROVER_PUBLISHER_PRIVATE_KEYS)  
--proverNode.publisherAddresses <value>     ($PROVER_PUBLISHER_ADDRESSES)
```

**Migration**: Create keystore directory, change singular to plural. Use `*_ADDRESSES` for remote signers. See [Advanced Keystore Guide](/network/operation/keystore).

## Validator configuration**v1.2.1:**

```
--sequencer.validatorPrivateKeys <value>    ($VALIDATOR_PRIVATE_KEYS)
```

**v2.0.2:**

```
--sequencer.validatorPrivateKeys <value>    ($VALIDATOR_PRIVATE_KEYS)  
--sequencer.validatorAddresses <value>      ($VALIDATOR_ADDRESSES)        # For remote signers  
--sequencer.disabledValidators <value>                                    # Temporarily disable
```

## Sync mode relocated**v1.2.1:** Component-specific

```
--node.syncMode <value>                     ($SYNC_MODE)  
--node.snapshotsUrl <value>                 ($SYNC_SNAPSHOTS_URL)  
--proverNode.syncMode <value>               ($SYNC_MODE)  
--proverNode.snapshotsUrl <value>           ($SYNC_SNAPSHOTS_URL)
```

**v2.0.2:** Global setting

```
--sync-mode <value>                         ($SYNC_MODE)          # Options: full, snapshot, force-snapshot  
--snapshots-url <value>                     ($SYNC_SNAPSHOTS_URL)
```

## World state separation**v1.2.1:** Prover-node-specific

```
--proverNode.worldStateBlockCheckIntervalMS <value>         ($WS_BLOCK_CHECK_INTERVAL_MS)  
--proverNode.worldStateProvenBlocksOnly <value>             ($WS_PROVEN_BLOCKS_ONLY)  
--proverNode.worldStateBlockRequestBatchSize <value>        ($WS_BLOCK_REQUEST_BATCH_SIZE)  
--proverNode.worldStateDbMapSizeKb <value>                  ($WS_DB_MAP_SIZE_KB)  
--proverNode.archiveTreeMapSizeKb <value>                   ($ARCHIVE_TREE_MAP_SIZE_KB)  
--proverNode.nullifierTreeMapSizeKb <value>                 ($NULLIFIER_TREE_MAP_SIZE_KB)  
--proverNode.noteHashTreeMapSizeKb <value>                  ($NOTE_HASH_TREE_MAP_SIZE_KB)  
--proverNode.messageTreeMapSizeKb <value>                   ($MESSAGE_TREE_MAP_SIZE_KB)  
--proverNode.publicDataTreeMapSizeKb <value>                ($PUBLIC_DATA_TREE_MAP_SIZE_KB)  
--proverNode.worldStateDataDirectory <value>                ($WS_DATA_DIRECTORY)  
--proverNode.worldStateBlockHistory <value>                 ($WS_NUM_HISTORIC_BLOCKS)
```

**v2.0.2:** Global settings only

```
--world-state-data-directory <value>        ($WS_DATA_DIRECTORY)  
--world-state-db-map-size-kb <value>        ($WS_DB_MAP_SIZE_KB)  
--world-state-block-history <value>         ($WS_NUM_HISTORIC_BLOCKS)
```

**Migration**: Move to global WORLD STATE section. Tree-specific map sizes and other world state settings removed.

## Removed features## Faucet service```
# All removed in v2.0.2  
--faucet  
--faucet.apiServer  
--faucet.apiServerPort <value>              ($FAUCET_API_SERVER_PORT)  
--faucet.viemPollingIntervalMS <value>      ($L1_READER_VIEM_POLLING_INTERVAL_MS)  
--faucet.l1Mnemonic <value>                 ($MNEMONIC)  
--faucet.mnemonicAddressIndex <value>       ($FAUCET_MNEMONIC_ADDRESS_INDEX)  
--faucet.interval <value>                   ($FAUCET_INTERVAL_MS)  
--faucet.ethAmount <value>                  ($FAUCET_ETH_AMOUNT)  
--faucet.l1Assets <value>                   ($FAUCET_L1_ASSETS)
```

## L1 transaction monitoringAll removed from archiver and sequencer:

```
--archiver.gasLimitBufferPercentage <value>             ($L1_GAS_LIMIT_BUFFER_PERCENTAGE)  
--archiver.maxGwei <value>                              ($L1_GAS_PRICE_MAX)  
--archiver.maxBlobGwei <value>                          ($L1_BLOB_FEE_PER_GAS_MAX)  
--archiver.priorityFeeBumpPercentage <value>            ($L1_PRIORITY_FEE_BUMP_PERCENTAGE)  
--archiver.priorityFeeRetryBumpPercentage <value>       ($L1_PRIORITY_FEE_RETRY_BUMP_PERCENTAGE)  
--archiver.fixedPriorityFeePerGas <value>               ($L1_FIXED_PRIORITY_FEE_PER_GAS)  
--archiver.maxAttempts <value>                          ($L1_TX_MONITOR_MAX_ATTEMPTS)  
--archiver.checkIntervalMs <value>                      ($L1_TX_MONITOR_CHECK_INTERVAL_MS)  
--archiver.stallTimeMs <value>                          ($L1_TX_MONITOR_STALL_TIME_MS)  
--archiver.txTimeoutMs <value>                          ($L1_TX_MONITOR_TX_TIMEOUT_MS)  
--archiver.txPropagationMaxQueryAttempts <value>        ($L1_TX_PROPAGATION_MAX_QUERY_ATTEMPTS)  
--archiver.cancelTxOnTimeout <value>                    ($L1_TX_MONITOR_CANCEL_TX_ON_TIMEOUT)  
  
# Same settings removed from --sequencer.*
```

**Migration**: L1 transaction management now uses optimized internal defaults.

## Rollup constants from archiverAll rollup constants now derived from L1 contracts:

```
# All removed in v2.0.2  
--archiver.ethereumSlotDuration <value>             ($ETHEREUM_SLOT_DURATION)  
--archiver.aztecSlotDuration <value>                ($AZTEC_SLOT_DURATION)  
--archiver.aztecEpochDuration <value>               ($AZTEC_EPOCH_DURATION)  
--archiver.aztecTargetCommitteeSize <value>         ($AZTEC_TARGET_COMMITTEE_SIZE)  
--archiver.aztecProofSubmissionEpochs <value>       ($AZTEC_PROOF_SUBMISSION_EPOCHS)  
--archiver.depositAmount <value>                    ($AZTEC_DEPOSIT_AMOUNT)  
--archiver.minimumStake <value>                     ($AZTEC_MINIMUM_STAKE)  
--archiver.slashingQuorum <value>                   ($AZTEC_SLASHING_QUORUM)  
--archiver.slashingRoundSize <value>                ($AZTEC_SLASHING_ROUND_SIZE)  
--archiver.governanceProposerQuorum <value>         ($AZTEC_GOVERNANCE_PROPOSER_QUORUM)  
--archiver.governanceProposerRoundSize <value>      ($AZTEC_GOVERNANCE_PROPOSER_ROUND_SIZE)  
--archiver.manaTarget <value>                       ($AZTEC_MANA_TARGET)  
--archiver.provingCostPerMana <value>               ($AZTEC_PROVING_COST_PER_MANA)  
--archiver.exitDelaySeconds <value>                 ($AZTEC_EXIT_DELAY_SECONDS)  
  
# Same settings removed from --sequencer.*
```

## Node deployment options```
# All removed in v2.0.2 (moved to sandbox only)  
--node.deployAztecContracts                         ($DEPLOY_AZTEC_CONTRACTS)  
--node.deployAztecContractsSalt <value>             ($DEPLOY_AZTEC_CONTRACTS_SALT)  
--node.assumeProvenThroughBlockNumber <value>       ($ASSUME_PROVEN_THROUGH_BLOCK_NUMBER)  
--node.publisherPrivateKey <value>                  ($L1_PRIVATE_KEY)
```

**Migration**: Contract deployment now sandbox-only via `--sandbox.deployAztecContractsSalt`. For production, deploy contracts separately.

## Other removed settings```
# Prover coordination  
--proverNode.proverCoordinationNodeUrls <value>     ($PROVER_COORDINATION_NODE_URLS)  
  
# Custom forwarder  
--sequencer.customForwarderContractAddress <value>  ($CUSTOM_FORWARDER_CONTRACT_ADDRESS)  
--proverNode.customForwarderContractAddress <value> ($CUSTOM_FORWARDER_CONTRACT_ADDRESS)  
  
# Component-specific settings now global  
--archiver.viemPollingIntervalMS <value>            ($ARCHIVER_VIEM_POLLING_INTERVAL_MS)  
--sequencer.viemPollingIntervalMS <value>           ($L1_READER_VIEM_POLLING_INTERVAL_MS)  
--blobSink.viemPollingIntervalMS <value>            ($L1_READER_VIEM_POLLING_INTERVAL_MS)  
--proverBroker.viemPollingIntervalMS <value>        ($L1_READER_VIEM_POLLING_INTERVAL_MS)  
  
--archiver.rollupVersion <value>                    ($ROLLUP_VERSION)  
--sequencer.rollupVersion <value>                   ($ROLLUP_VERSION)  
--blobSink.rollupVersion <value>                    ($ROLLUP_VERSION)  
--proverBroker.rollupVersion <value>                ($ROLLUP_VERSION)  
--pxe.rollupVersion <value>                         ($ROLLUP_VERSION)  
  
--archiver.dataStoreMapSizeKB <value>               ($DATA_STORE_MAP_SIZE_KB)  
--pxe.dataStoreMapSizeKB <value>                    ($DATA_STORE_MAP_SIZE_KB)  
--blobSink.dataStoreMapSizeKB <value>               ($DATA_STORE_MAP_SIZE_KB)  
--proverBroker.dataStoreMapSizeKB <value>           ($DATA_STORE_MAP_SIZE_KB)  
--p2pBootstrap.dataStoreMapSizeKB <value>           ($DATA_STORE_MAP_SIZE_KB)  
  
# Aztec node specific  
--node.worldStateBlockCheckIntervalMS <value>       ($WS_BLOCK_CHECK_INTERVAL_MS)  
--node.archiverUrl <value>                          ($ARCHIVER_URL)
```

## New features## P2P transaction collection```
--p2p.txCollectionNodeRpcUrls <value>                           ($TX_COLLECTION_NODE_RPC_URLS)  
--p2p.txCollectionFastNodeIntervalMs <value>                    ($TX_COLLECTION_FAST_NODE_INTERVAL_MS)  
--p2p.txCollectionFastMaxParallelRequestsPerNode <value>        ($TX_COLLECTION_FAST_MAX_PARALLEL_REQUESTS_PER_NODE)  
--p2p.txCollectionNodeRpcMaxBatchSize <value>                   ($TX_COLLECTION_NODE_RPC_MAX_BATCH_SIZE)  
--p2p.txCollectionFastNodesTimeoutBeforeReqRespMs <value>       ($TX_COLLECTION_FAST_NODES_TIMEOUT_BEFORE_REQ_RESP_MS)  
--p2p.txCollectionSlowNodesIntervalMs <value>                   ($TX_COLLECTION_SLOW_NODES_INTERVAL_MS)  
--p2p.txCollectionSlowReqRespIntervalMs <value>                 ($TX_COLLECTION_SLOW_REQ_RESP_INTERVAL_MS)  
--p2p.txCollectionSlowReqRespTimeoutMs <value>                  ($TX_COLLECTION_SLOW_REQ_RESP_TIMEOUT_MS)  
--p2p.txCollectionReconcileIntervalMs <value>                   ($TX_COLLECTION_RECONCILE_INTERVAL_MS)  
--p2p.txCollectionDisableSlowDuringFastRequests <value>         ($TX_COLLECTION_DISABLE_SLOW_DURING_FAST_REQUESTS)
```

## P2P security and testing```
# Discovery and security  
--p2p.p2pDiscoveryDisabled <value>                  ($P2P_DISCOVERY_DISABLED)  
--p2p.p2pAllowOnlyValidators <value>                ($P2P_ALLOW_ONLY_VALIDATORS)  
--p2p.p2pMaxFailedAuthAttemptsAllowed <value>       ($P2P_MAX_AUTH_FAILED_ATTEMPTS_ALLOWED)  
  
# Testing features  
--p2p.dropTransactions <value>                      ($P2P_DROP_TX)  
--p2p.dropTransactionsProbability <value>           ($P2P_DROP_TX_CHANCE)  
  
# Transaction handling  
--p2p.disableTransactions <value>                   ($TRANSACTIONS_DISABLED)  
--p2p.txPoolDeleteTxsAfterReorg <value>             ($P2P_TX_POOL_DELETE_TXS_AFTER_REORG)  
  
# Preferred peers  
--p2p.preferredPeers <value>                        ($P2P_PREFERRED_PEERS)
```

## Sequencer invalidation controls```
--sequencer.attestationPropagationTime <value>                              ($SEQ_ATTESTATION_PROPAGATION_TIME)  
--sequencer.secondsBeforeInvalidatingBlockAsCommitteeMember <value>         ($SEQ_SECONDS_BEFORE_INVALIDATING_BLOCK_AS_COMMITTEE_MEMBER)  
--sequencer.secondsBeforeInvalidatingBlockAsNonCommitteeMember <value>      ($SEQ_SECONDS_BEFORE_INVALIDATING_BLOCK_AS_NON_COMMITTEE_MEMBER)
```

## Other new features```
# Archiver - skip validation (testing only)  
--archiver.skipValidateBlockAttestations  
  
# Prover - transaction gathering timeout  
--proverNode.txGatheringTimeoutMs <value>           ($PROVER_NODE_TX_GATHERING_TIMEOUT_MS)
```

## Changed defaults| Flag | Environment Variable | v1.2.1 | v2.0.2 |
| --- | --- | --- | --- |
| `--p2p.overallRequestTimeoutMs` | `$P2P_REQRESP_OVERALL_REQUEST_TIMEOUT_MS` | 4000 | **10000** |
| `--p2p.individualRequestTimeoutMs` | `$P2P_REQRESP_INDIVIDUAL_REQUEST_TIMEOUT_MS` | 2000 | **10000** |
| `--p2p.dialTimeoutMs` | `$P2P_REQRESP_DIAL_TIMEOUT_MS` | 1000 | **5000** |
| `--proverAgent.proverAgentPollIntervalMs` | `$PROVER_AGENT_POLL_INTERVAL_MS` | 100 | **1000** |
| `--bot.l1ToL2MessageTimeoutSeconds` | `$BOT_L1_TO_L2_TIMEOUT_SECONDS` | 60 | **3600** |
| `--bot.recipientEncryptionSecret` | `$BOT_RECIPIENT_ENCRYPTION_SECRET` | [Redacted] | **0x...cafecafe** |

## Migration checklist## All nodes* Update to `--registry-address` only (remove all other contract addresses)
* Add `--rollup-version canonical` if needed
* Move `--sync-mode` and `--snapshots-url` to global config
* Remove component-specific `--*.rollupVersion`, `--*.dataStoreMapSizeKB`
* Set global `--data-store-map-size-kb` if needed (default: 134217728 KB)

## Sequencer nodes* Create and configure `--sequencer.keyStoreDirectory` (actually `--proverNode.keyStoreDirectory`)
* Change `--sequencer.publisherPrivateKey` → `--sequencer.publisherPrivateKeys`
* Update `--sequencer.validatorPrivateKeys` or add `--sequencer.validatorAddresses`
* Remove all `--sequencer.gasLimitBufferPercentage` and related L1 settings
* Remove `--sequencer.customForwarderContractAddress`, `--sequencer.viemPollingIntervalMS`
* Consider using `--sequencer.disabledValidators` for temporary disabling

## Prover nodes* Create and configure `--proverNode.keyStoreDirectory`
* Change `--proverNode.publisherPrivateKey` → `--proverNode.publisherPrivateKeys`
* Move world state settings to global WORLD STATE section
* Remove `--proverNode.archiveTreeMapSizeKb` and other tree-specific sizes
* Remove `--proverNode.proverCoordinationNodeUrls`, `--proverNode.customForwarderContractAddress`
* Set `--proverNode.txGatheringTimeoutMs` if needed

## Archiver nodes* Remove all `--archiver.gasLimitBufferPercentage` and related L1 settings
* Remove `--archiver.ethereumSlotDuration`, `--archiver.aztecSlotDuration`, etc.
* Remove `--archiver.viemPollingIntervalMS`

## P2P configuration* Configure `--p2p.txCollectionNodeRpcUrls` if using external nodes
* Review `--p2p.p2pAllowOnlyValidators` security settings
* Consider using `--p2p.preferredPeers`

## Sandbox/development* Move deployment to `--sandbox.deployAztecContractsSalt`
* Configure `--sandbox.l1Mnemonic` if needed
* Remove all faucet flags

## Troubleshooting## Node fails with contract address errors**Solution**: Remove all individual contract addresses, keep only `--registry-address`, add `--rollup-version canonical`

## Publisher key not found**Solution**:

* Check `--proverNode.keyStoreDirectory` ($KEY\_STORE\_DIRECTORY) is set
* Change `--*.publisherPrivateKey` to `--*.publisherPrivateKeys` (plural)
* See [Advanced Keystore Guide](/network/operation/keystore)

## World state sync failures (prover)")

**Solution**:

* Move `--proverNode.worldStateDataDirectory` to `--world-state-data-directory`
* Remove prover-specific world state settings
* Use global `--world-state-db-map-size-kb`

## Slow P2P after upgrade**Solution**:

* Configure `--p2p.txCollectionNodeRpcUrls` ($TX\_COLLECTION\_NODE\_RPC\_URLS)
* Adjust `--p2p.txCollectionFastNodeIntervalMs`

## Validator not attesting**Solution**:

* Check not in `--sequencer.disabledValidators` list
* Verify `--sequencer.validatorPrivateKeys` or `--sequencer.validatorAddresses`
* Check keystore permissions

## Missing sync snapshots**Solution**:

* Move `--node.syncMode` to `--sync-mode` (global)
* Set `--snapshots-url` at global level

## Next steps* [How to Run a Sequencer Node](/network/setup/sequencer_management) - Updated setup instructions
* [Advanced Keystore Usage](/network/operation/keystore) - Keystore configuration
* [Ethereum RPC Calls Reference](/network/reference/ethereum_rpc_reference) - Infrastructure requirements
* [Aztec Discord](https://discord.gg/aztec) - Upgrade support

---


# Cli Reference

Source: https://docs.aztec.network/network/reference/cli_reference

Version: Ignition (v2.1.9)

**Configuration notes:**

* The environment variable name corresponding to each flag is shown as $ENV\_VAR on the right hand side.
* If two subsystems can contain the same configuration option, only one needs to be provided. For example, `--archiver.blobSinkUrl` and `--sequencer.blobSinkUrl` point to the same value if the node is started with both the `--archiver` and `--sequencer` options.

```
  MISC  
  
    --network <value>                                                                                                                      ($NETWORK)  
          Network to run Aztec on  
  
    --auto-update <value>                                                    (default: disabled)                                           ($AUTO_UPDATE)  
          The auto update mode for this node  
  
    --auto-update-url <value>                                                                                                              ($AUTO_UPDATE_URL)  
          Base URL to check for updates  
  
    --sync-mode <value>                                                      (default: snapshot)                                           ($SYNC_MODE)  
          Set sync mode to `full` to always sync via L1, `snapshot` to download a snapshot if there is no local data, `force-snapshot` to download even if there is local data.  
  
    --snapshots-url <value>                                                                                                                ($SYNC_SNAPSHOTS_URL)  
          Base URL for snapshots index.  
  
  SANDBOX  
  
    --sandbox  
          Starts Aztec Sandbox  
  
    --sandbox.noPXE                                                                                                                        ($NO_PXE)  
          Do not expose PXE service on sandbox start  
  
    --sandbox.l1Mnemonic <value>                                             (default: test test test test test test test test test test test junk)($MNEMONIC)  
          Mnemonic for L1 accounts. Will be used  
  
    --sandbox.deployAztecContractsSalt <value>                                                                                             ($DEPLOY_AZTEC_CONTRACTS_SALT)  
          Numeric salt for deploying L1 Aztec contracts before starting the sandbox. Needs mnemonic or private key to be set.  
  
  API  
  
    --port <value>                                                           (default: 8080)                                               ($AZTEC_PORT)  
          Port to run the Aztec Services on  
  
    --admin-port <value>                                                     (default: 8880)                                               ($AZTEC_ADMIN_PORT)  
          Port to run admin APIs of Aztec Services on  
  
    --api-prefix <value>                                                                                                                   ($API_PREFIX)  
          Prefix for API routes on any service that is started  
  
  ETHEREUM  
  
    --l1-chain-id <value>                                                                                                                  ($L1_CHAIN_ID)  
          The chain ID of the ethereum host.  
  
    --l1-rpc-urls <value>                                                                                                                  ($ETHEREUM_HOSTS)  
          List of URLs of Ethereum RPC nodes that services will connect to (comma separated).  
  
    --l1-consensus-host-urls <value>                                                                                                       ($L1_CONSENSUS_HOST_URLS)  
          List of URLs of the Ethereum consensus nodes that services will connect to (comma separated)  
  
    --l1-consensus-host-api-keys <value>                                                                                                   ($L1_CONSENSUS_HOST_API_KEYS)  
          List of API keys for the corresponding L1 consensus clients, if needed. Added to the end of the corresponding URL as "?key=<api-key>" unless a header is defined  
  
    --l1-consensus-host-api-key-headers <value>                                                                                            ($L1_CONSENSUS_HOST_API_KEY_HEADERS)  
          List of header names for the corresponding L1 consensus client API keys, if needed. Added to the corresponding request as "<api-key-header>: <api-key>"  
  
  L1 CONTRACTS  
  
    --registry-address <value>                                                                                                             ($REGISTRY_CONTRACT_ADDRESS)  
          The deployed L1 registry contract address.  
  
    --rollup-version <value>                                                                                                               ($ROLLUP_VERSION)  
          The version of the rollup.  
  
  STORAGE  
  
    --data-directory <value>                                                                                                               ($DATA_DIRECTORY)  
          Optional dir to store data. If omitted will store in memory.  
  
    --data-store-map-size-kb <value>                                         (default: 134217728)                                          ($DATA_STORE_MAP_SIZE_KB)  
          The maximum possible size of a data store DB in KB. Can be overridden by component-specific options.  
  
  WORLD STATE  
  
    --world-state-data-directory <value>                                                                                                   ($WS_DATA_DIRECTORY)  
          Optional directory for the world state database  
  
    --world-state-db-map-size-kb <value>                                                                                                   ($WS_DB_MAP_SIZE_KB)  
          The maximum possible size of the world state DB in KB. Overwrites the general dataStoreMapSizeKb.  
  
    --world-state-block-history <value>                                      (default: 64)                                                 ($WS_NUM_HISTORIC_BLOCKS)  
          The number of historic blocks to maintain. Values less than 1 mean all history is maintained  
  
  AZTEC NODE  
  
    --node  
          Starts Aztec Node with options  
  
  ARCHIVER  
  
    --archiver  
          Starts Aztec Archiver with options  
  
    --archiver.blobSinkUrl <value>                                                                                                         ($BLOB_SINK_URL)  
          The URL of the blob sink  
  
    --archiver.blobSinkMapSizeKb <value>                                                                                                   ($BLOB_SINK_MAP_SIZE_KB)  
          The maximum possible size of the blob sink DB in KB. Overwrites the general dataStoreMapSizeKb.  
  
    --archiver.archiveApiUrl <value>                                                                                                       ($BLOB_SINK_ARCHIVE_API_URL)  
          The URL of the archive API  
  
    --archiver.archiverPollingIntervalMS <value>                             (default: 500)                                                ($ARCHIVER_POLLING_INTERVAL_MS)  
          The polling interval in ms for retrieving new L2 blocks and encrypted logs.  
  
    --archiver.archiverBatchSize <value>                                     (default: 100)                                                ($ARCHIVER_BATCH_SIZE)  
          The number of L2 blocks the archiver will attempt to download at a time.  
  
    --archiver.maxLogs <value>                                               (default: 1000)                                               ($ARCHIVER_MAX_LOGS)  
          The max number of logs that can be obtained in 1 "getPublicLogs" call.  
  
    --archiver.archiverStoreMapSizeKb <value>                                                                                              ($ARCHIVER_STORE_MAP_SIZE_KB)  
          The maximum possible size of the archiver DB in KB. Overwrites the general dataStoreMapSizeKb.  
  
    --archiver.skipValidateBlockAttestations <value>  
          Whether to skip validating block attestations (use only for testing).  
  
  SEQUENCER  
  
    --sequencer  
          Starts Aztec Sequencer with options  
  
    --sequencer.validatorPrivateKeys <value>                                 (default: [Redacted])                                         ($VALIDATOR_PRIVATE_KEYS)  
          List of private keys of the validators participating in attestation duties  
  
    --sequencer.validatorAddresses <value>                                   (default: )                                                   ($VALIDATOR_ADDRESSES)  
          List of addresses of the validators to use with remote signers  
  
    --sequencer.disableValidator <value>                                                                                                   ($VALIDATOR_DISABLED)  
          Do not run the validator  
  
    --sequencer.disabledValidators <value>                                   (default: )  
          Temporarily disable these specific validator addresses  
  
    --sequencer.attestationPollingIntervalMs <value>                         (default: 200)                                                ($VALIDATOR_ATTESTATIONS_POLLING_INTERVAL_MS)  
          Interval between polling for new attestations  
  
    --sequencer.validatorReexecute <value>                                   (default: true)                                               ($VALIDATOR_REEXECUTE)  
          Re-execute transactions before attesting  
  
    --sequencer.validatorReexecuteDeadlineMs <value>                         (default: 6000)                                               ($VALIDATOR_REEXECUTE_DEADLINE_MS)  
          Will re-execute until this many milliseconds are left in the slot  
  
    --sequencer.alwaysReexecuteBlockProposals <value>                                                                                      ($ALWAYS_REEXECUTE_BLOCK_PROPOSALS)  
          Whether to always reexecute block proposals, even for non-validator nodes (useful for monitoring network status).  
  
    --sequencer.transactionPollingIntervalMS <value>                         (default: 500)                                                ($SEQ_TX_POLLING_INTERVAL_MS)  
          The number of ms to wait between polling for pending txs.  
  
    --sequencer.maxTxsPerBlock <value>                                       (default: 32)                                                 ($SEQ_MAX_TX_PER_BLOCK)  
          The maximum number of txs to include in a block.  
  
    --sequencer.minTxsPerBlock <value>                                       (default: 1)                                                  ($SEQ_MIN_TX_PER_BLOCK)  
          The minimum number of txs to include in a block.  
  
    --sequencer.publishTxsWithProposals <value>                                                                                            ($SEQ_PUBLISH_TXS_WITH_PROPOSALS)  
          Whether to publish txs with proposals.  
  
    --sequencer.maxL2BlockGas <value>                                        (default: 10000000000)                                        ($SEQ_MAX_L2_BLOCK_GAS)  
          The maximum L2 block gas.  
  
    --sequencer.maxDABlockGas <value>                                        (default: 10000000000)                                        ($SEQ_MAX_DA_BLOCK_GAS)  
          The maximum DA block gas.  
  
    --sequencer.coinbase <value>                                                                                                           ($COINBASE)  
          Recipient of block reward.  
  
    --sequencer.feeRecipient <value>                                                                                                       ($FEE_RECIPIENT)  
          Address to receive fees.  
  
    --sequencer.acvmWorkingDirectory <value>                                                                                               ($ACVM_WORKING_DIRECTORY)  
          The working directory to use for simulation/proving  
  
    --sequencer.acvmBinaryPath <value>                                                                                                     ($ACVM_BINARY_PATH)  
          The path to the ACVM binary  
  
    --sequencer.maxBlockSizeInBytes <value>                                  (default: 1048576)                                            ($SEQ_MAX_BLOCK_SIZE_IN_BYTES)  
          Max block size  
  
    --sequencer.enforceTimeTable <value>                                     (default: true)                                               ($SEQ_ENFORCE_TIME_TABLE)  
          Whether to enforce the time table when building blocks  
  
    --sequencer.governanceProposerPayload <value>                            (default: 0x0000000000000000000000000000000000000000)         ($GOVERNANCE_PROPOSER_PAYLOAD_ADDRESS)  
          The address of the payload for the governanceProposer  
  
    --sequencer.maxL1TxInclusionTimeIntoSlot <value>                                                                                       ($SEQ_MAX_L1_TX_INCLUSION_TIME_INTO_SLOT)  
          How many seconds into an L1 slot we can still send a tx and get it mined.  
  
    --sequencer.attestationPropagationTime <value>                           (default: 2)                                                  ($SEQ_ATTESTATION_PROPAGATION_TIME)  
          How many seconds it takes for proposals and attestations to travel across the p2p layer (one-way)  
  
    --sequencer.secondsBeforeInvalidatingBlockAsCommitteeMember <value>      (default: 144)                                                ($SEQ_SECONDS_BEFORE_INVALIDATING_BLOCK_AS_COMMITTEE_MEMBER)  
          How many seconds to wait before trying to invalidate a block from the pending chain as a committee member (zero to never invalidate). The next proposer is expected to invalidate, so the committee acts as a fallback.  
  
    --sequencer.secondsBeforeInvalidatingBlockAsNonCommitteeMember <value>   (default: 432)                                                ($SEQ_SECONDS_BEFORE_INVALIDATING_BLOCK_AS_NON_COMMITTEE_MEMBER)  
          How many seconds to wait before trying to invalidate a block from the pending chain as a non-committee member (zero to never invalidate). The next proposer is expected to invalidate, then the committee, so other sequencers act as a fallback.  
  
    --sequencer.txPublicSetupAllowList <value>                                                                                             ($TX_PUBLIC_SETUP_ALLOWLIST)  
          The list of functions calls allowed to run in setup  
  
    --sequencer.keyStoreDirectory <value>                                                                                                  ($KEY_STORE_DIRECTORY)  
          Location of key store directory  
  
    --sequencer.publisherPrivateKeys <value>                                 (default: )                                                   ($SEQ_PUBLISHER_PRIVATE_KEYS)  
          The private keys to be used by the publisher.  
  
    --sequencer.publisherAddresses <value>                                   (default: )                                                   ($SEQ_PUBLISHER_ADDRESSES)  
          The addresses of the publishers to use with remote signers  
  
    --sequencer.l1PublishRetryIntervalMS <value>                             (default: 1000)                                               ($SEQ_PUBLISH_RETRY_INTERVAL_MS)  
          The interval to wait between publish retries.  
  
    --sequencer.publisherAllowInvalidStates <value>                                                                                        ($SEQ_PUBLISHER_ALLOW_INVALID_STATES)  
          True to use publishers in invalid states (timed out, cancelled, etc) if no other is available  
  
    --sequencer.blobSinkUrl <value>                                                                                                        ($BLOB_SINK_URL)  
          The URL of the blob sink  
  
    --sequencer.archiveApiUrl <value>                                                                                                      ($BLOB_SINK_ARCHIVE_API_URL)  
          The URL of the archive API  
  
  BLOB SINK  
  
    --blob-sink  
          Starts Aztec Blob Sink with options  
  
    --blobSink.port <value>                                                                                                                ($BLOB_SINK_PORT)  
          The port to run the blob sink server on  
  
    --blobSink.blobSinkMapSizeKb <value>                                                                                                   ($BLOB_SINK_MAP_SIZE_KB)  
          The maximum possible size of the blob sink DB in KB. Overwrites the general dataStoreMapSizeKb.  
  
    --blobSink.archiveApiUrl <value>                                                                                                       ($BLOB_SINK_ARCHIVE_API_URL)  
          The URL of the archive API  
  
  PROVER NODE  
  
    --prover-node  
          Starts Aztec Prover Node with options  
  
    --proverNode.keyStoreDirectory <value>                                                                                                 ($KEY_STORE_DIRECTORY)  
          Location of key store directory  
  
    --proverNode.acvmWorkingDirectory <value>                                                                                              ($ACVM_WORKING_DIRECTORY)  
          The working directory to use for simulation/proving  
  
    --proverNode.acvmBinaryPath <value>                                                                                                    ($ACVM_BINARY_PATH)  
          The path to the ACVM binary  
  
    --proverNode.bbWorkingDirectory <value>                                                                                                ($BB_WORKING_DIRECTORY)  
          The working directory to use for proving  
  
    --proverNode.bbBinaryPath <value>                                                                                                      ($BB_BINARY_PATH)  
          The path to the bb binary  
  
    --proverNode.bbSkipCleanup <value>                                                                                                     ($BB_SKIP_CLEANUP)  
          Whether to skip cleanup of bb temporary files  
  
    --proverNode.numConcurrentIVCVerifiers <value>                           (default: 8)                                                  ($BB_NUM_IVC_VERIFIERS)  
          Max number of client IVC verifiers to run concurrently  
  
    --proverNode.bbIVCConcurrency <value>                                    (default: 1)                                                  ($BB_IVC_CONCURRENCY)  
          Number of threads to use for IVC verification  
  
    --proverNode.nodeUrl <value>                                                                                                           ($AZTEC_NODE_URL)  
          The URL to the Aztec node to take proving jobs from  
  
    --proverNode.proverId <value>                                                                                                          ($PROVER_ID)  
          Hex value that identifies the prover. Defaults to the address used for submitting proofs if not set.  
  
    --proverNode.failedProofStore <value>                                                                                                  ($PROVER_FAILED_PROOF_STORE)  
          Store for failed proof inputs. Google cloud storage is only supported at the moment. Set this value as gs://bucket-name/path/to/store.  
  
    --proverNode.l1PublishRetryIntervalMS <value>                            (default: 1000)                                               ($PROVER_PUBLISH_RETRY_INTERVAL_MS)  
          The interval to wait between publish retries.  
  
    --proverNode.publisherAllowInvalidStates <value>                                                                                       ($PROVER_PUBLISHER_ALLOW_INVALID_STATES)  
          True to use publishers in invalid states (timed out, cancelled, etc) if no other is available  
  
    --proverNode.publisherPrivateKeys <value>                                (default: )                                                   ($PROVER_PUBLISHER_PRIVATE_KEYS)  
          The private keys to be used by the publisher.  
  
    --proverNode.publisherAddresses <value>                                  (default: )                                                   ($PROVER_PUBLISHER_ADDRESSES)  
          The addresses of the publishers to use with remote signers  
  
    --proverNode.proverNodeMaxPendingJobs <value>                            (default: 10)                                                 ($PROVER_NODE_MAX_PENDING_JOBS)  
          The maximum number of pending jobs for the prover node  
  
    --proverNode.proverNodePollingIntervalMs <value>                         (default: 1000)                                               ($PROVER_NODE_POLLING_INTERVAL_MS)  
          The interval in milliseconds to poll for new jobs  
  
    --proverNode.proverNodeMaxParallelBlocksPerEpoch <value>                 (default: 32)                                                 ($PROVER_NODE_MAX_PARALLEL_BLOCKS_PER_EPOCH)  
          The Maximum number of blocks to process in parallel while proving an epoch  
  
    --proverNode.proverNodeFailedEpochStore <value>                                                                                        ($PROVER_NODE_FAILED_EPOCH_STORE)  
          File store where to upload node state when an epoch fails to be proven  
  
    --proverNode.proverNodeEpochProvingDelayMs <value>  
          Optional delay in milliseconds to wait before proving a new epoch  
  
    --proverNode.txGatheringIntervalMs <value>                               (default: 1000)                                               ($PROVER_NODE_TX_GATHERING_INTERVAL_MS)  
          How often to check that tx data is available  
  
    --proverNode.txGatheringBatchSize <value>                                (default: 10)                                                 ($PROVER_NODE_TX_GATHERING_BATCH_SIZE)  
          How many transactions to gather from a node in a single request  
  
    --proverNode.txGatheringMaxParallelRequestsPerNode <value>               (default: 100)                                                ($PROVER_NODE_TX_GATHERING_MAX_PARALLEL_REQUESTS_PER_NODE)  
          How many tx requests to make in parallel to each node  
  
    --proverNode.txGatheringTimeoutMs <value>                                (default: 120000)                                             ($PROVER_NODE_TX_GATHERING_TIMEOUT_MS)  
          How long to wait for tx data to be available before giving up  
  
  PROVER BROKER  
  
    --prover-broker  
          Starts Aztec proving job broker  
  
    --proverBroker.proverBrokerJobTimeoutMs <value>                          (default: 30000)                                              ($PROVER_BROKER_JOB_TIMEOUT_MS)  
          Jobs are retried if not kept alive for this long  
  
    --proverBroker.proverBrokerPollIntervalMs <value>                        (default: 1000)                                               ($PROVER_BROKER_POLL_INTERVAL_MS)  
          The interval to check job health status  
  
    --proverBroker.proverBrokerJobMaxRetries <value>                         (default: 3)                                                  ($PROVER_BROKER_JOB_MAX_RETRIES)  
          If starting a prover broker locally, the max number of retries per proving job  
  
    --proverBroker.proverBrokerBatchSize <value>                             (default: 100)                                                ($PROVER_BROKER_BATCH_SIZE)  
          The prover broker writes jobs to disk in batches  
  
    --proverBroker.proverBrokerBatchIntervalMs <value>                       (default: 50)                                                 ($PROVER_BROKER_BATCH_INTERVAL_MS)  
          How often to flush batches to disk  
  
    --proverBroker.proverBrokerMaxEpochsToKeepResultsFor <value>             (default: 1)                                                  ($PROVER_BROKER_MAX_EPOCHS_TO_KEEP_RESULTS_FOR)  
          The maximum number of epochs to keep results for  
  
    --proverBroker.proverBrokerStoreMapSizeKb <value>                                                                                      ($PROVER_BROKER_STORE_MAP_SIZE_KB)  
          The size of the prover broker's database. Will override the dataStoreMapSizeKb if set.  
  
  PROVER AGENT  
  
    --prover-agent  
          Starts Aztec Prover Agent with options  
  
    --proverAgent.proverAgentCount <value>                                   (default: 1)                                                  ($PROVER_AGENT_COUNT)  
          Whether this prover has a local prover agent  
  
    --proverAgent.proverAgentPollIntervalMs <value>                          (default: 1000)                                               ($PROVER_AGENT_POLL_INTERVAL_MS)  
          The interval agents poll for jobs at  
  
    --proverAgent.proverAgentProofTypes <value>                                                                                            ($PROVER_AGENT_PROOF_TYPES)  
          The types of proofs the prover agent can generate  
  
    --proverAgent.proverBrokerUrl <value>                                                                                                  ($PROVER_BROKER_HOST)  
          The URL where this agent takes jobs from  
  
    --proverAgent.realProofs <value>                                         (default: true)                                               ($PROVER_REAL_PROOFS)  
          Whether to construct real proofs  
  
    --proverAgent.proverTestDelayType <value>                                (default: fixed)                                              ($PROVER_TEST_DELAY_TYPE)  
          The type of artificial delay to introduce  
  
    --proverAgent.proverTestDelayMs <value>                                                                                                ($PROVER_TEST_DELAY_MS)  
          Artificial delay to introduce to all operations to the test prover.  
  
    --proverAgent.proverTestDelayFactor <value>                              (default: 1)                                                  ($PROVER_TEST_DELAY_FACTOR)  
          If using realistic delays, what percentage of realistic times to apply.  
  
  P2P SUBSYSTEM  
  
    --p2p-enabled [value]                                                                                                                  ($P2P_ENABLED)  
          Enable P2P subsystem  
  
    --p2p.p2pDiscoveryDisabled <value>                                                                                                     ($P2P_DISCOVERY_DISABLED)  
          A flag dictating whether the P2P discovery system should be disabled.  
  
    --p2p.blockCheckIntervalMS <value>                                       (default: 100)                                                ($P2P_BLOCK_CHECK_INTERVAL_MS)  
          The frequency in which to check for new L2 blocks.  
  
    --p2p.debugDisableColocationPenalty <value>                                                                                            ($DEBUG_P2P_DISABLE_COLOCATION_PENALTY)  
          DEBUG: Disable colocation penalty - NEVER set to true in production  
  
    --p2p.peerCheckIntervalMS <value>                                        (default: 30000)                                              ($P2P_PEER_CHECK_INTERVAL_MS)  
          The frequency in which to check for new peers.  
  
    --p2p.l2QueueSize <value>                                                (default: 1000)                                               ($P2P_L2_QUEUE_SIZE)  
          Size of queue of L2 blocks to store.  
  
    --p2p.listenAddress <value>                                              (default: 0.0.0.0)                                            ($P2P_LISTEN_ADDR)  
          The listen address. ipv4 address.  
  
    --p2p.p2pPort <value>                                                    (default: 40400)                                              ($P2P_PORT)  
          The port for the P2P service. Defaults to 40400  
  
    --p2p.p2pBroadcastPort <value>                                                                                                         ($P2P_BROADCAST_PORT)  
          The port to broadcast the P2P service on (included in the node's ENR). Defaults to P2P_PORT.  
  
    --p2p.p2pIp <value>                                                                                                                    ($P2P_IP)  
          The IP address for the P2P service. ipv4 address.  
  
    --p2p.peerIdPrivateKey <value>                                                                                                         ($PEER_ID_PRIVATE_KEY)  
          An optional peer id private key. If blank, will generate a random key.  
  
    --p2p.peerIdPrivateKeyPath <value>                                                                                                     ($PEER_ID_PRIVATE_KEY_PATH)  
          An optional path to store generated peer id private keys. If blank, will default to storing any generated keys in the root of the data directory.  
  
    --p2p.bootstrapNodes <value>                                             (default: )                                                   ($BOOTSTRAP_NODES)  
          A list of bootstrap peer ENRs to connect to. Separated by commas.  
  
    --p2p.bootstrapNodeEnrVersionCheck <value>                                                                                             ($P2P_BOOTSTRAP_NODE_ENR_VERSION_CHECK)  
          Whether to check the version of the bootstrap node ENR.  
  
    --p2p.bootstrapNodesAsFullPeers <value>                                                                                                ($P2P_BOOTSTRAP_NODES_AS_FULL_PEERS)  
          Whether to consider our configured bootnodes as full peers  
  
    --p2p.maxPeerCount <value>                                               (default: 100)                                                ($P2P_MAX_PEERS)  
          The maximum number of peers to connect to.  
  
    --p2p.queryForIp <value>                                                                                                               ($P2P_QUERY_FOR_IP)  
          If announceUdpAddress or announceTcpAddress are not provided, query for the IP address of the machine. Default is false.  
  
    --p2p.gossipsubInterval <value>                                          (default: 700)                                                ($P2P_GOSSIPSUB_INTERVAL_MS)  
          The interval of the gossipsub heartbeat to perform maintenance tasks.  
  
    --p2p.gossipsubD <value>                                                 (default: 8)                                                  ($P2P_GOSSIPSUB_D)  
          The D parameter for the gossipsub protocol.  
  
    --p2p.gossipsubDlo <value>                                               (default: 4)                                                  ($P2P_GOSSIPSUB_DLO)  
          The Dlo parameter for the gossipsub protocol.  
  
    --p2p.gossipsubDhi <value>                                               (default: 12)                                                 ($P2P_GOSSIPSUB_DHI)  
          The Dhi parameter for the gossipsub protocol.  
  
    --p2p.gossipsubDLazy <value>                                             (default: 8)                                                  ($P2P_GOSSIPSUB_DLAZY)  
          The Dlazy parameter for the gossipsub protocol.  
  
    --p2p.gossipsubFloodPublish <value>                                                                                                    ($P2P_GOSSIPSUB_FLOOD_PUBLISH)  
          Whether to flood publish messages. - For testing purposes only  
  
    --p2p.gossipsubMcacheLength <value>                                      (default: 6)                                                  ($P2P_GOSSIPSUB_MCACHE_LENGTH)  
          The number of gossipsub interval message cache windows to keep.  
  
    --p2p.gossipsubMcacheGossip <value>                                      (default: 3)                                                  ($P2P_GOSSIPSUB_MCACHE_GOSSIP)  
          How many message cache windows to include when gossiping with other peers.  
  
    --p2p.gossipsubSeenTTL <value>                                           (default: 1200000)                                            ($P2P_GOSSIPSUB_SEEN_TTL)  
          How long to keep message IDs in the seen cache.  
  
    --p2p.gossipsubTxTopicWeight <value>                                     (default: 1)                                                  ($P2P_GOSSIPSUB_TX_TOPIC_WEIGHT)  
          The weight of the tx topic for the gossipsub protocol.  
  
    --p2p.gossipsubTxInvalidMessageDeliveriesWeight <value>                  (default: -20)                                                ($P2P_GOSSIPSUB_TX_INVALID_MESSAGE_DELIVERIES_WEIGHT)  
          The weight of the tx invalid message deliveries for the gossipsub protocol.  
  
    --p2p.gossipsubTxInvalidMessageDeliveriesDecay <value>                   (default: 0.5)                                                ($P2P_GOSSIPSUB_TX_INVALID_MESSAGE_DELIVERIES_DECAY)  
          Determines how quickly the penalty for invalid message deliveries decays over time. Between 0 and 1.  
  
    --p2p.peerPenaltyValues <value>                                          (default: 2,10,50)                                            ($P2P_PEER_PENALTY_VALUES)  
          The values for the peer scoring system. Passed as a comma separated list of values in order: low, mid, high tolerance errors.  
  
    --p2p.doubleSpendSeverePeerPenaltyWindow <value>                         (default: 30)                                                 ($P2P_DOUBLE_SPEND_SEVERE_PEER_PENALTY_WINDOW)  
          The "age" (in L2 blocks) of a tx after which we heavily penalize a peer for sending it.  
  
    --p2p.blockRequestBatchSize <value>                                      (default: 20)                                                 ($P2P_BLOCK_REQUEST_BATCH_SIZE)  
          The number of blocks to fetch in a single batch.  
  
    --p2p.archivedTxLimit <value>                                                                                                          ($P2P_ARCHIVED_TX_LIMIT)  
          The number of transactions that will be archived. If the limit is set to 0 then archiving will be disabled.  
  
    --p2p.trustedPeers <value>                                               (default: )                                                   ($P2P_TRUSTED_PEERS)  
          A list of trusted peer ENRs that will always be persisted. Separated by commas.  
  
    --p2p.privatePeers <value>                                               (default: )                                                   ($P2P_PRIVATE_PEERS)  
          A list of private peer ENRs that will always be persisted and not be used for discovery. Separated by commas.  
  
    --p2p.preferredPeers <value>                                             (default: )                                                   ($P2P_PREFERRED_PEERS)  
          A list of preferred peer ENRs that will always be persisted and not be used for discovery. Separated by commas.  
  
    --p2p.p2pStoreMapSizeKb <value>                                                                                                        ($P2P_STORE_MAP_SIZE_KB)  
          The maximum possible size of the P2P DB in KB. Overwrites the general dataStoreMapSizeKb.  
  
    --p2p.txPublicSetupAllowList <value>                                                                                                   ($TX_PUBLIC_SETUP_ALLOWLIST)  
          The list of functions calls allowed to run in setup  
  
    --p2p.maxTxPoolSize <value>                                              (default: 100000000)                                          ($P2P_MAX_TX_POOL_SIZE)  
          The maximum cumulative tx size of pending txs (in bytes) before evicting lower priority txs.  
  
    --p2p.txPoolOverflowFactor <value>                                       (default: 1.1)                                                ($P2P_TX_POOL_OVERFLOW_FACTOR)  
          How much the tx pool can overflow before it starts evicting txs. Must be greater than 1  
  
    --p2p.seenMessageCacheSize <value>                                       (default: 100000)                                             ($P2P_SEEN_MSG_CACHE_SIZE)  
          The number of messages to keep in the seen message cache  
  
    --p2p.p2pDisableStatusHandshake <value>                                                                                                ($P2P_DISABLE_STATUS_HANDSHAKE)  
          True to disable the status handshake on peer connected.  
  
    --p2p.p2pAllowOnlyValidators <value>                                                                                                   ($P2P_ALLOW_ONLY_VALIDATORS)  
          True to only permit validators to connect.  
  
    --p2p.p2pMaxFailedAuthAttemptsAllowed <value>                            (default: 3)                                                  ($P2P_MAX_AUTH_FAILED_ATTEMPTS_ALLOWED)  
          Number of auth attempts to allow before peer is banned. Number is inclusive  
  
    --p2p.dropTransactions <value>                                                                                                         ($P2P_DROP_TX)  
          True to simulate discarding transactions. - For testing purposes only  
  
    --p2p.dropTransactionsProbability <value>                                                                                              ($P2P_DROP_TX_CHANCE)  
          The probability that a transaction is discarded. - For testing purposes only  
  
    --p2p.disableTransactions <value>                                                                                                      ($TRANSACTIONS_DISABLED)  
          Whether transactions are disabled for this node. This means transactions will be rejected at the RPC and P2P layers.  
  
    --p2p.txPoolDeleteTxsAfterReorg <value>                                                                                                ($P2P_TX_POOL_DELETE_TXS_AFTER_REORG)  
          Whether to delete transactions from the pool after a reorg instead of moving them back to pending.  
  
    --p2p.overallRequestTimeoutMs <value>                                    (default: 10000)                                              ($P2P_REQRESP_OVERALL_REQUEST_TIMEOUT_MS)  
          The overall timeout for a request response operation.  
  
    --p2p.individualRequestTimeoutMs <value>                                 (default: 10000)                                              ($P2P_REQRESP_INDIVIDUAL_REQUEST_TIMEOUT_MS)  
          The timeout for an individual request response peer interaction.  
  
    --p2p.dialTimeoutMs <value>                                              (default: 5000)                                               ($P2P_REQRESP_DIAL_TIMEOUT_MS)  
          How long to wait for the dial protocol to establish a connection  
  
    --p2p.p2pOptimisticNegotiation <value>                                                                                                 ($P2P_REQRESP_OPTIMISTIC_NEGOTIATION)  
          Whether to use optimistic protocol negotiation when dialing to another peer (opposite of `negotiateFully`).  
  
    --p2p.txCollectionFastNodesTimeoutBeforeReqRespMs <value>                (default: 200)                                                ($TX_COLLECTION_FAST_NODES_TIMEOUT_BEFORE_REQ_RESP_MS)  
          How long to wait before starting reqresp for fast collection  
  
    --p2p.txCollectionSlowNodesIntervalMs <value>                            (default: 12000)                                              ($TX_COLLECTION_SLOW_NODES_INTERVAL_MS)  
          How often to collect from configured nodes in the slow collection loop  
  
    --p2p.txCollectionSlowReqRespIntervalMs <value>                          (default: 12000)                                              ($TX_COLLECTION_SLOW_REQ_RESP_INTERVAL_MS)  
          How often to collect from peers via reqresp in the slow collection loop  
  
    --p2p.txCollectionSlowReqRespTimeoutMs <value>                           (default: 20000)                                              ($TX_COLLECTION_SLOW_REQ_RESP_TIMEOUT_MS)  
          How long to wait for a reqresp response during slow collection  
  
    --p2p.txCollectionReconcileIntervalMs <value>                            (default: 60000)                                              ($TX_COLLECTION_RECONCILE_INTERVAL_MS)  
          How often to reconcile found txs from the tx pool  
  
    --p2p.txCollectionDisableSlowDuringFastRequests <value>                  (default: true)                                               ($TX_COLLECTION_DISABLE_SLOW_DURING_FAST_REQUESTS)  
          Whether to disable the slow collection loop if we are dealing with any immediate requests  
  
    --p2p.txCollectionFastNodeIntervalMs <value>                             (default: 500)                                                ($TX_COLLECTION_FAST_NODE_INTERVAL_MS)  
          How many ms to wait between retried request to a node via RPC during fast collection  
  
    --p2p.txCollectionNodeRpcUrls <value>                                    (default: )                                                   ($TX_COLLECTION_NODE_RPC_URLS)  
          A comma-separated list of Aztec node RPC URLs to use for tx collection  
  
    --p2p.txCollectionFastMaxParallelRequestsPerNode <value>                 (default: 4)                                                  ($TX_COLLECTION_FAST_MAX_PARALLEL_REQUESTS_PER_NODE)  
          Maximum number of parallel requests to make to a node during fast collection  
  
    --p2p.txCollectionNodeRpcMaxBatchSize <value>                            (default: 50)                                                 ($TX_COLLECTION_NODE_RPC_MAX_BATCH_SIZE)  
          Maximum number of transactions to request from a node in a single batch  
  
  P2P BOOTSTRAP  
  
    --p2p-bootstrap  
          Starts Aztec P2P Bootstrap with options  
  
    --p2pBootstrap.p2pBroadcastPort <value>                                                                                                ($P2P_BROADCAST_PORT)  
          The port to broadcast the P2P service on (included in the node's ENR). Defaults to P2P_PORT.  
  
    --p2pBootstrap.peerIdPrivateKeyPath <value>                                                                                            ($PEER_ID_PRIVATE_KEY_PATH)  
          An optional path to store generated peer id private keys. If blank, will default to storing any generated keys in the root of the data directory.  
  
  TELEMETRY  
  
    --tel.metricsCollectorUrl <value>                                                                                                      ($OTEL_EXPORTER_OTLP_METRICS_ENDPOINT)  
          The URL of the telemetry collector for metrics  
  
    --tel.tracesCollectorUrl <value>                                                                                                       ($OTEL_EXPORTER_OTLP_TRACES_ENDPOINT)  
          The URL of the telemetry collector for traces  
  
    --tel.logsCollectorUrl <value>                                                                                                         ($OTEL_EXPORTER_OTLP_LOGS_ENDPOINT)  
          The URL of the telemetry collector for logs  
  
    --tel.otelCollectIntervalMs <value>                                      (default: 60000)                                              ($OTEL_COLLECT_INTERVAL_MS)  
          The interval at which to collect metrics  
  
    --tel.otelExportTimeoutMs <value>                                        (default: 30000)                                              ($OTEL_EXPORT_TIMEOUT_MS)  
          The timeout for exporting metrics  
  
    --tel.otelExcludeMetrics <value>                                         (default: )                                                   ($OTEL_EXCLUDE_METRICS)  
          A list of metric prefixes to exclude from export  
  
    --tel.publicMetricsCollectorUrl <value>                                                                                                ($PUBLIC_OTEL_EXPORTER_OTLP_METRICS_ENDPOINT)  
          A URL to publish a subset of metrics for public consumption  
  
    --tel.publicMetricsCollectFrom <value>                                   (default: )                                                   ($PUBLIC_OTEL_COLLECT_FROM)  
          The role types to collect metrics from  
  
    --tel.publicIncludeMetrics <value>                                       (default: )                                                   ($PUBLIC_OTEL_INCLUDE_METRICS)  
          A list of metric prefixes to publicly export  
  
    --tel.publicMetricsOptOut <value>                                                                                                      ($PUBLIC_OTEL_OPT_OUT)  
          Whether to opt out of sharing optional telemetry  
  
  BOT  
  
    --bot  
          Starts Aztec Bot with options  
  
    --bot.nodeUrl <value>                                                                                                                  ($AZTEC_NODE_URL)  
          The URL to the Aztec node to check for tx pool status.  
  
    --bot.nodeAdminUrl <value>                                                                                                             ($AZTEC_NODE_ADMIN_URL)  
          The URL to the Aztec node admin API to force-flush txs if configured.  
  
    --bot.l1Mnemonic <value>                                                                                                               ($BOT_L1_MNEMONIC)  
          The mnemonic for the account to bridge fee juice from L1.  
  
    --bot.l1PrivateKey <value>                                                                                                             ($BOT_L1_PRIVATE_KEY)  
          The private key for the account to bridge fee juice from L1.  
  
    --bot.l1ToL2MessageTimeoutSeconds <value>                                (default: 3600)                                               ($BOT_L1_TO_L2_TIMEOUT_SECONDS)  
          How long to wait for L1 to L2 messages to become available on L2  
  
    --bot.senderPrivateKey <value>                                                                                                         ($BOT_PRIVATE_KEY)  
          Signing private key for the sender account.  
  
    --bot.senderSalt <value>                                                                                                               ($BOT_ACCOUNT_SALT)  
          The salt to use to deploy the sender account.  
  
    --bot.tokenSalt <value>                                                  (default: 0x0000000000000000000000000000000000000000000000000000000000000001)($BOT_TOKEN_SALT)  
          The salt to use to deploy the token contract.  
  
    --bot.txIntervalSeconds <value>                                          (default: 60)                                                 ($BOT_TX_INTERVAL_SECONDS)  
          Every how many seconds should a new tx be sent.  
  
    --bot.privateTransfersPerTx <value>                                      (default: 1)                                                  ($BOT_PRIVATE_TRANSFERS_PER_TX)  
          How many private token transfers are executed per tx.  
  
    --bot.publicTransfersPerTx <value>                                       (default: 1)                                                  ($BOT_PUBLIC_TRANSFERS_PER_TX)  
          How many public token transfers are executed per tx.  
  
    --bot.feePaymentMethod <value>                                           (default: fee_juice)                                          ($BOT_FEE_PAYMENT_METHOD)  
          How to handle fee payments. (Options: fee_juice)  
  
    --bot.noStart <value>                                                                                                                  ($BOT_NO_START)  
          True to not automatically setup or start the bot on initialization.  
  
    --bot.txMinedWaitSeconds <value>                                         (default: 180)                                                ($BOT_TX_MINED_WAIT_SECONDS)  
          How long to wait for a tx to be mined before reporting an error.  
  
    --bot.followChain <value>                                                (default: NONE)                                               ($BOT_FOLLOW_CHAIN)  
          Which chain the bot follows  
  
    --bot.maxPendingTxs <value>                                              (default: 128)                                                ($BOT_MAX_PENDING_TXS)  
          Do not send a tx if the node's tx pool already has this many pending txs.  
  
    --bot.flushSetupTransactions <value>                                                                                                   ($BOT_FLUSH_SETUP_TRANSACTIONS)  
          Make a request for the sequencer to build a block after each setup transaction.  
  
    --bot.l2GasLimit <value>                                                                                                               ($BOT_L2_GAS_LIMIT)  
          L2 gas limit for the tx (empty to have the bot trigger an estimate gas).  
  
    --bot.daGasLimit <value>                                                                                                               ($BOT_DA_GAS_LIMIT)  
          DA gas limit for the tx (empty to have the bot trigger an estimate gas).  
  
    --bot.contract <value>                                                   (default: TokenContract)                                      ($BOT_TOKEN_CONTRACT)  
          Token contract to use  
  
    --bot.maxConsecutiveErrors <value>                                                                                                     ($BOT_MAX_CONSECUTIVE_ERRORS)  
          The maximum number of consecutive errors before the bot shuts down  
  
    --bot.stopWhenUnhealthy <value>                                                                                                        ($BOT_STOP_WHEN_UNHEALTHY)  
          Stops the bot if service becomes unhealthy  
  
    --bot.ammTxs <value>                                                                                                                   ($BOT_AMM_TXS)  
          Deploy an AMM and send swaps to it  
  
  PXE  
  
    --pxe  
          Starts Aztec PXE with options  
  
    --pxe.l2BlockBatchSize <value>                                           (default: 50)                                                 ($PXE_L2_BLOCK_BATCH_SIZE)  
          Maximum amount of blocks to pull from the stream in one request when synchronizing  
  
    --pxe.bbBinaryPath <value>                                                                                                             ($BB_BINARY_PATH)  
          Path to the BB binary  
  
    --pxe.bbWorkingDirectory <value>                                                                                                       ($BB_WORKING_DIRECTORY)  
          Working directory for the BB binary  
  
    --pxe.bbSkipCleanup <value>                                                                                                            ($BB_SKIP_CLEANUP)  
          True to skip cleanup of temporary files for debugging purposes  
  
    --pxe.proverEnabled <value>                                              (default: true)                                               ($PXE_PROVER_ENABLED)  
          Enable real proofs  
  
    --pxe.nodeUrl <value>                                                                                                                  ($AZTEC_NODE_URL)  
          Custom Aztec Node URL to connect to  
  
  TXE  
  
    --txe  
          Starts Aztec TXE with options
```

---


# Node JSON RPC API reference

Source: https://docs.aztec.network/network/reference/node_api_reference

Version: Ignition (v2.1.9)

On this page

This document provides a complete reference for the Aztec Node JSON RPC API. All methods are exposed via JSON RPC on the node's configured ports.

## API endpoint**Public RPC URL**: `http://localhost:8080`

**Admin URL**: `http://localhost:8880`

Note that the above ports are only defaults, and can be modified by setting `--port` and `--admin-port` flags upon startup.

All methods use standard JSON RPC 2.0 format with methods prefixed by `node_` or `nodeAdmin_`.

## Block queries## node\_getBlockNumberReturns the latest block number synchronized by the node.

**Parameters**: None

**Returns**: `number`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getBlockNumber","params":[],"id":1}'
```

## node\_getProvenBlockNumberReturns the latest proven block number.

**Parameters**: None

**Returns**: `number`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getProvenBlockNumber","params":[],"id":1}'
```

## node\_getL2TipsReturns the tips of the L2 chain (latest, pending, proven).

**Parameters**: None

**Returns**: Object containing `latest`, `pending`, and `proven` block info

**Example**:

```
curl -s -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getL2Tips","params":[],"id":67}' \  
  | jq -r ".result.proven.number"
```

## node\_getBlockGets a block by its number.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number or "latest"

**Returns**: `L2Block | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getBlock","params":[12345],"id":1}'
```

## node\_getBlocksGets multiple blocks in a range.

**Parameters**:

1. `from` - `number` - Starting block number (≥ 1)
2. `limit` - `number` - Max blocks to return (1-100)

**Returns**: `L2Block[]`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getBlocks","params":[100,50],"id":1}'
```

## node\_getBlockHeaderGets a block header.

**Parameters**:

1. `blockNumber` - `number | "latest" | undefined` - Block number or omit for latest

**Returns**: `BlockHeader | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getBlockHeader","params":["latest"],"id":1}'
```

## Transaction operations## node\_sendTxSubmits a transaction to the P2P mempool.

**Parameters**:

1. `tx` - `Tx` - The transaction object

**Returns**: `void`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_sendTx","params":[{"data":"0x..."}],"id":1}'
```

## node\_getTxReceiptGets a transaction receipt.

**Parameters**:

1. `txHash` - `string` - Transaction hash (32-byte hex)

**Returns**: `TxReceipt` - Receipt with status (mined, pending, or dropped)

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getTxReceipt","params":["0x1234..."],"id":1}'
```

## node\_getTxEffectGets the transaction effect for a given transaction.

**Parameters**:

1. `txHash` - `string` - Transaction hash

**Returns**: `IndexedTxEffect | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getTxEffect","params":["0x1234..."],"id":1}'
```

## node\_getTxByHashGets a single pending transaction by hash.

**Parameters**:

1. `txHash` - `string` - Transaction hash

**Returns**: `Tx | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getTxByHash","params":["0x1234..."],"id":1}'
```

## node\_getPendingTxsGets pending transactions from the mempool.

**Parameters**:

1. `limit` - `number | undefined` - Max txs to return (1-100, default: 100)
2. `after` - `string | undefined` - Return txs after this tx hash

**Returns**: `Tx[]`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getPendingTxs","params":[50],"id":1}'
```

## node\_getPendingTxCountGets the count of pending transactions.

**Parameters**: None

**Returns**: `number`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getPendingTxCount","params":[],"id":1}'
```

## node\_isValidTxValidates a transaction for correctness.

**Parameters**:

1. `tx` - `Tx` - Transaction to validate
2. `options` - `object | undefined` - Options: `isSimulation`, `skipFeeEnforcement`

**Returns**: `TxValidationResult`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_isValidTx","params":[{"data":"0x..."},{"isSimulation":true}],"id":1}'
```

## node\_simulatePublicCallsSimulates the public part of a transaction.

**Parameters**:

1. `tx` - `Tx` - Transaction to simulate
2. `skipFeeEnforcement` - `boolean | undefined` - Skip fee enforcement

**Returns**: `PublicSimulationOutput`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_simulatePublicCalls","params":[{"data":"0x..."},false],"id":1}'
```

## State queries## node\_getPublicStorageAtGets public storage value at a contract slot.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `contract` - `string` - Contract address (32-byte hex)
3. `slot` - `string` - Storage slot (32-byte hex)

**Returns**: `string` - Storage value (32-byte hex)

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getPublicStorageAt","params":["latest","0x1234...","0x0000..."],"id":1}'
```

## node\_getWorldStateSyncStatusGets the sync status of the node's world state.

**Parameters**: None

**Returns**: `WorldStateSyncStatus`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getWorldStateSyncStatus","params":[],"id":1}'
```

## Merkle tree queries## node\_findLeavesIndexesFinds indexes of leaves in a merkle tree.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `treeId` - `number` - Tree ID (0-6)
3. `leafValues` - `string[]` - Leaf values (max 1000, 32-byte hex each)

**Tree IDs**:

* `0` - NULLIFIER\_TREE
* `1` - NOTE\_HASH\_TREE
* `2` - PUBLIC\_DATA\_TREE
* `3` - L1\_TO\_L2\_MESSAGE\_TREE
* `4` - ARCHIVE
* `5` - BLOCKS\_TREE

**Returns**: Array of leaf indexes with block metadata

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_findLeavesIndexes","params":["latest",1,["0x1234...","0x5678..."]],"id":1}'
```

## node\_getNullifierSiblingPathGets sibling path for a nullifier tree leaf.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `leafIndex` - `string` - Leaf index (bigint as string)

**Returns**: `string[]` - Sibling path

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getNullifierSiblingPath","params":[12345,"100"],"id":1}'
```

## node\_getNoteHashSiblingPathGets sibling path for a note hash tree leaf.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `leafIndex` - `string` - Leaf index (bigint as string)

**Returns**: `string[]` - Sibling path

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getNoteHashSiblingPath","params":["latest","100"],"id":1}'
```

## node\_getArchiveSiblingPathGets sibling path for an archive tree leaf.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `leafIndex` - `string` - Leaf index (bigint as string)

**Returns**: `string[]` - Sibling path

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getArchiveSiblingPath","params":[12345,"50"],"id":1}'
```

## node\_getPublicDataSiblingPathGets sibling path for a public data tree leaf.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `leafIndex` - `string` - Leaf index (bigint as string)

**Returns**: `string[]` - Sibling path

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getPublicDataSiblingPath","params":["latest","200"],"id":1}'
```

## Membership witnesses## node\_getNullifierMembershipWitnessGets a nullifier membership witness.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `nullifier` - `string` - Nullifier value (32-byte hex)

**Returns**: `NullifierMembershipWitness | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getNullifierMembershipWitness","params":[12345,"0x1234..."],"id":1}'
```

## node\_getLowNullifierMembershipWitnessGets a low nullifier membership witness for non-inclusion proofs.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `nullifier` - `string` - Nullifier value (32-byte hex)

**Returns**: `NullifierMembershipWitness | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getLowNullifierMembershipWitness","params":["latest","0x1234..."],"id":1}'
```

## node\_getPublicDataWitnessGets a public data tree witness for a leaf slot.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `leafSlot` - `string` - Leaf slot (32-byte hex)

**Returns**: `PublicDataWitness | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getPublicDataWitness","params":[12345,"0x0000..."],"id":1}'
```

## node\_getArchiveMembershipWitnessGets archive tree membership witness.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `archive` - `string` - Archive leaf value (32-byte hex)

**Returns**: `MembershipWitness | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getArchiveMembershipWitness","params":[12345,"0x1234..."],"id":1}'
```

## node\_getNoteHashMembershipWitnessGets note hash tree membership witness.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `noteHash` - `string` - Note hash value (32-byte hex)

**Returns**: `MembershipWitness | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getNoteHashMembershipWitness","params":["latest","0x1234..."],"id":1}'
```

## L1 to L2 messages## node\_getL1ToL2MessageMembershipWitnessGets L1 to L2 message membership witness.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number
2. `l1ToL2Message` - `string` - L1 to L2 message (32-byte hex)

**Returns**: `[string, string[]] | null` - Tuple of [index, sibling path]

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getL1ToL2MessageMembershipWitness","params":[12345,"0x1234..."],"id":1}'
```

## node\_getL1ToL2MessageBlockGets the L2 block number when an L1 to L2 message becomes available.

**Parameters**:

1. `l1ToL2Message` - `string` - L1 to L2 message (32-byte hex)

**Returns**: `number | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getL1ToL2MessageBlock","params":["0x1234..."],"id":1}'
```

## node\_isL1ToL2MessageSyncedChecks if an L1 to L2 message is synced.

**Parameters**:

1. `l1ToL2Message` - `string` - L1 to L2 message (32-byte hex)

**Returns**: `boolean`

**Deprecated**: Use `node_getL1ToL2MessageBlock` instead.

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_isL1ToL2MessageSynced","params":["0x1234..."],"id":1}'
```

## node\_getL2ToL1MessagesGets all L2 to L1 messages in a block.

**Parameters**:

1. `blockNumber` - `number | "latest"` - Block number

**Returns**: `string[][] | null` - Array of message arrays

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getL2ToL1Messages","params":[12345],"id":1}'
```

## Log queries## node\_getPrivateLogsGets private logs from a block range.

**Parameters**:

1. `from` - `number` - Starting block (≥ 1)
2. `limit` - `number` - Number of blocks (max 1000)

**Returns**: `PrivateLog[]`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getPrivateLogs","params":[100,50],"id":1}'
```

## node\_getPublicLogsGets public logs based on filter.

**Parameters**:

1. `filter` - `LogFilter` - Filter object with `fromBlock`, `toBlock`, `contractAddress`, etc.

**Returns**: `GetPublicLogsResponse`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getPublicLogs","params":[{"fromBlock":100,"toBlock":200}],"id":1}'
```

## node\_getContractClassLogsGets contract class logs based on filter.

**Parameters**:

1. `filter` - `LogFilter` - Filter object

**Returns**: `GetContractClassLogsResponse`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getContractClassLogs","params":[{"fromBlock":100}],"id":1}'
```

## node\_getLogsByTagsGets logs matching specific tags.

**Parameters**:

1. `tags` - `string[]` - Array of tags (max 1000, 32-byte hex each)
2. `logsPerTag` - `number | undefined` - Max logs per tag (1-10, default: 10)

**Returns**: `TxScopedL2Log[][]` - For each tag, array of matching logs

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getLogsByTags","params":[["0x1234...","0x5678..."],10],"id":1}'
```

## Contract queries## node\_getContractClassGets a registered contract class by ID.

**Parameters**:

1. `id` - `string` - Contract class ID (32-byte hex)

**Returns**: `ContractClassPublic | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getContractClass","params":["0x1234..."],"id":1}'
```

## node\_getContractGets a deployed contract instance by address.

**Parameters**:

1. `address` - `string` - Contract address (32-byte hex)

**Returns**: `ContractInstanceWithAddress | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getContract","params":["0x1234..."],"id":1}'
```

## Node information## node\_isReadyChecks if the node is ready to accept transactions.

**Parameters**: None

**Returns**: `boolean`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_isReady","params":[],"id":1}'
```

## node\_getNodeInfoGets information about the node.

**Parameters**: None

**Returns**: `NodeInfo` - Node version, protocol version, chain ID, contracts, etc.

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getNodeInfo","params":[],"id":42}'
```

## node\_getNodeVersionGets the node package version.

**Parameters**: None

**Returns**: `string`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getNodeVersion","params":[],"id":1}'
```

## node\_getVersionGets the rollup protocol version.

**Parameters**: None

**Returns**: `number`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getVersion","params":[],"id":1}'
```

## node\_getChainIdGets the L1 chain ID.

**Parameters**: None

**Returns**: `number`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getChainId","params":[],"id":1}'
```

## node\_getL1ContractAddressesGets deployed L1 contract addresses.

**Parameters**: None

**Returns**: `L1ContractAddresses`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getL1ContractAddresses","params":[],"id":1}'
```

## node\_getProtocolContractAddressesGets protocol contract addresses.

**Parameters**: None

**Returns**: `ProtocolContractAddresses`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getProtocolContractAddresses","params":[],"id":1}'
```

## node\_getEncodedEnrGets the node's ENR for P2P discovery.

**Parameters**: None

**Returns**: `string | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getEncodedEnr","params":[],"id":1}'
```

## node\_getCurrentBaseFeesGets current base fees for transactions.

**Parameters**: None

**Returns**: `GasFees`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getCurrentBaseFees","params":[],"id":1}'
```

## Validator queries## node\_getValidatorsStatsGets statistics for all validators.

**Parameters**: None

**Returns**: `ValidatorsStats`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getValidatorsStats","params":[],"id":1}'
```

## node\_getValidatorStatsGets statistics for a single validator.

**Parameters**:

1. `validatorAddress` - `string` - Validator address (20-byte hex)
2. `fromSlot` - `string | undefined` - Starting slot (bigint as string)
3. `toSlot` - `string | undefined` - Ending slot (bigint as string)

**Returns**: `SingleValidatorStats | null`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getValidatorStats","params":["0x1234...","100","200"],"id":1}'
```

## Debug operations## node\_registerContractFunctionSignaturesRegisters contract function signatures for debugging.

**Parameters**:

1. `functionSignatures` - `string[]` - Array of function signatures (max 100)

**Returns**: `void`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_registerContractFunctionSignatures","params":[["transfer(address,uint256)"]],"id":1}'
```

## node\_getAllowedPublicSetupGets the list of allowed public setup function calls.

**Parameters**: None

**Returns**: `AllowedElement[]`

**Example**:

```
curl -X POST http://localhost:8080 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"node_getAllowedPublicSetup","params":[],"id":1}'
```

## Admin APIAdministrative operations are exposed on port 8880 under the `nodeAdmin_` namespace.

Security: Admin API Access

For security reasons, the admin port (8880) should **not be exposed** to the host machine in Docker deployments. The examples below show both CLI and Docker methods:

**CLI Method** (when running with `aztec start` directly):

```
curl -X POST http://localhost:8880 ...
```

**Docker Method** (when running with Docker Compose):

```
docker exec -it <container-name> curl -X POST http://localhost:8880 ...
```

Replace `<container-name>` with your container name (e.g., `aztec-node`, `aztec-sequencer`, `prover-node`).

## nodeAdmin\_getConfigGets the current node configuration.

**Parameters**: None

**Returns**: `AztecNodeAdminConfig`

**Example (CLI)**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_getConfig","params":[],"id":1}'
```

**Example (Docker)**:

```
docker exec -it aztec-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_getConfig","params":[],"id":1}'
```

## nodeAdmin\_setConfigUpdates the node configuration.

**Parameters**:

1. `config` - `Partial<AztecNodeAdminConfig>` - Configuration updates

**Returns**: `void`

**Example (CLI)**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_setConfig","params":[{"archiverPollingIntervalMS":1000}],"id":1}'
```

**Example (Docker)**:

```
docker exec -it aztec-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_setConfig","params":[{"archiverPollingIntervalMS":1000}],"id":1}'
```

## nodeAdmin\_pauseSyncPauses archiver and world state syncing.

**Parameters**: None

**Returns**: `void`

**Example (CLI)**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_pauseSync","params":[],"id":1}'
```

**Example (Docker)**:

```
docker exec -it aztec-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_pauseSync","params":[],"id":1}'
```

## nodeAdmin\_resumeSyncResumes archiver and world state syncing.

**Parameters**: None

**Returns**: `void`

**Example (CLI)**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_resumeSync","params":[],"id":1}'
```

**Example (Docker)**:

```
docker exec -it aztec-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_resumeSync","params":[],"id":1}'
```

## nodeAdmin\_rollbackToRolls back the database to a target block.

**Parameters**:

1. `targetBlockNumber` - `number` - Block to roll back to
2. `force` - `boolean | undefined` - Clear world state/p2p if needed

**Returns**: `void`

**Example (CLI)**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_rollbackTo","params":[12000,true],"id":1}'
```

**Example (Docker)**:

```
docker exec -it aztec-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_rollbackTo","params":[12000,true],"id":1}'
```

## nodeAdmin\_startSnapshotUploadStarts uploading a database snapshot.

**Parameters**:

1. `location` - `string` - Upload location/URL

**Returns**: `void`

**Example (CLI)**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_startSnapshotUpload","params":["gs://bucket/snapshots/"],"id":1}'
```

**Example (Docker)**:

```
docker exec -it aztec-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_startSnapshotUpload","params":["gs://bucket/snapshots/"],"id":1}'
```

## nodeAdmin\_getSlashPayloadsGets all monitored slash payloads for the current round.

**Parameters**: None

**Returns**: `SlashPayloadRound[]`

**Example (CLI)**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_getSlashPayloads","params":[],"id":1}'
```

**Example (Docker)**:

```
docker exec -it aztec-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_getSlashPayloads","params":[],"id":1}'
```

## nodeAdmin\_getSlashOffensesGets all offenses for a specific round.

**Parameters**:

1. `round` - `string | "all" | "current"` - Round number or "all"/"current"

**Returns**: `Offense[]`

**Example (CLI)**:

```
curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_getSlashOffenses","params":["current"],"id":1}'
```

**Example (Docker)**:

```
docker exec -it aztec-node curl -X POST http://localhost:8880 \  
  -H 'Content-Type: application/json' \  
  -d '{"jsonrpc":"2.0","method":"nodeAdmin_getSlashOffenses","params":["current"],"id":1}'
```

## Next steps* [How to Run a Sequencer Node](/network/setup/sequencer_management) - Set up a node
* [Ethereum RPC Calls Reference](/network/reference/ethereum_rpc_reference) - L1 RPC usage
* [CLI Reference](/network/reference/cli_reference) - Command-line options
* [Aztec Discord](https://discord.gg/aztec) - Developer support

---


# Ethereum RPC call reference

Source: https://docs.aztec.network/network/reference/ethereum_rpc_reference

Version: Ignition (v2.1.9)

On this page

This guide provides a comprehensive reference of Ethereum RPC calls used by different Aztec node components. Understanding these calls helps with infrastructure planning, monitoring, and debugging.

## PrerequisitesBefore proceeding, you should:

* Understand how Aztec nodes interact with Ethereum L1
* Be familiar with Ethereum JSON-RPC API specifications
* Have basic knowledge of the viem library (Aztec's Ethereum client library)

## OverviewAztec nodes interact with Ethereum L1 through the [viem](https://viem.sh) library, which provides a type-safe interface to Ethereum JSON-RPC methods. Different node components make different RPC calls based on their responsibilities:

* **Archiver**: Monitors L1 for new blocks and events
* **Sequencer**: Proposes blocks and submits them to L1
* **Prover**: Submits proofs to L1
* **Validator**: Reads L1 state for validation
* **Slasher**: Monitors for misbehavior and submits slashing payloads

## RPC call mappingThis table shows the Ethereum JSON-RPC calls used by Aztec nodes:

| Ethereum RPC Call | Description |
| --- | --- |
| `eth_getBlockByNumber` | Retrieve block information |
| `eth_blockNumber` | Get latest block number |
| `eth_getTransactionByHash` | Get transaction details |
| `eth_getTransactionReceipt` | Get transaction receipt |
| `eth_getTransactionCount` | Get account nonce |
| `eth_getLogs` | Retrieve event logs |
| `eth_getBalance` | Get account ETH balance |
| `eth_getCode` | Get contract bytecode |
| `eth_getStorageAt` | Read contract storage slot |
| `eth_chainId` | Get chain identifier |
| `eth_estimateGas` | Estimate gas for transaction |
| `eth_call` | Execute read-only call |
| `eth_sendRawTransaction` | Broadcast signed transaction |
| `eth_gasPrice` | Get current gas price |
| `eth_maxPriorityFeePerGas` | Get priority fee (EIP-1559) |

## Archiver nodeThe archiver continuously monitors L1 for new blocks and retrieves historical data.

## Block retrieval**Purpose**: Sync L2 block data published to L1

**RPC calls used**:

* `eth_blockNumber` - Get latest L1 block number
* `eth_getLogs` - Retrieve rollup contract events
* `eth_getBlockByNumber` - Get block timestamps and metadata

**Example RPC calls**:

```
// eth_blockNumber  
{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1}  
  
// eth_getLogs  
{"jsonrpc":"2.0","method":"eth_getLogs","params":[{  
  "fromBlock":"0x100",  
  "toBlock":"0x200",  
  "address":"0x...",  
  "topics":["0x..."]  
}],"id":2}  
  
// eth_getBlockByNumber  
{"jsonrpc":"2.0","method":"eth_getBlockByNumber","params":["0x100",false],"id":3}
```

## L1 to L2 message retrieval**Purpose**: Track messages sent from L1 to L2

**RPC calls used**:

* `eth_getLogs` - Retrieve `MessageSent` events from Inbox contract

## Contract event monitoring**Purpose**: Monitor contract deployments and updates

**RPC calls used**:

* `eth_getLogs` - Retrieve events from ClassRegistry and InstanceRegistry

**Events monitored**:

* `ContractClassPublished`
* `ContractInstancePublished`
* `ContractInstanceUpdated`
* `PrivateFunctionBroadcasted`
* `UtilityFunctionBroadcasted`

## Sequencer nodeSequencers propose blocks and submit them to L1, they also read L1 state to validate blocks and participate in consensus.

## Transaction broadcasting**Purpose**: Submit block proposals to L1

**RPC calls used**:

* `eth_getTransactionCount` - Get nonce for sender account
* `eth_estimateGas` - Estimate gas for proposal transaction
* `eth_sendRawTransaction` - Broadcast signed transaction
* `eth_getTransactionReceipt` - Verify transaction inclusion

**Example RPC calls**:

```
// eth_getTransactionCount  
{"jsonrpc":"2.0","method":"eth_getTransactionCount","params":["0x...","latest"],"id":1}  
  
// eth_estimateGas  
{"jsonrpc":"2.0","method":"eth_estimateGas","params":[{  
  "from":"0x...",  
  "to":"0x...",  
  "data":"0x..."  
}],"id":2}  
  
// eth_sendRawTransaction  
{"jsonrpc":"2.0","method":"eth_sendRawTransaction","params":["0x..."],"id":3}  
  
// eth_getTransactionReceipt  
{"jsonrpc":"2.0","method":"eth_getTransactionReceipt","params":["0x..."],"id":4}
```

## State reading**Purpose**: Read rollup state and validate proposals

**RPC calls used**:

* `eth_call` - Read contract state
* `eth_getStorageAt` - Read specific storage slots
* `eth_blockNumber` - Get current L1 block for validation context
* `eth_getBlockByNumber` - Get block timestamps

**Example RPC calls**:

```
// eth_call  
{"jsonrpc":"2.0","method":"eth_call","params":[{  
  "to":"0x...",  
  "data":"0x..."  
},"latest"],"id":1}  
  
// eth_getStorageAt  
{"jsonrpc":"2.0","method":"eth_getStorageAt","params":["0x...","0x0","latest"],"id":2}
```

## Gas management**Purpose**: Monitor gas prices and publisher account balances

**RPC calls used**:

* `eth_getBalance` - Check publisher account balance
* `eth_gasPrice` / `eth_maxPriorityFeePerGas` - Get current gas prices

## Block simulation**Purpose**: Validate block proposals before submission

**RPC calls used**:

* `eth_call` - Simulate contract call to validate proposals

## Prover nodeThe prover submits validity proofs to L1.

## Proof submission**Purpose**: Submit epoch proofs to the Rollup contract

**RPC calls used**:

* `eth_getTransactionCount` - Get nonce for prover publisher
* `eth_estimateGas` - Estimate gas for proof submission
* `eth_sendRawTransaction` - Broadcast proof transaction
* `eth_getTransactionReceipt` - Confirm proof inclusion

**Note**: Uses the same transaction flow as sequencer broadcasting

## Chain state monitoring**Purpose**: Track L1 state for attestation validation

**RPC calls used**:

* `eth_getBlockByNumber` - Get L1 timestamps for epoch calculations
* `eth_chainId` - Verify connected to correct chain

## Slasher nodeThe slasher monitors for validator misbehavior and submits slashing payloads.

## Misbehavior detection**Purpose**: Monitor for slashable offenses and create slash payloads

**RPC calls used**:

* `eth_getLogs` - Retrieve rollup events for analysis
* `eth_getBlockByNumber` - Get block timestamps for slashing proofs
* `eth_call` - Read validator state

## Slashing payload submission**Purpose**: Submit slash payloads to L1

**RPC calls used**:

* `eth_getTransactionCount` - Get nonce for slasher account
* `eth_sendRawTransaction` - Broadcast slashing transaction
* `eth_getTransactionReceipt` - Verify slash transaction inclusion

**Note**: Uses the same transaction flow as sequencer broadcasting

## Shared infrastructureAztec provides shared transaction management utilities for all components that submit to L1.

## Core functionality**RPC calls used**:

* `eth_getTransactionCount` - Nonce management
* `eth_estimateGas` - Gas estimation
* `eth_gasPrice` / `eth_maxPriorityFeePerGas` - Gas pricing (EIP-1559)
* `eth_sendRawTransaction` - Transaction broadcasting
* `eth_getTransactionReceipt` - Transaction status checking
* `eth_getTransactionByHash` - Transaction lookup for replacement
* `eth_getBlockByNumber` - Block timestamp for timeout checks
* `eth_getBalance` - Publisher balance monitoring

## Transaction lifecycle1. **Preparation**: Estimate gas and get gas price
2. **Nonce management**: Get and track nonce via `NonceManager`
3. **Signing**: Sign transaction with keystore
4. **Broadcasting**: Send via `eth_sendRawTransaction`
5. **Monitoring**: Poll with `eth_getTransactionReceipt`
6. **Replacement**: Replace stuck transactions if needed
7. **Cancellation**: Send zero-value transaction to cancel

## RPC endpoint configuration## Environment variablesConfigure L1 RPC endpoints using:

```
# Single endpoint  
ETHEREUM_HOSTS=https://eth-mainnet.example.com  
  
# Multiple endpoints (fallback)  
ETHEREUM_HOSTS=https://eth-mainnet-1.example.com,https://eth-mainnet-2.example.com  
  
# Consensus endpoints for archiver  
L1_CONSENSUS_HOST_URLS=https://beacon-node.example.com
```

## Fallback configurationAztec automatically retries failed requests on alternative endpoints when multiple RPC URLs are configured. This provides reliability and redundancy for critical operations.

## Monitoring and debugging## RPC call loggingEnable detailed RPC logging:

```
LOG_LEVEL=debug # or verbose
```

Look for log entries related to:

* Transaction lifecycle and nonce management
* Block sync and event retrieval
* Block proposal submissions
* Contract interactions

## Common issues**Issue**: `eth_getLogs` query exceeds limits

**Solution**:

* Reduce block range in queries
* Use archive node with higher limits
* Implement chunked log retrieval

**Issue**: Transaction replacement failures

**Solution**:

* Ensure `eth_getTransactionCount` returns consistent nonces
* Configure appropriate gas price bumps
* Monitor transaction pool status

**Issue**: Stale state reads

**Solution**:

* Use specific block tags (not `latest`)
* Disable caching with `cacheTime: 0`
* Ensure RPC node is fully synced

## Next steps* Review [How to Run a Sequencer Node](/network/setup/sequencer_management) for operational guidance
* Learn about [High Availability Sequencers](/network/setup/high_availability_sequencers) for production redundancy configurations
* Explore [Advanced Keystore Patterns](/network/operation/keystore/advanced_patterns) for complex key management
* Check [Useful Commands](/network/operation/sequencer_management/useful_commands) for monitoring tools
* Join the [Aztec Discord](https://discord.gg/aztec) for infrastructure support

---
